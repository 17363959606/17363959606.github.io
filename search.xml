<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sprng中使用Profile注解]]></title>
    <url>%2F2021%2F03%2F03%2Fspring-Profile%2F</url>
    <content type="text"><![CDATA[Spring框架中使用profile注解实现根据环境条件开启特定功能1、Profile; 1234567@Component//spring.profiles.active = test\pro\pre 此实体类就会生效@Profile(&#123;&quot;test&quot;, &quot;pro&quot;, &quot;pre&quot;&#125;)//在虚拟机参数位置加载 -Dspring.profiles.active=testpublic class xx &#123; &#125; 2.spring框架中可使用active参数的配：不配置默认为default 1.jvm参数的形式指定active的值： 123-Dsrping.profiles.active=test 2.在web.xml中配置： 12345678910111213141516171819202122232425方式二或三选择其一即可方式一：&lt;!-- 在上下文context-param中设置profile.default的默认值 --&gt; &lt;context-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;development&lt;/param-value&gt; &lt;/context-param&gt; 方式二： &lt;!-- 在上下文context-param中设置profile.active的默认值 --&gt; &lt;!-- 设置active后default失效，web启动时会加载对应的环境信息 --&gt; &lt;context-param&gt; &lt;param-name&gt;spring.profiles.active&lt;/param-name&gt; &lt;param-value&gt;development&lt;/param-value&gt; &lt;/context-param&gt; 方式三： &lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 在DispatcherServlet参数中设置profile的默认值，active同理 --&gt; &lt;init-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;development&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt;]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[page]]></title>
    <url>%2F2021%2F03%2F03%2Fspring%E4%B8%AD%E4%BD%BF%E7%94%A8profile%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[redissonScheduling]]></title>
    <url>%2F2021%2F03%2F03%2F%E5%88%86%E5%B8%83%E9%94%81%E5%A4%84%E7%90%86%E5%A4%9A%E8%8A%82%E7%82%B9%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[使用Redisson分布式锁实现服务多节点部署，单节点执行定时任务，保证任务不会重复执行1.技术点Scheduling、redisson; redssion依赖 pom% 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.12.0&lt;/version&gt;&lt;/dependency&gt; 2.配置redisson实体类 12345678910111213141516171819@Configurationpublic class RedissonConfig &#123; @Value(&quot;$&#123;redis.port&#125;&quot;) private int port; @Value(&quot;$&#123;redis.host&#125;&quot;) private String host; @Value(&quot;$&#123;redis.pass&#125;&quot;) private String pwd; @Bean(destroyMethod = &quot;shutdown&quot;) public RedissonClient redissonClient() &#123; //1.创建配置 Config config = new Config(); config.useSingleServer().setAddress(&quot;redis://&quot; + host + &quot;:&quot; + port).setPassword(pwd); //2、根据Config创建出来RedissonClient示例 RedissonClient redissonClient = Redisson.create(config); return redissonClient; &#125;&#125; 3.实战demo 1234567891011121314151617181920212223@Scheduled(fixedRate = 1000 * 60) public void RobinTask() throws ParseException &#123; //从reids中获取robinTaskLock这个值key RLock lock = redissonClient.getLock(&quot;robinTaskLock&quot;); log.info(&quot;robinTaskLock start&quot;); //判断此key是否占用（不等待立马返回），没有占用返回true if (lock.tryLock()) &#123; log.info(&quot;RobinTask available&quot;); try &#123; //your code ... &#125; &#125; finally &#123; //判断这key是否由当前线程锁持有，根据自身需求可以不做该判断 if (lock.isHeldByCurrentThread()) &#123; //释放robinTaskLock锁，（同时删除redis中robinTaskLock这个key） lock.unlock(); &#125; &#125; &#125; else &#123; //key被占用根据自身业务做处理。 log.info(&quot;RobinTask not available&quot;); &#125; &#125; 更多redisson使用技巧请看官网 @Redissongithub.com/redisson/redisson]]></content>
      <categories>
        <category>redission</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat Too many open files]]></title>
    <url>%2F2021%2F01%2F20%2FTooManyOpenFiles%2F</url>
    <content type="text"><![CDATA[一：Tomcat报错： org.apache.tomcat.util.net.NioEndpoint$Acceptor.run Socket accept failed java.io.IOException: Too many open files前天下午所有调用到后台服务的功能，出现大面积的响应超时或连接拒绝。于是特意上Linux服务器上查看了相关的服务日志，日志里面一直打印： 1234567891011121314org.apache.tomcat.util.net.NioEndpoint$Acceptor.run Socket accept failed java.io.IOException: Too many open files at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250) at org.apache.tomcat.util.net.NioEndpoint$Acceptor.run(NioEndpoint.java:453) at java.lang.Thread.run(Thread.java:748)java.net.SocketException: Too many open files at java.net.PlainSocketImpl.accept(Compiled Code) at java.net.ServerSocket.implAccept(Compiled Code) at java.net.ServerSocket.accept(Compiled Code) at weblogic.t3.srvr.ListenThread.run(Compiled Code) 第一次时间想到的是项目中的Socket错误使用没有正常的关闭，然后查询了下项目发现项目中并没有使用到Socket技术栈。 然后上网搜索了，网上上解释为： 打开的文件过多，一般来说是由于应用程序对资源使用不当造成，比如没有及时关闭Socket或数据库连接等。但也可能应用确实需要打开比较多的文件句柄，而系统本身的设置限制了这一数量。 文件打开数过多最坏的情况可以使系统崩溃，到时候只能是重起服务器了。 1操作系统的中打开文件的最大句柄数受限所致，常常发生在很多个并发用户访问服务器的时候.因为为了执行每个用户的应用服务器都要加载很多文件(new一个socket就需要一个文件句柄),这就会导致打开文件的句柄的缺乏. 解决: 1.尽量把类打成jar包,因为一个jar包只消耗一个文件句柄,如果不打包,一个类就消耗一个文件句柄.2.java的垃圾回收不能关闭网络连接打开的文件句柄,如果没有执行close()(例如:java.net.Socket.close())则文件句柄将一直存在,而不能被关闭.你也可以考虑设置socket的最大。 3.对操作系统做相关的设置,增加最大文件句柄数量。 服务器端修改： 查看系统允许打开的最大文件数 #cat /proc/sys/fs/file-max 查看每个用户允许打开的最大文件数ulimit -a发现系统默认的是open files (-n) 1024，问题就出现在这里。 在系统文件/etc/security/limits.conf中修改这个数量限制，在文件中加入内容： 1234567891011* soft nofile 65536 * hard nofile 65536* soft nproc 65536* hard nproc 65536* soft memlock unlimited* hard memlock unlimited或只加下面两个参数都行的* soft nofile 65536 * hard nofile 65536修改完成保存，重启服务器 另外方法：1.使用ps -ef |grep java (java代表你程序，查看你程序进程) 查看你的进程ID，记录ID号，假设进程ID为13052.使用：lsof -p 1305 | wc -l 查看当前进程id为1305的 文件操作状况执行该命令出现文件使用情况为 11923.使用命令：ulimit -a 查看每个用户允许打开的最大文件数发现系统默认的是open files (-n) 1024，问题就出现在这里。4.然后执行：ulimit -n 4096将open files (-n) 1024 设置成open files (-n) 4096 这样就增大了用户允许打开的最大文件数。 注：本文转自：https://blog.51cto.com/meiling/2128296]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windosDocker]]></title>
    <url>%2F2020%2F12%2F02%2FwindosDocker%2F</url>
    <content type="text"><![CDATA[在Windows系统上安装Docker前提条件： 1.必须启用Hyper-V和Containers Windows功能。 要在Windows 10上成功运行Client Hyper-V，需要满足以下硬件先决条件： 具有二级地址转换（SLAT）的64位处理器4GB系统内存 开启Windows需要的环境 1.在Windows上开启Hyper-V，打开“控制面板” -&gt;”程序和功能”-&gt;”启动或关闭Windows功能” 如果找不到Hyper-V请自行创建.txt文件然后追加下面的内容最后改为.bat文件，再以管理员的权限运行直到运行完成输入YES然后重新电脑，在执行步骤1即可找到Hyper-V 1234567pushd &quot;%~dp0&quot;dir /b %SystemRoot%\servicing\Packages\*Hyper-V*.mum &gt;hyper-v.txtfor /f %%i in (&apos;findstr /i . hyper-v.txt 2^&gt;nul&apos;) do dism /online /norestart /add-package:&quot;%SystemRoot%\servicing\Packages\%%i&quot;del hyper-v.txtDism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL 安装Docker 前往docker官方下载： https://hub.docker.com/editions/community/docker-ce-desktop-windows/ 安装完后，根据步骤下载安装wsl2 然后安装Ubuntu，这样安装就算完成，完成后就可以使用Ubuntu来进行docker的使用； 题外话 使用ubuntu安装mysql 第一步前往docke官方并登陆，然后找到msyql 点击View Available Tags 查询所有mysql的版本号 然后选中自己想要的tag进行复制，然后再ubuntu中输入 后面的步骤请查看我的博客安装docker安装mysql的章节 这里有几点需要注意： 1. 12345docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tagtag的值一定要带上 2.在进入mysql时候可能会提示找不到mysql命令，这时候执行系统提示的语句：apt install mysql-client-core-8.0 可能会出现 ：”E: Unable to locate package mysql-client-core-8.0“ 错误提示 那么此时需要执行：sudo apt-get update 语句 原因是：软件源没有更新导致，或者是修改了软件源 或者自行百度修改软件源。 3.最后在修改Mysql char是的时候，里面的localhost需要替换为%。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>windosDocker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket]]></title>
    <url>%2F2020%2F11%2F24%2Fsocket%2F</url>
    <content type="text"><![CDATA[关于网络通讯TCP/IP的一些基础知识 TCP/IP协议族主要协议有IP协议（互联网协议，InternetProtocol）,TCP协议（传输控制协议，TransmissionControlProtocol）和UDP协议（用户数据报协议，UserDatagramProtocol） IP协议只是一个“尽力而为“（best-effort）的协议：它试图分发每一个分组报文，但在网络传输过程中，偶尔也会发生丢失报文，使报文顺序被打乱或重复发送报文的情况。 IP协议层之上称为传输层（transport layer）。它提供了两种可选择的协议：TCP协议和UDP协议 这两种协议都建立在IP层所提供的服务基础上，但根据应用程序协议（application protocols）的不同需求 它们使用了不同的方法来实现不同方式的传输。TCP协议和UDP协议有一个共同的功能“寻址”。IP协议只是将分组报文分发到了不同的主机，还需要更细粒度的寻址将报文发送到主机中指定的应用程序，因为同一主机可能有多个应用程序在使用网络。TCP协议和UDP协议使用的地址叫做端口号（port numbers),都是用来区分同一主机中的不同应用程序。TCP协议和UDP协议也称为端到端传输协议（end-to-end transport protocols），因为他们将数据从一个应用程序传输到另一个应用程序，而IP协议只是将数据从一个主机传输到另一个主机。 TCP协议能检测和恢复IP层提供的主机到主机的信道中可能发生的报文丢失、重复及其他错误。TCP协议提供了一个可信赖的字节流（reliable byte-stream）信道，这样应用程序就不需要在处理上述的问题。TCP协议是一种面向连接（connection-oriented）的协议：在通信之前两个程序首先要建立一个TCP连接，这涉及到相互通信的两台电脑的TCP部件间完成的握手信息（handshake messages）的交换。使用TCP协议在很多方面都与文件的输入输出（I/O，Input/Output）相似。另一方面UDP协议不会IP层产生的错误进行修复，它仅仅只是简单的扩展了IP协议的数据报服务。 信息编码1.基本整型 TCP和UDP套接字是我们能发送和接收字节列（数组）,即范围在0-255之间的整数。使用这个功能，我们可以对值更大的基本整型数据进行编码，不过发送者和接收者必须先在一些方面达成共识。 一：是要传输的每个整数的字节大小（size），如：int数据类型由32位表示，因此我们可以使用4个字节来传输任意的int类型变量或常量。 二：对于超过一个字节来表示的数据类型，我们必须知道这些字节的发送顺序。有两种选择：从整数的右边开始， 由低位到高位地发送，即little-endian顺序；或者从左边开始，由高位到低位，即big-endian顺序。 （java是大端即big-endian,c是小端即little-endian，所以若是java和c端使用socket通信注意大小端的转换） 三：所传输的数值是有符号的（signed）还是无符号的（unsigned）。Java中的四种基本数据类型都是有符号的，它们的值以二进制补码（two’s-complement）的方式存储。 2.反码和补码]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机进制换算]]></title>
    <url>%2F2020%2F10%2F15%2Fsystem%2F</url>
    <content type="text"><![CDATA[计算机进制的知识： 数制是用一组固定的符号和统一的规则来表示数值的方法 计算机底层使用的数制是二进制 用Java编程使用的是十进制。Java底层使用的是二进制 计算机常用的数制还有八进制和十六进制 区分：2进制 只有0和1组成 如:0101018进制 以0开头，0~7组成 如:01234510进制 以1~9开头，0~9组成 如:10016进制 以0X开头，0~9或者a~f组成 如:0x12c 十进制： 十进制与其他进制的转换： 二进制： 二进制数与其他进制的转换： 十六进制： 图中的A、B、C、D、E、F分别表示10,11,12,13,14,15 十六进制与其他进制的转换： 补码反码问题：机器数： 一个数在计算机中的二进制表示形式, 叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1。 反码： 反码的表示方法是:正数的反码是其本身，负数的反码是在其原码的基础上, 符号位不变，其余各个位取反 补码：补码的表示方法是:正数的补码就是其本身，负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)； 以btye为例：byte是8位，二进制表示为： [0111 1111] = 最高位表示符号不参与运算（即第一位，二进制是从右往左排的）= 1 2^0 + …+ 1 2^6 = 127; [1111 1111] = 最高位为1表示负数需要进行反码:[1000 0000] = 反码后需要补码：[1000 0001] = 这个值的规律为：-（正在最大值 + 1 ） = -128 byte 数据类型是8位、有符号的，以二进制补码表示的整数； 最小值是 -128（-2^7）； 最大值是 127（2^7-1）； 默认值是 0； byte 类型用在大型数组中节约空间，主要代替整数，因为 byte 变量占用的空间只有 int 类型的四分之一； 例子：byte a = 100，byte b = -50。 short： short 数据类型是 16 位、有符号的以二进制补码表示的整数 最小值是 -32768（-2^15）； 最大值是 32767（2^15 - 1）； Short 数据类型也可以像 byte 那样节省空间。一个short变量是int型变量所占空间的二分之一； 默认值是 0； 例子：short s = 1000，short r = -20000。 int： int 数据类型是32位、有符号的以二进制补码表示的整数； 最小值是 -2,147,483,648（-2^31）； 最大值是 2,147,483,647（2^31 - 1）； 一般地整型变量默认为 int 类型； 默认值是 0 ； 例子：int a = 100000, int b = -200000。 long： long 数据类型是 64 位、有符号的以二进制补码表示的整数； 最小值是 -9,223,372,036,854,775,808（-2^63）； 最大值是 9,223,372,036,854,775,807（2^63 -1）； 这种类型主要使用在需要比较大整数的系统上； 默认值是 0L； 例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float： float 数据类型是单精度、32位、符合IEEE 754标准的浮点数； float 在储存大型浮点数组的时候可节省内存空间； 默认值是 0.0f； 浮点数不能用来表示精确的值，如货币； 例子：float f1 = 234.5f。 double： double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数； 浮点数的默认类型为double类型； double类型同样不能表示精确的值，如货币； 默认值是 0.0d； 例子：double d1 = 123.4。 boolean： boolean数据类型表示一位的信息； 只有两个取值：true 和 false； 这种类型只作为一种标志来记录 true/false 情况； 默认值是 false； 例子：boolean one = true。 char： char类型是一个单一的 16 位 Unicode 字符； 最小值是 \u0000（即为 0）； 最大值是 \uffff（即为65、535）； char 数据类型可以储存任何字符； 例子：char letter = ‘A’;。 位运算符：&lt;&lt;运算符1、 左移运算符 左移运算符&lt;&lt;使指定值的所有位都左移规定的次数。 1）它的通用格式如下所示： value &lt;&lt; num num 指定要移位值value 移动的位数。 左移的规则只记住一点：丢弃最高位，0补最低位 如果移动的位数超过了该类型的最大位数，那么编译器会对移动的位数取模。如对int型移动33位，实际上只移动了32位。 2）运算规则 按二进制形式把所有的数字向左移动对应的位数，高位移出(舍弃)，低位的空位补零。 当左移的运算数是int 类型时，每移动1位它的第31位就要被移出并且丢弃； 当左移的运算数是long 类型时，每移动1位它的第63位就要被移出并且丢弃。 当左移的运算数是byte 和short类型时，将自动把这些类型扩大为 int 型。 3）数学意义 在数字没有溢出的前提下，对于正数和负数，左移一位都相当于乘以2的1次方，左移n位就相当于乘以2的n次方 4）计算过程： 例如：3 &lt;&lt;2(3为int型) 1）把3转换为二进制数字0000 0000 0000 0000 0000 0000 0000 0011， 2）把该数字高位(左侧)的两个零移出，其他的数字都朝左平移2位， 3）在低位(右侧)的两个空位补零。则得到的最终结果是0000 0000 0000 0000 0000 0000 0000 1100， 转换为十进制是12。 移动的位数超过了该类型的最大位数， 注：n位二进制，最高位为符号位，因此表示的数值范围-2^(n-1) ——2^(n-1) -1,所以模为2^(n-1)。 ‘&gt;&gt;’运算符右移运算符&lt;&lt;使指定值的所有位都右移规定的次数。 1）它的通用格式如下所示： value &gt;&gt; num num 指定要移位值value 移动的位数。 右移的规则只记住一点：符号位不变，左边补上符号位 2）运算规则： 按二进制形式把所有的数字向右移动对应的位数，低位移出(舍弃)，高位的空位补符号位，即正数补零，负数补1 当右移的运算数是byte 和short类型时，将自动把这些类型扩大为 int 型。 例如，如果要移走的值为负数，每一次右移都在左边补1，如果要移走的值为正数，每一次右移都在左边补0，这叫做符号位扩展（保留符号位）（sign extension ），在进行右移 操作时用来保持负数的符号。 3）数学意义 右移一位相当于除2，右移n位相当于除以2的n次方。 4）计算过程 11 &gt;&gt;2(11为int型) 1)11的二进制形式为：0000 0000 0000 0000 0000 0000 0000 1011 2)把低位的最后两个数字移出，因为该数字是正数，所以在高位补零。 3)最终结果是0000 0000 0000 0000 0000 0000 0000 0010。 转换为十进制是2。 ‘&gt;&gt;&gt;’运算符无符号右移运算符&gt;&gt;&gt; 它的通用格式如下所示： value &gt;&gt;&gt; num num 指定要移位值value 移动的位数。 无符号右移的规则只记住一点：忽略了符号位扩展，0补最高位 1 无符号右移规则和右移运算是一样的，只是填充时不管左边的数字是正是负都用0来填充，无符号右移运算只针对负数计算，因为对于正数来说这种运算没有意义 无符号右移运算符&gt;&gt;&gt; 只是对32位和64位的值有意义 &amp;运算符：与运算符用符号“&amp;”表示，其使用规律如下： 两个操作数中位都为1，结果才为1，否则结果为0， int a = 129; int b = 128; a值的二进制表示：1000 0001 ,b值的二进制表示:1000 0000 ,根据规律a &amp; b = 1000 0000 = 128; |运算符：或运算符用符号“|”表示，其运算规律如下： 两个位只要有一个为1，那么结果就是1，否则就为0，下面看一个简单的例子。 int a = 129; int b = 128; a值的二进制表示：1000 0001 ,b值的二进制表示:1000 0000 ,根据规律a &amp; b = 1000 0001 = 129; ^运算符：异或运算符是用符号“^”表示的，其运算规律是： 两个操作数的位中，相同则结果为0，不同则结果为1。下面看一个简单的例子。 int a=15; int b=2; a值的二进制表示：0000 1111，b值的二进制表示：0000 0010，根据规律 a ^ b = 0000 1101 = 13 ~运算符：非运算符用符号“~”表示，其运算规律如下： 如果位为0，结果是1，如果位为1，结果是0，下面看一个简单例子。 int a = 2; ~a = -3;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker配置镜像加速]]></title>
    <url>%2F2020%2F06%2F29%2Fdockeraccelerate%2F</url>
    <content type="text"><![CDATA[为什么要配置镜像加速器？因为国内从docker hub上拉取镜像有时会遇到困难，比如在拉去Oracle镜像下载困难。 配置加速地址Ubuntu 16.04+, Debian 8+, CentOS7+ 创建或修改 /etc/docker/daemon.json 123456789&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125;注：内容一定要按照json格式，其中的地址有很多请自行百度 检查加速器是否生效1234567891011执行：docker info若看到：Registry Mirrors https://registry.docker-cn.com则表示成功，但是本人在配置完测试，并没有看到有改变，但是速度确实是提示了很多。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker创建oracle]]></title>
    <url>%2F2020%2F06%2F29%2FdockerOracle%2F</url>
    <content type="text"><![CDATA[安装oracle:https://hub.docker.com/r/loliconneko/oracle-ee-11g 123docker pull loliconneko/oracle-ee-11g 启动123docker run -h &quot;oracle&quot; --name &quot;oracle&quot; -d -p 1521:1521 loliconneko/oracle-ee-11g 进入容器123docker exec -it &lt;container_id&gt; /bin/bash 登录oracle1234567sqlplus sysdba/oracle若上面那个命令不行可以改为：sqlplus system/oracle 创建用户123create user &lt;user_name&gt; identified by &lt;password&gt;; 查询用户123select username,password from dba_users; 用户授权12345grant connect,resource to &lt;user_name&gt;;connect 是保证该用户能连接数据库 resource 是该用户可以使用数据库资源 连接配置jdbc:oracle:thin:@::EE.oracle.docker 或者 jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=)(PORT=))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=EE.oracle.docker))) 查看服务名]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reids]]></title>
    <url>%2F2020%2F06%2F16%2Freids%2F</url>
    <content type="text"><![CDATA[一：reids的基础知识1.启动redis: ./reids.server /xx/xx/redis.conf 2.检测reids是否启动成功： ./reids.cli -p 6379 (6379为redis的默认端口号) 输入Ping恢复pong表示成功。 3.redis默认是16个库，select x (x表示索引 0-15)命令切换数据库 库的索引从0开始，select 1表示切换到第二个库。 redis的库数量可以在reids.conf文件中修改定义:如下图 4.dbsize命令查看当前数据库的key的数量，keys * 查询当前库所有的数据， keys k?查询当前库所有以k开头的数据 5.flushdb:清空当前库数据，flushall:清空全部库数据 二.redis的五大数据类型 redis String (字符串) redis Hash （哈希） redis list （列表） redis set （集合） redis zset （sorted set:有序集合） 2.常见的redis数据类型操作命令：http://redisdoc.com/ 三.redis数据类型的常用命令Redis键（key） : keys * exists key的名字判断某个key是否存在（1/0） move key db –&gt; 丛当前库中移除指定的k到指定的库中： move k2 2 (把当前库中的k2移除到第3个数据库中) expire key 秒钟：为给定的key设置过期时间 ttl key 查看还有多少秒过期，-1表示永不过期、-2表示已过期 type key 查看你的Key是什么类型 Redis 字符串(String) : set/get/del/append/strlen append k2 123 //在 k2后面最近123 Incr/decr/incrby/decrby,一定要是数字才能进行加减 getrange/setrange 取范围内的值/设置范围内的值: getrange k2 0 2/setrange k2 0 xxx setex(set with expire) :setex k2 10 k2 //设置k2十秒后过期删除 setnx（set if not exist）:setnx k2 123 //判断k2是存在，不存在则设置它的值为123 mset/mget/msetnx ：设置/获取多个值/多个不存在：mset k1 v2 k2 v2 /mget k1 k2/msetnx k1 v1 k2 v2 getset(先get再set) Redis列表（List）: lpush/rpush/lrange lpop/rpop出站。（从前面开始【正顺】/从后面开始【反顺】） lindex，按照索引下标获取元素（从上到下） lindex list01 1 llen 长度 lrem list删除n个value : lrem list01 1 3 (就是删除集合中的1个3) ltrim list 开始index 接收index,截取指定范围的值然后赋值给key (就是截取集合中1-3索引的值)：list=1234 ,ltrim list 1 3 结果为：234 rpoplpush 源列表 目的列表 lset key index value : 对集合指定的索引赋值 linsert key before/after 值1 值2 把某个值插入到某个Key的前面或者后面 Redis集合（Set） sadd/smembers/sismember scard ,获取集合里面有多少个元素 srem key value 删除集合中的元素 srandmember key 某个整数（随机出几个数）：srandmember set 3,在set这个集合中随机去除3个数据 spop key 随机出栈 （比如：spop set01 即从set01中随便“移除”一个值出来） smove key1 key2 ，作用是将key1里面的某个‘移除值赋值’给key2 sdiff：差集、sinter:交集、sunion:并集 Redis哈希（Hash） kv模式不变，但是v是一个键值对 hset/hget/hmget/hgetall/hdel : hset user id 11:设置user里面的id为11 hlen hexists key 在key里面是否存在某个值的key (1/0) hkeys/hvals 单独获取hash里面的Key或value ： hkeys user hincrby/hincybyfloat hsetnx : hsetnx user id 11(判断user里面是否存在id，存在则不出返回0，否则新增id 11,返回1) Redis有序集合(zset) 在set基础上加了一个score值，之前set是 k1 v1 v2 v3 现在zset是 k1 score1 v1 score2 v2 zadd/zrange/zrange withscores zrangebyscore key 开始score 结束score “(“表示不包含，limit表示从第几个开始取值，取多少个 zrem key ，某score下对应的value值，作用是删除元素 zcard 获取长度 zcount key score 区间， 获取区间大小 zrank key values ：获取下标值 zscore key : 对应值获取对应分数（通过v 获取 score） zrevrank key values 值 ：作用是逆序获取下标值 zrevrange：作用是反转zset里面的，比如： 13 –》31 zrevangebyscore key 和zrangebyscore 相反：zrevangebyscore key 90 60 ，zrangebyscore key 60 90 四：redis配置文件介绍INCLUDES：可以通过includes包含，redis.conf可以作为总闸，包含其他 GENERAL: SNAPSHOTTING: save : 保存快照，数据保存至dump.rdb文件中 stop-writes-on-bgsave-error yes（默认yes）(stop-writes-on-bgsave-error:如果保存错误那么前台停止写) 如果配置成no，表示你不在乎数据的一致性或者有其他手段发现和控制 rdbcompression yes：对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话redis将采用laf算法进行压缩。如果你不想消耗cpu来进行压缩的话，可以设置为No rdbchecksum yes : 在存储快照后，还可以让redis使用crc64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以设置为no dbfilename dump.rdb : 备份文件的名称默认为：dumo.rdb dir ./ :文件目录 五：redis的持久化1：rdb（Redis DataBase） 官方介绍： 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的**Snapshot**快照， ​ 它恢复时是将快照文件直接读到内存里。 是什么： ​ Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中。 ​ 待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 ​ 整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能 ​ 如果需要进行大规模的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比 ​ AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 Fork: ​ 作用是复制一个与当前进程一样的进程。新进程的所有数据、数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程 Redis保存的文件： ​ dump.rdb 配置位置： ​ 配置文件中的：Snapshot ​ 含义： 三种行为触发一个就执行保存操作： 900秒内（15分钟），如果至少更换了一个key 300秒内（5分钟），如果至少10个key发生变化 60秒内，发生10000个key更改 禁用RDB策略释放：save “” 在执行shutdow命令是会立马生产快照dump.rdb文件 flushall命令也会产生dump.rdb，但里面是空的无意义 save:save时只管保存快照，其他不管，全部阻塞 bfsave：redis会在后台异步进行快照操作，快照同时还可以响应客户端请求 可以通过lastsave命令获取最后一次成功执行快照的时间 假如有些数据非常重要更改后要求立马备份，那么就直接执行save，如图： 如何停止： ​ 动态所有停止RDB保存规则的方法：redis-lic config set save “” 2：aof (Append Only File) appendonly no 默认为no，开启为Yes appendfilename “appendonly.aof” aop数据存储数据文件名 aop三个属性： no: 不要fsync，只要让操作系统在需要的时候刷新数据就行了。更快。（不同步，设置数据时自行设置更新）always: fsync after every write to the append only log. Slow, Safest.（每次写入仅追加日志后fsync。慢点，最安全。）everysec: fsync only one time every second. Compromise.（每秒钟只同步一次） 默认为：appendfsync everysec aof的重写功能： auto-aof-rewrite-percentage 100 （默认是文件的一倍：100）auto-aof-rewrite-min-size 64mb （默认文件大小） — 即：当appendonly.aof文件的内容大于64的一倍时，进行重写。（压缩） 六：redis的主从复制：（一主二从，主写从读）REPLICATION配置规则：配从不配主 replicaof masterip masterport – masterid:主redis的地址，masterport：主redis的端口 masterauth master-password – 如果主机有设置密码，那么这个地方一定要写上主机的密码 配置后启动reids服务查看如下 role:master 表主服务器，slave表从服务器 connected_slaves:1 表示有1个从服务器连接 等等信息 七：哨兵模式：为什么要哨兵模式： ​ 简单理解即主从模式会存在主从宕机问题，这个时候就需要程序自己去发现然后去选举新的主服务。 reids安装目录中有个： sentinel.conf文件这个就是配置哨兵模式的文件 port 26379 为默认端口 protected-mode no 这个参数请和redis.conf配置保持一致 bind 127.0.0.1这个参数请和redis.conf配置保持一致 daemonize no 这个设置为yes 即开启后台运行开启 logfile “xx.log” 这个为哨兵日志文件配置 dir /tmp 为文件存储地址 默认为：/tmp sentinel monitor master-name ip redis-port quorum ​ master-name :为主机名称 一般为：mymaster ​ ip :为主机ip地址 ​ redis-port： 为主机端口 ​ quorum ：选举投票数，即有多少票通过则表示认定主机宕机，然后开始选举新的主机 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel auth-pass master-name password ​ 若主机有密码则设置此项没有则不用设置，若设置那么主从的密码一定要一致 sentinel down-after-milliseconds master-name milliseconds sentinel down-after-milliseconds mymaster 30000 主机在30s内无法访问，则认定为宕机。 sentinel parallel-syncs master-name numreplicas **sentinel parallel-syncs mymaster 1 主从转移要有多少个主副本，这里设置为1，数量越多耗时越长。 sentinel failover-timeout master-name milliseconds **sentinel failover-timeout mymaster 180000 指定故障转移超时（毫秒）默认是3分钟 从服务的配置和主服务器配置一样，但port 要改。且端口一定要开放 ​]]></content>
      <categories>
        <category>reids</category>
      </categories>
      <tags>
        <tag>reids</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见Java性能问题定位]]></title>
    <url>%2F2020%2F05%2F08%2Fperformance%2F</url>
    <content type="text"><![CDATA[注：本文转自：https://juejin.im/post/5e9725c1f265da47dc4cd8bd 作者：木木匠 本人只是照搬作为一个记录收藏方便学习。 概述性能优化一向是后端服务优化的重点，但是线上性能故障问题不是经常出现，或者受限于业务产品，根本就没办法出现性能问题，包括笔者自己遇到的性能问题也不多，所以为了提前储备知识，当出现问题的时候不会手忙脚乱，我们本篇文章来模拟下常见的几个Java性能故障，来学习怎么去分析和定位。 预备知识既然是定位问题，肯定是需要借助工具，我们先了解下需要哪些工具可以帮忙定位问题。 top命令 top命令使我们最常用的Linux命令之一，它可以实时的显示当前正在执行的进程的CPU使用率，内存使用率等系统信息。top -Hp pid 可以查看线程的系统资源使用情况。 vmstat命令 vmstat是一个指定周期和采集次数的虚拟内存检测工具，可以统计内存，CPU，swap的使用情况，它还有一个重要的常用功能，用来观察进程的上下文切换。字段说明如下: r: 运行队列中进程数量（当数量大于CPU核数表示有阻塞的线程） b: 等待IO的进程数量 swpd: 使用虚拟内存大小 free: 空闲物理内存大小 buff: 用作缓冲的内存大小(内存和硬盘的缓冲区) cache: 用作缓存的内存大小（CPU和内存之间的缓冲区） si: 每秒从交换区写到内存的大小，由磁盘调入内存 so: 每秒写入交换区的内存大小，由内存调入磁盘 bi: 每秒读取的块数 bo: 每秒写入的块数 in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 us: 用户进程执行时间百分比(user time) sy: 内核系统进程执行时间百分比(system time) wa: IO等待时间百分比 id: 空闲时间百分比 pidstat命令 pidstat 是 Sysstat 中的一个组件，也是一款功能强大的性能监测工具，top 和 vmstat 两个命令都是监测进程的内存、CPU 以及 I/O 使用情况，而 pidstat 命令可以检测到线程级别的。pidstat命令线程切换字段说明如下： UID ：被监控任务的真实用户ID。 TGID ：线程组ID。 TID：线程ID。 cswch/s：主动切换上下文次数，这里是因为资源阻塞而切换线程，比如锁等待等情况。 nvcswch/s：被动切换上下文次数，这里指CPU调度切换了线程。 jstack命令 jstack是JDK工具命令，它是一种线程堆栈分析工具，最常用的功能就是使用 jstack pid 命令查看线程的堆栈信息，也经常用来排除死锁情况。 jstat 命令 它可以检测Java程序运行的实时情况，包括堆内存信息和垃圾回收信息，我们常常用来查看程序垃圾回收情况。常用的命令是jstat -gc pid。信息字段说明如下： S0C：年轻代中 To Survivor 的容量（单位 KB）； S1C：年轻代中 From Survivor 的容量（单位 KB）； S0U：年轻代中 To Survivor 目前已使用空间（单位 KB）； S1U：年轻代中 From Survivor 目前已使用空间（单位 KB）； EC：年轻代中 Eden 的容量（单位 KB）； EU：年轻代中 Eden 目前已使用空间（单位 KB）； OC：老年代的容量（单位 KB）； OU：老年代目前已使用空间（单位 KB）； MC：元空间的容量（单位 KB）； MU：元空间目前已使用空间（单位 KB）； YGC：从应用程序启动到采样时年轻代中 gc 次数； YGCT：从应用程序启动到采样时年轻代中 gc 所用时间 (s)； FGC：从应用程序启动到采样时 老年代（Full Gc）gc 次数； FGCT：从应用程序启动到采样时 老年代代（Full Gc）gc 所用时间 (s)； GCT：从应用程序启动到采样时 gc 用的总时间 (s)。 jmap命令 jmap也是JDK工具命令，他可以查看堆内存的初始化信息以及堆内存的使用情况，还可以生成dump文件来进行详细分析。查看堆内存情况命令jmap -heap pid。 mat内存工具 MAT(Memory Analyzer Tool)工具是eclipse的一个插件(MAT也可以单独使用)，它分析大内存的dump文件时，可以非常直观的看到各个对象在堆空间中所占用的内存大小、类实例数量、对象引用关系、利用OQL对象查询，以及可以很方便的找出对象GC Roots的相关信息。下载地址可以点击这里 模拟环境准备基础环境jdk1.8，采用SpringBoot框架来写几个接口来触发模拟场景，首先是模拟CPU占满情况 CPU占满模拟CPU占满还是比较简单，直接写一个死循环计算消耗CPU即可。 123456789101112131415161718 /** * 模拟CPU占满 */ @GetMapping(&quot;/cpu/loop&quot;) public void testCPULoop() throws InterruptedException &#123; System.out.println(&quot;请求cpu死循环&quot;); Thread.currentThread().setName(&quot;loop-thread-cpu&quot;); int num = 0; while (true) &#123; num++; if (num == Integer.MAX_VALUE) &#123; System.out.println(&quot;reset&quot;); &#125; num = 0; &#125; &#125;复制代码 请求接口地址测试curl localhost:8080/cpu/loop,发现CPU立马飙升到100% 通过执行top -Hp 32805 查看Java线程情况 执行 printf &#39;%x&#39; 32826 获取16进制的线程id，用于dump信息查询，结果为 803a。最后我们执行jstack 32805 |grep -A 20 803a来查看下详细的dump信息。 这里dump信息直接定位出了问题方法以及代码行，这就定位出了CPU占满的问题。 内存泄露模拟内存泄漏借助了ThreadLocal对象来完成，ThreadLocal是一个线程私有变量，可以绑定到线程上，在整个线程的生命周期都会存在，但是由于ThreadLocal的特殊性，ThreadLocal是基于ThreadLocalMap实现的，ThreadLocalMap的Entry继承WeakReference，而Entry的Key是WeakReference的封装，换句话说Key就是弱引用，弱引用在下次GC之后就会被回收，如果ThreadLocal在set之后不进行后续的操作，因为GC会把Key清除掉，但是Value由于线程还在存活，所以Value一直不会被回收，最后就会发生内存泄漏。 1234567891011/** * 模拟内存泄漏 */ @GetMapping(value = &quot;/memory/leak&quot;) public String leak() &#123; System.out.println(&quot;模拟内存泄漏&quot;); ThreadLocal&lt;Byte[]&gt; localVariable = new ThreadLocal&lt;Byte[]&gt;(); localVariable.set(new Byte[4096 * 1024]);// 为线程添加变量 return &quot;ok&quot;; &#125;复制代码 我们给启动加上堆内存大小限制，同时设置内存溢出的时候输出堆栈快照并输出日志。 java -jar -Xms500m -Xmx500m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/tmp/heaplog.log analysis-demo-0.0.1-SNAPSHOT.jar 启动成功后我们循环执行100次,for i in {1..500}; do curl localhost:8080/memory/leak;done,还没执行完毕，系统已经返回500错误了。查看系统日志出现了如下异常： 12java.lang.OutOfMemoryError: Java heap space复制代码 我们用jstat -gc pid 命令来看看程序的GC情况。 很明显，内存溢出了，堆内存经过45次 Full Gc 之后都没释放出可用内存，这说明当前堆内存中的对象都是存活的，有GC Roots引用，无法回收。那是什么原因导致内存溢出呢？是不是我只要加大内存就行了呢？如果是普通的内存溢出也许扩大内存就行了，但是如果是内存泄漏的话，扩大的内存不一会就会被占满，所以我们还需要确定是不是内存泄漏。我们之前保存了堆 Dump 文件，这个时候借助我们的MAT工具来分析下。导入工具选择Leak Suspects Report，工具直接就会给你列出问题报告。 这里已经列出了可疑的4个内存泄漏问题，我们点击其中一个查看详情。 这里已经指出了内存被线程占用了接近50M的内存，占用的对象就是ThreadLocal。如果想详细的通过手动去分析的话，可以点击Histogram,查看最大的对象占用是谁，然后再分析它的引用关系，即可确定是谁导致的内存溢出。 上图发现占用内存最大的对象是一个Byte数组，我们看看它到底被那个GC Root引用导致没有被回收。按照上图红框操作指引，结果如下图： 我们发现Byte数组是被线程对象引用的，图中也标明，Byte数组对像的GC Root是线程，所以它是不会被回收的，展开详细信息查看，我们发现最终的内存占用对象是被ThreadLocal对象占据了。这也和MAT工具自动帮我们分析的结果一致。 死锁死锁会导致耗尽线程资源，占用内存，表现就是内存占用升高，CPU不一定会飙升(看场景决定)，如果是直接new线程，会导致JVM内存被耗尽，报无法创建线程的错误，这也是体现了使用线程池的好处。 123456789101112131415161718192021222324252627282930313233343536373839404142 ExecutorService service = new ThreadPoolExecutor(4, 10, 0, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); /** * 模拟死锁 */ @GetMapping(&quot;/cpu/test&quot;) public String testCPU() throws InterruptedException &#123; System.out.println(&quot;请求cpu&quot;); Object lock1 = new Object(); Object lock2 = new Object(); service.submit(new DeadLockThread(lock1, lock2), &quot;deadLookThread-&quot; + new Random().nextInt()); service.submit(new DeadLockThread(lock2, lock1), &quot;deadLookThread-&quot; + new Random().nextInt()); return &quot;ok&quot;; &#125;public class DeadLockThread implements Runnable &#123; private Object lock1; private Object lock2; public DeadLockThread(Object lock1, Object lock2) &#123; this.lock1 = lock1; this.lock2 = lock2; &#125; @Override public void run() &#123; synchronized (lock2) &#123; System.out.println(Thread.currentThread().getName()+&quot;get lock2 and wait lock1&quot;); try &#123; TimeUnit.MILLISECONDS.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lock1) &#123; System.out.println(Thread.currentThread().getName()+&quot;get lock1 and lock2 &quot;); &#125; &#125; &#125;&#125;复制代码 我们循环请求接口2000次，发现不一会系统就出现了日志错误，线程池和队列都满了,由于我选择的当队列满了就拒绝的策略，所以系统直接抛出异常。 12java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@2760298 rejected from java.util.concurrent.ThreadPoolExecutor@7ea7cd51[Running, pool size = 10, active threads = 10, queued tasks = 1024, completed tasks = 846]复制代码 通过ps -ef|grep java命令找出 Java 进程 pid，执行jstack pid 即可出现java线程堆栈信息，这里发现了5个死锁，我们只列出其中一个，很明显线程pool-1-thread-2锁住了0x00000000f8387d88等待0x00000000f8387d98锁，线程pool-1-thread-1锁住了0x00000000f8387d98等待锁0x00000000f8387d88,这就产生了死锁。 1234567891011121314151617181920212223Java stack information for the threads listed above:===================================================&quot;pool-1-thread-2&quot;: at top.luozhou.analysisdemo.controller.DeadLockThread2.run(DeadLockThread.java:30) - waiting to lock &lt;0x00000000f8387d98&gt; (a java.lang.Object) - locked &lt;0x00000000f8387d88&gt; (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)&quot;pool-1-thread-1&quot;: at top.luozhou.analysisdemo.controller.DeadLockThread1.run(DeadLockThread.java:30) - waiting to lock &lt;0x00000000f8387d88&gt; (a java.lang.Object) - locked &lt;0x00000000f8387d98&gt; (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Found 5 deadlocks.复制代码 线程频繁切换上下文切换会导致将大量CPU时间浪费在寄存器、内核栈以及虚拟内存的保存和恢复上，导致系统整体性能下降。当你发现系统的性能出现明显的下降时候，需要考虑是否发生了大量的线程上下文切换。 123456789101112131415161718192021222324 @GetMapping(value = &quot;/thread/swap&quot;) public String theadSwap(int num) &#123; System.out.println(&quot;模拟线程切换&quot;); for (int i = 0; i &lt; num; i++) &#123; new Thread(new ThreadSwap1(new AtomicInteger(0)),&quot;thread-swap&quot;+i).start(); &#125; return &quot;ok&quot;; &#125;public class ThreadSwap1 implements Runnable &#123; private AtomicInteger integer; public ThreadSwap1(AtomicInteger integer) &#123; this.integer = integer; &#125; @Override public void run() &#123; while (true) &#123; integer.addAndGet(1); Thread.yield(); //让出CPU资源 &#125; &#125;&#125;复制代码 这里我创建多个线程去执行基础的原子+1操作，然后让出 CPU 资源，理论上 CPU 就会去调度别的线程，我们请求接口创建100个线程看看效果如何，curl localhost:8080/thread/swap?num=100。接口请求成功后，我们执行`vmstat 1 10，表示每1秒打印一次，打印10次，线程切换采集结果如下： 123456789procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st101 0 128000 878384 908 468684 0 0 0 0 4071 8110498 14 86 0 0 0100 0 128000 878384 908 468684 0 0 0 0 4065 8312463 15 85 0 0 0100 0 128000 878384 908 468684 0 0 0 0 4107 8207718 14 87 0 0 0100 0 128000 878384 908 468684 0 0 0 0 4083 8410174 14 86 0 0 0100 0 128000 878384 908 468684 0 0 0 0 4083 8264377 14 86 0 0 0100 0 128000 878384 908 468688 0 0 0 108 4182 8346826 14 86 0 0 0复制代码 这里我们关注4个指标，r,cs,us,sy。 r=100,说明等待的进程数量是100，线程有阻塞。 cs=800多万，说明每秒上下文切换了800多万次，这个数字相当大了。 us=14，说明用户态占用了14%的CPU时间片去处理逻辑。 sy=86，说明内核态占用了86%的CPU，这里明显就是做上下文切换工作了。 我们通过top命令以及top -Hp pid查看进程和线程CPU情况，发现Java线程CPU占满了，但是线程CPU使用情况很平均，没有某一个线程把CPU吃满的情况。 123PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87093 root 20 0 4194788 299056 13252 S 399.7 16.1 65:34.67 java 复制代码 1234567 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87189 root 20 0 4194788 299056 13252 R 4.7 16.1 0:41.11 java 87129 root 20 0 4194788 299056 13252 R 4.3 16.1 0:41.14 java 87130 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.51 java 87133 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.59 java 87134 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.95 java 复制代码 结合上面用户态CPU只使用了14%，内核态CPU占用了86%，可以基本判断是Java程序线程上下文切换导致性能问题。 我们使用pidstat命令来看看Java进程内部的线程切换数据，执行pidstat -p 87093 -w 1 10,采集数据如下： 12345678910111211:04:30 PM UID TGID TID cswch/s nvcswch/s Command11:04:30 PM 0 - 87128 0.00 16.07 |__java11:04:30 PM 0 - 87129 0.00 15.60 |__java11:04:30 PM 0 - 87130 0.00 15.54 |__java11:04:30 PM 0 - 87131 0.00 15.60 |__java11:04:30 PM 0 - 87132 0.00 15.43 |__java11:04:30 PM 0 - 87133 0.00 16.02 |__java11:04:30 PM 0 - 87134 0.00 15.66 |__java11:04:30 PM 0 - 87135 0.00 15.23 |__java11:04:30 PM 0 - 87136 0.00 15.33 |__java11:04:30 PM 0 - 87137 0.00 16.04 |__java复制代码 根据上面采集的信息，我们知道Java的线程每秒切换15次左右，正常情况下，应该是个位数或者小数。结合这些信息我们可以断定Java线程开启过多，导致频繁上下文切换，从而影响了整体性能。 为什么系统的上下文切换是每秒800多万，而 Java 进程中的某一个线程切换才15次左右？ 系统上下文切换分为三种情况: 1、多任务：在多任务环境中，一个进程被切换出CPU，运行另外一个进程，这里会发生上下文切换。 2、中断处理：发生中断时，硬件会切换上下文。在vmstat命令中是in 3、用户和内核模式切换：当操作系统中需要在用户模式和内核模式之间进行转换时，需要进行上下文切换,比如进行系统函数调用。 Linux 为每个 CPU 维护了一个就绪队列，将活跃进程按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。也就是vmstat命令中的r。 那么，进程在什么时候才会被调度到 CPU 上运行呢？ 进程执行完终止了，它之前使用的 CPU 会释放出来，这时再从就绪队列中拿一个新的进程来运行 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片被轮流分配给各个进程。当某个进程时间片耗尽了就会被系统挂起，切换到其它等待 CPU 的进程运行。 进程在系统资源不足时，要等待资源满足后才可以运行，这时进程也会被挂起，并由系统调度其它进程运行。 当进程通过睡眠函数 sleep 主动挂起时，也会重新调度。 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 结合我们之前的内容分析，阻塞的就绪队列是100左右，而我们的CPU只有4核，这部分原因造成的上下文切换就可能会相当高，再加上中断次数是4000左右和系统的函数调用等，整个系统的上下文切换到800万也不足为奇了。Java内部的线程切换才15次，是因为线程使用Thread.yield()来让出CPU资源，但是CPU有可能继续调度该线程，这个时候线程之间并没有切换，这也是为什么内部的某个线程切换次数并不是非常大的原因。 总结本文模拟了常见的性能问题场景，分析了如何定位CPU100%、内存泄漏、死锁、线程频繁切换问题。分析问题我们需要做好两件事，第一，掌握基本的原理，第二，借助好工具。本文也列举了分析问题的常用工具和命令，希望对你解决问题有所帮助。当然真正的线上环境可能十分复杂，并没有模拟的环境那么简单，但是原理是一样的，问题的表现也是类似的，我们重点抓住原理，活学活用，相信复杂的线上问题也可以顺利解决。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat默认请求参数最大值及页面图片压缩处理方法]]></title>
    <url>%2F2020%2F03%2F24%2FtomcatSize%2F</url>
    <content type="text"><![CDATA[引：最近小编在开发一套活体流程业务，其中有这么一个功能：需要用户上送自己的身份证正反面照片，所以就得自己开发一个页面，然后页面展示用户上传的图片再以base64格式将数据传输到后台做处理。 遇到的问题点：pc端上传图片数据到后台服务器正常、而手机端上传数据服务器却接收不到数据信息。 解决思路：1、通过查阅资料发现：tomcat中默认的请求数据内容大小为：2M 2、通过抓包或者对比手机端和pc端的图片信息发现手机端的图片数据高达6M而pc端的图片却只有300KB不到，明明是同样的图片为什么数据大小会差这么多？后面发现我是通过“微信”发送图片到的电脑上，这个过程微信自己给我们的图片做了压缩处理。 3、在页面新增压缩语句处理解决问题。（当然也可以修改tomcat中的参数） 压缩代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//画布压缩图片function dealImage(base64, w, callback) &#123; var newImage = new Image(); var quality = 0.6; //压缩系数0-1之间 newImage.src = base64; // newImage.setAttribute(&quot;crossOrigin&quot;, &apos;Anonymous&apos;); //url为外域时需要 var imgWidth, imgHeight; newImage.onload = function () &#123; imgWidth = this.width; imgHeight = this.height; var canvas = document.createElement(&quot;canvas&quot;); var ctx = canvas.getContext(&quot;2d&quot;); if (Math.max(imgWidth, imgHeight) &gt; w) &#123; if (imgWidth &gt; imgHeight) &#123; canvas.width = w; canvas.height = w * imgHeight / imgWidth; &#125; else &#123; canvas.height = w; canvas.width = w * imgWidth / imgHeight; &#125; &#125; else &#123; canvas.width = imgWidth; canvas.height = imgHeight; quality = 0.6; &#125; ctx.clearRect(0, 0, canvas.width, canvas.height); ctx.drawImage(this, 0, 0, canvas.width, canvas.height); var base64 = canvas.toDataURL(&quot;image/jpeg&quot;, quality); //压缩语句 // 如想确保图片压缩到自己想要的尺寸,如要求在50-150kb之间，请加以下语句，quality初始值根据情况自定 while (base64.length / 1024 &gt; 150) &#123; quality -= 0.01; base64 = canvas.toDataURL(&quot;image/jpeg&quot;, quality); &#125; // 防止最后一次压缩低于最低尺寸，只要quality递减合理，无需考虑 while (base64.length / 1024 &lt; 50) &#123; quality += 0.001; base64 = canvas.toDataURL(&quot;image/jpeg&quot;, quality); &#125; callback(base64);//必须通过回调函数返回，否则无法及时拿到该值 &#125;&#125; dealImage(front, 800, rFont); function rFont(frontBase64) &#123; $(&quot;#front&quot;).val(frontBase64); //国徽压缩 &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle同库夸角色查表]]></title>
    <url>%2F2020%2F02%2F25%2Foraclerole%2F</url>
    <content type="text"><![CDATA[业务场景： 在A服务器上有个oracle数据库里面有2个不用的用户权限：role1和role2, 每个用户分别管理不同的表，role1下面有user表,role2下面有个city表。 此时业务需要用到user表里的某个字段去联表查询city表，查询用户是属于那个地区的。 解决方式： 1.用role1用户连接数据库执行： grant select on role1.user to role2; 即：将role1用户中的user表查询权限赋予role2. 2.用role2用户连接数据库执行： grant select on role2.city to role1; 即：将role2用户中的city 表查询权限赋予role1. 3.用户orle1查询语法： select u.name,c.cityId from user u left join orle2.city c on u.id=c.id;]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>role</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat步骤及注意事项]]></title>
    <url>%2F2020%2F02%2F25%2Ftomcatupgrade%2F</url>
    <content type="text"><![CDATA[一：检测tomcat版本：1.直接找到tomcat中bin目录下的：version.sh运行： 2.在执行version.sh时可以会提示如下错误: 这是由于jdk版本不符合导致的。 解决方法有很多比如： 1、直接在tomcat中的server.xml直接jdk的路径（不推荐） 2、修改系统环境变量 3、在执行如下语法：export JAVA_HOME=/usr/java/jdk1.8.0_131/;sh apache-tomcat-8.5.51/bin/version.sh 二：升级tomcat1、下载需要更新的Tomcat版本：https://tomcat.apache.org 2、上传至：服务器上然后解压：tar xzvf tomcat.xx.tar.gz 3、然后删除解压后的tomcat中webapps下面的内容留下root目录。 4、修改tomcat中conf目录下的server.conf文件： 4.1：禁用tomcat中的shutdown功能 原因：从安全性的角度出发禁用shutdown功能可防止服务被恶意关闭。 注：shutdown功能被禁用后，关闭服务就不能用shutdown.sh这个脚本了 ，而需要使用’kill -9 进程’这个命令关闭服务. 4.2：修改tomcat中的服务port： 确保此端口是没有被占用的： 注：使用netstat -anp| grep 端口号 可以查询端口是否占用 如上图：监控状态为LISTEN表示已经被占用 4.3：禁用AJP端口 原因：AJP端口用来在应用服务器交互时候用，比如apache链接tomcat等，一般也用不着，可以禁止掉。 直接注释掉即可； 5：此时执行复制命令把在用的tomcat下root目录及内容复制到新版本的tomcat目录root下： cp -r oldTomcat/ROOT/ /newTomcat/ROOT 6.启动tomcat查看日志是否正常启动，启动正常则kill掉服务然后修改端口改为在用的服务的端口，然后停掉旧版tomcat直接启动新版的tomcat就行。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch]]></title>
    <url>%2F2020%2F01%2F13%2FElasticsearch%2F</url>
    <content type="text"><![CDATA[何为ElasticsearchElasticsearch是分布式搜索和分析引擎 . Elasticsearch为所有类型的数据提供实时搜索和分析。无论您是结构化文本还是非结构化文本，数字数据或地理空间数据，Elasticsearch都能以支持快速搜索的方式有效地对其进行存储和索引。您不仅可以进行简单的数据检索，还可以汇总信息来发现数据中的趋势和模式。随着数据和查询量的增长，Elasticsearch的分布式特性使您的部署可以随之无缝地增长 本篇写Elasticsearch的目的利用Elasticsearch实现多维度、高效率、海量数据的搜索查询。 听起来是不是高大上？ 小编就吹吹，但是利用Elasticsearch确实可以实现，小编这里主要讲一下它的基本实现和基本功能。深入的功能还得各位在实践中探索。 需要的工具及软件Jdk8或Jdk8以上、Elasticsearch7.5、Kibana7.5、logstash7.5、Mysql. 其中ELK必须统一、jdk必须8或8以上、数据库可自行选择,小编这里选择的是Mysql. 注：elk基本搭建，本篇不做讲解请自行查看官网文档:https://www.elastic.co/ 开始步骤1在数据库随便建立一些数据如下： 步骤2编辑logstash中的管道配置文件，使其从数据库中指定的表读取数据，配置如下： 配置参数详解： jdbc_driver_library:表示mysql的jar，这个文件需要自己自行下载且版本要和自己的数据库版本一致，否则会报错。 jdbc_driver_class：这个是连接配置类，这里是mysql的案列。 jdbc_connection_string：这个是数据库链接地址：jdbc:mysql://localhost:3306/数据库名。 jdbc_user：数据库链接账号。 jdbc_password：数据库链接密码。 schedule：定时任务执行时间，即间隔多久时间执行statement的语句去刷新数据。 statement：需要查询的表。 last_run_metadata_path：查询出来的数据保存的文件。 use_column_value：当设置为true时，使用定义的tracking_column值作为：sql_last_值。当设置为false时，：sql_last_值反映上次执行查询的时间 ， tracking_cloumn：如果use_column_value设置为true，则要跟踪其值的列 – 官方文档配置地址：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html#plugins-inputs-jdbc-tracking_column 配置完后启动logstash. 步骤3打开Kibana,就可以看到如下数据面板： 这时表明数据已经读取成功了。 步骤4：简单的获取功能在Java语言中用Elasticsearch实现数据的获取. 第一步： 导入相关依赖，此处以maven为例： 1234567891011121314151617181920212223242526272829303132333435 &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.5.1&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;version&gt;7.5.1&lt;/version&gt; &lt;/dependency&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;elastic-lucene-snapshots&lt;/id&gt; &lt;name&gt;Elastic Lucene Snapshots&lt;/name&gt; &lt;url&gt;https://s3.amazonaws.com/download.elasticsearch.org/lucenesnapshots/83f9835&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 第二步，编写代码： 12345678910111213 RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(&quot;localhost&quot;, 9200, &quot;http&quot;))); GetRequest getRequest = new GetRequest(&quot;index&quot;, &quot;id&quot;);// getRequest.fetchSourceContext(FetchSourceContext.DO_NOT_FETCH_SOURCE); String[] includes = new String[]&#123;&quot;*&quot;&#125;; String[] excludes = Strings.EMPTY_ARRAY; FetchSourceContext fetchSourceContext = new FetchSourceContext(true, includes, excludes); getRequest.fetchSourceContext(fetchSourceContext); getRequest.refresh(true); GetResponse getResponse = client.get(getRequest, RequestOptions.DEFAULT); byte[] sourceAsBytes = getResponse.getSourceAsBytes(); String toStr = IOUtils.toString(sourceAsBytes, &quot;utf-8&quot;); 此时就已经完成了一个简单的获取功能。 步骤五：简单搜索功能实现123456789101112131415161718192021222324252627282930313233343536373839404142434445RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(&quot;localhost&quot;, 9200, &quot;http&quot;))); SearchRequest searchRequest = new SearchRequest(&quot;index&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchAllQuery()); searchSourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS)); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); //有关于请求执行本身，如HTTP状态代码 RestStatus status = searchResponse.status(); //执行时间或请求是否提前终止或超时的有用信息 TimeValue took = searchResponse.getTook(); Boolean terminatedEarly = searchResponse.isTerminatedEarly(); boolean timedOut = searchResponse.isTimedOut(); //响应还通过提供受搜索影响的碎片总数以及成功碎片与不成功碎片的统计数据，提供有关在碎片级别上执行的信息。也可以通过对ShardSearchFailures以外的数组进行迭代来处理可能的故障， int totalShards = searchResponse.getTotalShards(); int successfulShards = searchResponse.getSuccessfulShards(); int failedShards = searchResponse.getFailedShards();// for (ShardSearchFailure failure : searchResponse.getShardFailures()) &#123;// // failures should be handled here 故障处理逻辑业务// &#125; //要访问返回的文档，我们需要首先获取响应中包含的搜索结果 SearchHits hits = searchResponse.getHits(); //该SearchHits提供所有命中的全局信息，比如命中总数或最大比分 TotalHits totalHits = hits.getTotalHits(); //点击总数，必须在total hits.relation的上下文中解释 long numHits = totalHits.value; //命中数是准确的（等于）还是总数的下限（大于等于） TotalHits.Relation relation = totalHits.relation; float maxScore = hits.getMaxScore(); //内部嵌套SearchHits是可以遍历单独的搜索结果： SearchHit[] searchHits = hits.getHits(); JSONObject jsonObject = new JSONObject(); for (SearchHit hit : searchHits) &#123; String index = hit.getIndex(); String id = hit.getId(); float score = hit.getScore(); String sourceAsString = hit.getSourceAsString(); jsonObject.put(id,sourceAsString); &#125; log.info(&quot;source:[&#123;&#125;]&quot;, jsonObject.toJSONString()); 结果如下： 至此，简单的搜索功能已经完成。更多详情功能请参考官网：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-search.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elk]]></title>
    <url>%2F2020%2F01%2F07%2Felk%2F</url>
    <content type="text"><![CDATA[ELK搭建（收集java日志）1.何为ELK： Elasticsearch是Elastic Stack核心的分布式搜索和分析引擎。 Logstash和Beats有助于收集，聚合和丰富您的数据并将其存储在Elasticsearch中。 Kibana，使您可以交互式地探索，可视化和共享对数据的见解，并管理和监视堆栈 。 2.下载ELK安装资源： 下载资源地址：https://www.elastic.co/cn/downloads/ 3.安装环境 需要配置java的环境变量,Java版本在8或11以上 4.下载完后资源如下： 5.Elasticsearch （Logstash,kibana）在安装后不会自动启动。如何启动和停止Elasticsearch取决于您的系统是使用SysV init还是systemd 检测指令如下：ps -p 1 6.启动、查看、停止命令： systemctl start elasticsearch(logstash,kibana) systemctl status elasticsearch(logstash,kibana) sysstemctl stop elasticsearch(logstash,kibana) –当然也可以找到相关的软件安装目录执行bin/下面的相关启动脚本，查询相关软件安装地址：whereis elasticsearch(logstash,kibana) 7.配置Elasticsearch 、Logstash、kibana相关属性文件，使它们交互起来； 7.1 首先执行指令：whereis elasticsearch 找到elasticsearch的配置文件所在位置，然后编辑elasticsearch.yml文件，如图： 主要开放host和Port功能，其它的功能可自行根据的自己的需要配置，然后启动elasticsearch ； 7.2 用同样的方式找到kibana的配置文件所在位置,然后编辑kibana.yml文件，同样打开host和Port功能，然后启动kibana。 7.3 最后配置Logstash的配置文件logstash.yml将config.reload.automatic: false改为true,表示自动监听配置文件的变化及刷新，然后自己写一个或多个管道配置文件内容然后启动Logstash； 如图： 管道配置文件在哪里查询，请查看安装目录中的pipelines.yml,Logstash7.5版本的一般是在安装目前中的conf.d/下； –具体详情配置文件配置及各个软件的使用请自行查看官网文档：https://www.elastic.co/ . 8.启动后查询是否启动成功如下： 如上状态表示成功， 9.打开kibana查询数据。 $]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm]]></title>
    <url>%2F2019%2F12%2F05%2Fjvm%2F</url>
    <content type="text"><![CDATA[浅谈Could not create the Java Virtual Machine.错误如下： 12345Conflicting collector combinations in option list; please refer to the release notes for the combinations allowedError: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit. 叙述： ​ 这个问题的发生是在我把springBoot项目改造为dubbo项目时发生。当时改造完后再测试环境用脚本启动是正常的，但是在生产环境就发生了如上问题的导致项目无法运行。 脚本如下： 当时出现这个问题第一反应就是启动脚本出现了问题：直接看了Error的错误语句然后去百度，走了很多弯路。 弯路：以为是jdk版本的问题、可能是服务器空间不足，又认为可能是脚本中存在大小写或者中文格式的问题。 排查了一段时间发现都不是。后来仔细看了下错误提示，第一句才是关键：Conflicting collector combinations in option list; please refer to the release notes for the combinations allowed 这里很明显提示了你：列表收集器组合有问题(这个问题深深的告知了我遇到问题一定要仔细看错误日志、一定要仔细)。 所以问题最后的处理就是去掉图中标明地方的：-XX:+UseG1GC 重点这个脚本在测试环境是可以正常运行的，但是在生产环境就必须去掉-XX：UseG1GC才能运行。不然就会提示上面的错误。 后来百度搜索了一番有资料说明是java自身中这个UseG1GC确实存在bug。有时候正常有时候就会有问题（这个我也不知道真假）。 java官方地址： https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jsp中的绝对路径${pageContext.request.contextPath}]]></title>
    <url>%2F2019%2F11%2F19%2FpageContext%2F</url>
    <content type="text"><![CDATA[​ 为啥写这边文章?我相信很多萌新在刚刚开发的时候，也许也和我一样在部署项目到其他环境时（比如：测试环境）打开页面会提示找不到相应的外部样式代码和图片等。那么如何解决请看下面的步骤解说： 1.问题点如图： ​ ​ 图上提示：找不到相应的图片文件，从而导致页面的图片样式没有生效。 2.解决方法：pageContext.request.contextPath ​ 1.pageContext.request.contextPath等同于后台的：request.getContextPath()方法，获取项目的项目名称。如图： ​ 如上图request.getContextPath()输出的是/idap,而这个/idap这个名称则是在Tomcat中的Application context中配置的。“/”这个则表示的是：http://localhost:8080 这个是在本地跑的一个项目名配置，那么在测试环境或者生产环境我们又改如何配置项目的项目名称呢? 当然是在Tomcat中conf中的service.conf文件中配置。如下图： 这样我们就配置相关的项目数据配置名称了。 3.jsp中如何引用pageContext.request.contextPath ​ 这里使用的是c标签即在Jsp页面顶部加上如下代码： 1&lt;c:set var=&quot;ctx&quot; value=&quot;$&#123;pageContext.request.contextPath&#125;&quot;/&gt; 使用规则： 1&lt;script src=&quot;$&#123;ctx&#125;/js/wx.js&quot;&gt;&lt;/script&gt; 1&lt;img src=&quot;$&#123;ctx&#125;/images/succeed.png&quot; alt=&quot;&quot; width=&quot;80px&quot;&gt; 即在相应的src前面加上${ctx}前缀，那么这个问题就完美的解决了。简单有通俗易懂。 题外话：c标签以及pageContext.request.contextPath需要引入相应的lib,如下： 123456789101112//c标签依赖&lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt;//request.contextPath依赖&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt;&lt;/dependency&gt; —–本篇到此结束——]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jsp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Map多种用法]]></title>
    <url>%2F2019%2F11%2F18%2FMapDemo%2F</url>
    <content type="text"><![CDATA[前言： ​ 最近小编在写项目的时候碰到定义一些常用数据的问题，比如说颜色的常量有：白色、黑色、红色等等这样的话一般想到的是用：枚举类。但是我看项目中用的却是Map的另一个实现方式，所以引起了小编的好奇新大陆。那这么多废话赶快亮出代码 1.常用的HashMap： 这个Map的实现类HashMap是我们开发中常用的一个类，主要就是用Put和Get值，其中getOrDefault的方法是获取Map中是否存在Key存在则取其Value,没有则取后面自己定义的值。 2.双向DualHashBidiMap 这就是我上面说的可以用来替代枚举的一个Map,它是一个双向的Map,即它可以通过key获取value,也可以通过value获取key。 其中inverseBidiMap()方法是一个逆视图； 3.一对多MultiValueMap MultiValueMap一个key可以存放多个value，其中values()和containsValue()方法分别用来返回Map中所有的值和判断该值是否存在（boolean） 4.固定大小LRUMap LRUMap：大小固定。它不是同步的，也不是线程安全的,新增的元素个数大于允许的最大集合个数时，则会执行LRU淘汰算法。 所有的元素在LRUMap中会根据最近使用情况进行排序。最近使用的会放在元素的最前面(LRUMap是通过链表来存储元素内容). 所以LRUMap进行淘汰时只需要删除链表最后一个即可（即header.after所指的元素对象） 影响集合元素顺序的方法： 1.put 当新增加一个集合元素对象，则表示该对象是最近被访问的 2.get 操作会把当前访问的元素对象作为最近被访问的，会被移到链接表头 isFull():用来判断集合容量是否已满； 5.多个关键字创建MultiKeyMap 创建方式如下： MultiKeyMap.decorate(new LinkedMap()) MultiKeyMap.decorate(new LRUMap()) MultiKeyMap.decorate(new ReferenceMap()) ——本篇到次结束—–]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中如何查看磁盘占用率及日志文件的删除]]></title>
    <url>%2F2019%2F11%2F14%2FdfCommand%2F</url>
    <content type="text"><![CDATA[​ 别问小编为什么写这篇文章，写这篇文章肯定是有原因的 是这么回事，去年小编入职一家深圳的小型外包创业公司，当时给甲方（买过保险的大家可能会知道‘亚太财险’这家公司）做了一个小程序端的视频在线理赔功能。功能完成后在测试环境测了下没发现什么问题，只是觉得日志打印的内容有点多，当时小编也是刚刚出来实习没想那么多也不懂什么影不影响环境、数据过大导致服务器崩溃的问题。项目经理说上我就跟着上呗。 ​ 结果功能当天晚上9点左右上线，第二天一早服务器就崩溃了，一查问题发现项目的日志高达40GB,经过监控发现只要使用视频功能就会打印出海量的日志信息。后来经过排查问题也解决了是引用了一个第三方的jar导致的。 ​ 当天我在甲方现场办公一天，任务就是定时查看服务器磁盘占用率和定时清理日志文件以及查找问题，所以这就是我今天为什么写这篇的文章的原因（恰好昨天也在清理文件所以就写了这篇文件）。 df命令 df命令的基本使用格式： df 【选项】【目录或文件名】 选项 作用 -a 显示所有文件系统信息，包括系统特有的 /proc、/sysfs 等文件系统； -m 以 MB 为单位显示容量； -k 以 KB 为单位显示容量，默认以 KB 为单位； -h 使用人们习惯的 KB、MB 或 GB 等单位自行显示容量； -T 显示该分区的文件系统名称； -i 不用硬盘容量显示，而是以含有 inode 的数量来显示。 【例1】 如上：不使用任何选项的 df 命令，默认会将系统内所有的文件系统信息，以 KB 为单位显示出来。 本例中，由 df 命令显示出的各列信息的含义分别是： Filesystem：表示该文件系统位于哪个分区，因此该列显示的是设备名称； 1K-blocks：此列表示文件系统的总大小，默认以 KB 为单位； Used：表示用掉的硬盘空间大小； Available：表示剩余的硬盘空间大小； Use%：硬盘空间使用率。如果使用率高达 90% 以上，就需要额外注意，因为容量不足，会严重影响系统的正常运行； Mounted on：文件系统的挂载点，也就是硬盘挂载的目录位置。 【例2】 这里使用- h选项，因此文件系统的各种容量数据，会以人们习惯的单位（通常使用 GB 或 MB）显示出来 【例3】 这里使用 -h 选项及文件目录名。在这种情况下，df 命令会自动分析该目录所在的分区，并将所在分区的有关信息显示出来。由此，我们就可以知道，该目录下还可以使用多少容量。 【例4】 这里使用 du -sh * ，来查看当前目录下的各自文件占用内存详情。 通过上面的命令我们可以轻松的发现磁盘占用过高的文件有哪些，然后在进行相关的数据清理操作， 这个我主要说一下日志的清理方式： 方式1：rm -rf /xx/xx/xx.info —这个命令直接删除相关的文件（有个弊端：如果项目正在运行此时你执行删除命令，则可能会导致日志数据找不到相关的文件输出，从而引发相关问题） 方式2：&gt;/xx/xx/xx.info —-这个命令会直接清空给定文件的文件内容，而不会删除文件。 有些项目的日志文件可能并没有规范起来，那么它可能会一直生成的新的日志文件从而导致日志文件过多删除起来没有那么方便，那么此时可能使用方式3删除： 方式3：&gt;rm -f /xx/xx/info.19*.info 如下图 其中*号为19后面的任何字符。 .info为info结尾的文件类型 好了本篇文章到此落笔。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux df指令用法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中查看各进程资源占用状况命令Top]]></title>
    <url>%2F2019%2F11%2F12%2FLinuxTop%2F</url>
    <content type="text"><![CDATA[前段时间作者本人发布了小小的java项目，但是过了一段时间后领导发现我发布的那个项目内存占用率太高了有点反常，所以今天特写此文来记录一下相关的处理方式一遍加深印象。 首先用到的命令：Top top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。 top显示系统当前的进程和其他状况,是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止. 比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 下面详细介绍它的使用方法。 参数含义： 统计信息区前五行是系统整体的统计信息。第一行是任务队列信息 14:42:25 当前时间 up 713 days 系统运行时间，格式为天 1 user 当前登录用户数 load average: 0.00, 0.00, 0.00 系统负载即任务队列的平均长度，三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下 Tasks: total:进程总数， running:正在运行的进程数 sleeping:睡眠的进程数 stopped:停止的进程数 zombie:僵尸进程数 Cpu(s): 0.6%us:用户空间占用cpu百分比 0.2%sy:内核（系统）空间占用cpu百分比 0.0%ni:用户进程空间内改变过优先级的进程占用cpu百分比 99.2%id:空闲cpu百分比 0.1%wa:等待输入输出的cpu时间百分比 0.0%hi:硬件cpu中断占用百分比 0.0%si:软件中断占用百分比 0.0%st:虚拟机占用百分比 最后两行信息为内存信息： Mem: 80…K total:物理内存总量 79…K used:使用的物理内存总量 12…K free:空闲内存总量 9…K buffers:用在内核缓存的内存量 Swap: 41…K total: 交换区总量 37…K used:使用的交换区总量 47…K free:空闲交换区总量 54…K cached:缓冲的交换区总量 内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。 怎么看内存有多少空闲： totalFree = (Mem) 12…K free + (Mem)9…K buffers + (Swap) 54…K cached; 进程信息区统计信息区域的下方显示了各个进程的详细信息。含义如下： 序号 列名 含义a PID 进程idb PPID 父进程idc RUSER Real user named UID 进程所有者的用户ide USER 进程所有者的用户名f GROUP 进程所有者的组名g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ?h PR 优先级i NI nice值。负值表示高优先级，正值表示低优先级j P 最后使用的CPU，仅在多CPU环境下有意义k %CPU 上次更新到现在的CPU时间占用百分比l TIME 进程使用的CPU时间总计，单位秒m TIME+ 进程使用的CPU时间总计，单位1/100秒n %MEM 进程使用的物理内存百分比o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RESp SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATAr CODE 可执行代码占用的物理内存大小，单位kbs DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kbt SHR 共享内存大小，单位kbu nFLT 页面错误次数v nDRT 最后一次写入到现在，被修改过的页面数。w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程)x COMMAND 命令名/命令行y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h 默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 列。可以通过下面的快捷键来更改显示内容。 更改显示内容通过 f 键可以选择显示的内容。按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定。按 o 键可以改变列的显示顺序。按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z 可以将相应的列向左移动。最后按回车键确定。按大写的 F 或 O 键，然后按 a-z 可以将进程按照相应的列进行排序。而大写的 R 键可以将当前的排序倒转。 top的命令使用规则： top使用格式 top 【-】 【d】 【p】 【q】 【c】 【C】 【S】 【s】 【n】 参数说明 d 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。p 通过指定监控进程ID来仅仅监控某个进程的状态。q 该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。S 指定累计模式s 使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。i 使top不显示任何闲置或者僵死进程。 c 显示整个命令行而不只是显示命令名 下面介绍在top命令执行过程中可以使用的一些交互命令。从使用角度来看，熟练的掌握这些命令比掌握选项还重要一些。这些命令都是单字母的，如果在命令行选项中使用了s选项，则可能其中一些命令会被屏蔽掉。 Ctrl+L 擦除并且重写屏幕。h或者? 显示帮助画面，给出一些简短的命令总结说明。k 终止一个进程。系统将提示用户输入需要终止的进程PID，以及需要发送给该进程什么样的信号。一般的终止进程可以使用15信号；如果不能正常结束那就使用信号9强制结束该进程。默认值是信号15。在安全模式中此命令被屏蔽。i 忽略闲置和僵死进程。这是一个开关式命令。q 退出程序。r 重新安排一个进程的优先级别。系统提示用户输入需要改变的进程PID以及需要设置的进程优先级值。输入一个正值将使优先级降低，反之则可以使该进程拥有更高的优先权。默认值是10。S 切换到累计模式。s 改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s。需要注意的是如果设置太小的时间，很可能会引起不断刷新，从而根本来不及看清显示的情况，而且系统负载也会大大增加。f或者F 从当前显示中添加或者删除项目。o或者O 改变显示项目的顺序。l 切换显示平均负载和启动时间信息。m 切换显示内存信息。t 切换显示进程和CPU状态信息。c 切换显示命令名称和完整命令行。M 根据驻留内存大小进行排序。P 根据CPU使用百分比大小进行排序。T 根据时间/累计时间进行排序。 W 将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法。 常用操作: top //每隔5秒显式所有进程的资源占用情况top -d 2 //每隔2秒显式所有进程的资源占用情况top -c //每隔5秒显式进程的资源占用情况，并显示进程的命令行参数(默认只有进程名)top -p 12345 -p 6789//每隔5秒显示pid是12345和pid是6789的两个进程的资源占用情况top -d 2 -c -p 123456 //每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数 通过top命令排查了相关进程信息后那么就需要考虑为什么会导致内存占用这么高以及导致内存溢出的主要原因： 导致内存溢出的主要原因内存溢出是指应用系统中存在无法回收的内存或使用的内存过多，最终使得程序运行要用到的内存大于虚拟机能提供的最大内存。（可以从以下几个方面去思考） 引起内存溢出的原因有很多种，常见的有以下几种： 1.内存中加载的数据量过于庞大，如一次从数据库取出过多数据； 2.集合类中有对对象的引用，使用完后未清空，使得JVM不能回收； 3.代码中存在死循环或循环产生过多重复的对象实体； 4.使用的第三方软件中的BUG； 5.启动参数内存值设定的过小； 解决办法内存溢出的解决方案： ​ 第一步，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。) 第二步，检查错误日志，查看“OutOfMemory”错误前是否有其它异常或错误。 第三步，对代码进行走查和分析，找出可能发生内存溢出的位置。 重点排查以下几点： 1.检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。 2.检查代码中是否有死循环或递归调用。 3.检查是否有大循环重复产生新对象实体。 4.检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中 数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。 5.检查List、MAP等集合对象是否有使用完后，未清除的问题。List、MAP等集合对象会始终存有对对象的引用，使得这些对象不能被GC回收。 6.做好监控，持续发现内存使用率的攀升（内存查看工具有许多，比较有名的有：Optimizeit Profiler、JProbe Profiler、JinSight和Java1.5的Jconsole等）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>topCommand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql中基本的语句用法]]></title>
    <url>%2F2019%2F11%2F04%2FsqlBaseGrammar%2F</url>
    <content type="text"><![CDATA[前言之所以写这篇基础语句文章，主要是因为前段时间在接触一些比较复杂的数据语法操作时，发现自己对这方面的知识还是有些欠缺的，希望这篇初级语句文章对自己以后的职业生涯有所帮助。 本文摘自：https://juejin.im/entry/591427a22f301e006b861726 insert into 语句insert into用于向数据表中插入数据。对于标准的SQL语句而言，每次只能插入一条记录。insert into语法格式如下： 12345insert into tableName (column,clumn,..)values(value,value,..) 执行插入操作时，表名后可以用括号列出所有需要插入值的列名，而value后用括号列出对应需要插入的值。 例如： 123insert into tableName value (&apos;Vincent&apos;); 如果不想在表后用括号列出所有列，则需要为所有列指定值；如果某列的值不能确定，则为该列分配一个null值 123insert into tableName # 使用null代替主键列的值 values(null, &apos;Pigeau&apos;); 然而此时，Pigeau记录的主键列的值是2，而不是SQL语句插入的null，因为该主键列是自增长，系统会自动为该列分配值 根据外键约束规则：外键列里的值必须是被参照列里已有的值，所以向从表中插入记录之前，通常应该先向主表中插入记录，否则从表记录的外键列只能为null。现向从表student_table2中插入记录 在一些特殊的情况下，我们可以使用带子查询的插入语句，带子查询的插入语句可以一次插入多条记录 1234567insert into tableName(column1)`--使用子查询的值来插入`select column1 from tableName update语句update语句用于修改数据表的记录，每次可以修改多条记录，通过使用where子句限定修改哪些记录。没有where子句则意味着where表达式的值总是true，即该表的所有记录都会被修改，update语句的语法格式如下: 12345update tableName set column = value,column2 = value2,..[where condition] delete from 语句delete from语句用于删除指定数据表的记录。使用delete from语句删除时不需要指定列名，因为总是整行地删除。使用delete from语句可以一次删除多行，删除哪些行采用where字句限定，只删除满足where条件的记录。没有where字句限定将会把表里的全部记录删除 delete from语句的语法格式如下： 12345delete from tableName [where condition] 当主表记录被从表记录参照时，主表记录不能被删除，只有先将从表中参照主表记录的所有记录全部删除后，才可删除主表记录。还有一种情况，定义外键约束时定义了主表记录和从表记录之间的联级删除on delete cascade，或者使用on delete null用于指定当主表记录被删除时，从表中参照该记录的从表记录把外键列的值设为null 单表查询select语句的功能就是查询数据。select语句也是SQL语句中功能最丰富的语句，select语句不仅可以执行单表查询，而且可以执行多表连接查询，还可以进行子查询，select语句用于从一个或多个数据表中选出特定行、特定列的交集 单表查询的select语句的语法如下: 12345select column,column2 from tableName[where condition] 当使用select语句进行查询时，还可以在select语句中使用算术运算符(+、-、*、/)，从而形成算术表达式：使用算术表达式的规则如下 对数值型数据列、变量、常量可以使用算术运算符(+、-、*、/) 创建表达式 对日期型数据列、变量、常量可以使用部分算术运算符(+、-、)创建表达式，两个日期之间可以进行减法运算，日期和数值之间可以进行加、减运算 运算符不仅可以在列和常量、变量之间进行运算，也可以在两列之间进行运算 下面的select语句中使用了算术运算符 1234567select columnId + 5 from tableName--查询出teacher_table表中teacher_id * 3 大于4的记录 select * from teacher_table where teacher_id * 3 &gt; 4; SQL语言中算术符的优先级与java语言的运算符优先级完全相同，MySQL使用concat函数来进行字符串连接运算。 1234567--选择出teacher_name和&apos;xx&apos;字符串连接后的结果 select concat(teacher_name, &apos;xx&apos;)form tableName 对于MySQL而言，如果在算术表达式中使用null，将会导致整个算术表达式的返回值为null；如果在字符串连接运算符中出现null，将会导致连接后的结果也是null 123select concat(teacher_name, null) from teacher_table; select默认会把所有符合条件的记录全部选出来，即使两行记录完全一样。如果想去除重复行，则可以使用distinct关键字从查询结果中清除重复行，比较下面两条SQL语句的执行结果： 123456789--选出所有记录，包括重复行select column,column2 from tableName;--去除重复行select distinct column,column2 from tableName; 注：使用distinct去除重复行时，distinct紧跟select关键字，它的作用是去除后面字段组合的重复值，而不管对应对应记录在数据库是否重复 前面已经看到了where字句的作用：可以控制只选择指定的行。因为where字句里包含的是一个条件表达式，所以可以使用&gt;、&gt;=、&lt;、&lt;=、=和&lt;&gt;等基本的比较运算符。SQL中的比较运算符不仅可以比较数值之间的大小，也可以比较字符串、日期之间的大小 SQL判断两个值是否相等的比较运算符是单等号=，判断不等的运算符是&lt;&gt;；SQL中的赋值运算符不是等号，而是冒号等号(:=) SQL支持的特殊比较运算符 运算符 含义 expr1 between expr2 and expr3 要求expr1 &gt;= expr2 并且 expr2 &lt;= expr3 expr1 in(expr2,expr3,expr4,…) 要求expr1等于后面括号里任意一个表达式的值 like 字符串匹配，like后的字符串支持通配符 is null 要求指定值等于null 下面的SQL语句选出id大于等于2，且小于等于4的所有记录. 123456789select * from tableName where id between 2 and 4;-- 选出java_teacher小于等于2，student_id大于等于2的所有记录 select * from tableName where 2 between java_teacher and student_id 使用in比较运算时，必须在in后的括号里列出一个或多个值，它要求指定列必须与in括号里任意一个值相等 1234567--选出id 列的值为2或4的所有记录 select * from tableNamewhere id in (2,4); 与之类似的是，in括号里的值既可以是常量，也可以是变量或者列名 1234567--选出student_id、java_teacher列的值为2的所有记录 select * from tableNamewhere 2 in (student_id,java_teacher) like运算符主要用于进行模糊查询，例如，若要查询名字以“孙”开头的所有记录，这就需要用到迷糊查询，在模糊查询中需要使用like关键字。SQL语句中可以使用两个通配符：下划线(_)和百分号(%)，其中下划线可以代表一个任意的字符，百分号可以代表任意多个字符。如下SQL语句将查询出所有学生中名字以”孙”开头的学生 12345select * from tableNamewhere name like &apos;孙%&apos; 下面的SQL语句将查出名字为两个字符的所有姓名 123456 select * from tableNamewhere nale like &apos;__&apos; 在某些特殊情况下，查询的条件里需要使用下划线或百分号，不希望SQL把下划线和百分号当成通配符使用，这就需要使用转义字符，MySQL使用反斜线(/)作为转义字符 1234567--选出所有名字以下划线开头的学生 select * from tableName where name like &apos;/_%&apos; is null 用于判断某些值是否为空，判断是否为空不能用=null来判断，因为SQL中null=null返回null。如下SQL语句将选择出student_table表中student_name为null的所有记录 12345select * from tableNamewhere columnName is null; 如果where字句后面有多个条件需要组合，SQL提供了and和or逻辑运算符来组合2个条件，并提供了not来对逻辑表达式求否，如下SQL语句将选出学生名字为2个字符，且student_id 大于3的所有记录。 12345select * from tableName select * from student_table where student_name like &apos;__&apos; and studnent_id &gt; 3; 下面的SQL语句将选出student_table表中姓名不以下划线开头的所有记录。 1234567select * from student_table -- 使用not对where条件取否 where not student_name like &apos;/_%&apos;; SQL中比较运算符、逻辑运算符的优先级 所有比较运算符&gt;not&gt;and&gt;or order by语句执行查询后的结果默认按插入顺序排序；如果需要在查询结果按某列值的大小进行排序，则可以使用order by字句 ORDER BY 语句用于根据指定的列对结果集进行排序。ORDER BY 语句默认按照升序对记录进行排序。如果您希望按照降序对记录进行排序，可以使用 DESC 关键字 123order by column1 [asc], column2 [desc]... 进行排序时默认按升序排序排列，如果强制按降序排序，则需要在列后使用desc关键字(与之对应的是asc关键字，用不用该关键字的效果完全一样，因为默认是按升序排列)。上面语法中设定排序列时可采用列名、序列名和列别名。如下SQL语句选出student_table表中的所有记录，选出后按java_teacher列的升序排列。 数据库函数每个数据库都会在标准的SQL基础上扩展一些函数，这些函数用于进行数据处理或复杂计算，他们通常对一组数据进行计算，得到最终需要的输出结果。函数一般都会有一个或者多个输入，这些输入被称为函数的参数，函数内部会对这些参数进行判断和计算，最终只有一个值作为返回值。函数可以出现在SQL语句中的各个位置，比较常用的位置是select之后的where子句中 根据函数对多行数据的处理方式，函数被分为单行函数和多行函数，单行函数对每行输入值单独计算，每行得到一个计算结果返回给用户；多行函数对多行输入值整体计算，最后只会得到一个结果 SQL中的函数和java语言中的方法有点相似，但SQL中的函数是独立的程序单元，也就是说，调用函数时无需使用任何类、对象作为调用者，而是直接执行函数。如下： 多行函数也称为聚集函数、分组函数，主要用于完成一些统计功能，在大部分数据库中基本相同。但不同数据库中的单行函数差别非常大，MySQL中的单行函数具有如下特征 单行函数的参数可以是变量、常数或数据列。单行函数可以接收多个参数，但只返回一个值 单行函数会对每行单独起作用，每行(可能包括多个参数)返回一个结果 使用单行函数可以改变参数的数据类型。单行函数支持嵌套使用，即内层函数的返回值是外层函数的参数 MySQL的单行函数分类如图所示 MySQ数据库的数据类型大致分为数值型、字符型、和日期时间型。所以mysql分别提供了对应的函数。转换函数主要负责完成类型转换，其他函数又大致分为如下几类 位函数 流程控制函数 加密解密函数 信息函数 12345678910111213141516171819202122232425--选出teacher_table表中teacher_name列的字符长度select char_length(teacher_name) from teacher_table; --计算teacher_name列的字符长度的sin值 select sin(char_length(teacher_name)) from teacher_table; --为指定日期添加一定的时间，在这种用法下interval是关键字，需要一个数值还有一个单位 select DATE_ADD(&apos;1998-01-02&apos;, interval 2 MONTH); --获取当前日期 select CURDATE(); --获取当前时间 select curtime(); --下面的MD5是MD5加密函数 select MD5(&apos;testing&apos;); MySQL提供了如下几个处理null的函数 ifnull(expr1, expr2)：如果expr1为null，则返回expr2，否则返回expr1 nullif(expr1, expr2)：如果expr1和expr2相等，则返回null，否则返回expr1 if(expr1, expr2, expr3)：有点类似于?:三目运算符，如果expr1为true，不等于0，且不等于null，则返回expr2，否则返回expr3 isnull(expr1)：判断expr1是否为null，如果为null则返回true，否则返回false 12345678910111213-- 如果student_name列为null，则返回&apos;没有名字&apos; select ifnull(student_name, &apos;没有名字&apos;) from student_table; -- 如果CTO_name列为&apos;吴局&apos;，则返回null select nullif(CTO_name, &apos;吴局&apos;) from CTO_table; --如果student_name列为null，则返回&apos;没有名字&apos;，否则返回&apos;有名字&apos; select if(isnull(student_name), &apos;没有名字&apos;, &apos;有名字&apos;) from student_table; case函数case函数，流程控制函数。case函数有两个用法 case函数第一个用法的语法 12345678910111213case valuewhen compare_calue1 then result1when compare_calue2 then result2...else resultend case函数用value和后面的compare_value1、compare_value2、…依次进行比较，如果value和指定的compare_value1相等，则返回对应的result1，否则返回else后的result case函数的第二个用法的语法 12345678910111213case when condition1 then result1where condition2 then result2...else resultend condition返回boolean值的条件表达式 123456789101112131415--id小于3的为初级工程师，3~6为中级工程师，其他为高级工程师 select name, casewhen id&lt;=3 then &apos;初级工程师 &apos;when id &lt;=6 then &apos;中级工程师 &apos;else &apos;高级工程师&apos; end from tableName; 分组和组函数组函数也就是前面提到的多行函数，组函数是将一组作为整体计算，每组记录返回一个结果，而不是每条记录返回一个结果 avg([distinct|all]expr)：计算多行expr平均值，其中expr可以是变量、常量或者数据列，但其数据类型必须是数值型。使用distinct表明不计算重复值；all表明需要计算重复值 count({|[distinct|all] expr})：计算多行expr的总条数，其中expr可以是变量、常量或者数据列，但其数据类型必须是数值型。用星号（）表示统计该表内的记录行数 max(expr)：计算多行expr的最大值 min(expr)：计算多行expr的最小值 sum([distanct|all]expr)：计算多行expr的总和 12345678910111213141516171819202122232425262728293031323334-- 计算student_table表中的记录条数select count(*)-- 计算java_teacher列总共有多少个值select count(distinct java_teacher)-- 统计所有student_id 的总和select sum(student_id)-- 计算的结果是20 * 记录的行数select sum(20)-- 选出student_table表中student_id最大的值select max(student_id)-- 选出student_table表中student_id最小的值select min(student_id)-- 因为sum里的expr是常量23，所以每行的值都相同-- 使用distinct强制不计算重复值，所以下面计算结果为23select sum(distinct 23)-- 使用count统计记录行数，null不会被计算在内select count(student_name)-- 对于可能出现null的列，可以使用ifnull函数来处理该列-- 计算java_teacher列所有记录的平均值select avg(ifnull(java_teacher, 0))from student_table;-- distinct和*不可同时使用 group by语句组函数会把所有记录当成一组，为了对记录进行显式分组，可以在select语句后使用group by子句后通常跟一个或多个列名，表明查询结果根据一列或多列进行分组——当一列或多列组合的值完全相同时，系统会把这些记录当成一组 sql group by 语法 123select column1 from tableName group by column1; 如果对多列进行分组，则要求多列的值完全相同才会被当成一组 1234567select count(*) from tableName-- 将column、column1两列的值完全相同时才会被当成一组 group by column,column1; having语句如果需要对分组进行过滤，则应该使用having子句，having子句后面也是一个条件表达式，只有满足该条件表达式的分组才会被选出来。having子句和where子句非常容易混淆，它们都有过滤功能，但它们有如下区别 不能在where子句中过滤组，where子句仅用于过滤行。过滤组必须使用having子句 不能在where子句中使用组函数，having子句才可使用组函数 sql having 语法 1234567select column,column1 from tableNamewhere [contidtion]group by column having [operator value] 多表连接查询以下book与student数据表： 交叉连接（cross join） 交叉连接无须任何连接条件。返回左表中的所有行，左表中的每一行与右表中的所有行组合。交叉联接也称作笛卡尔积 12345select * from book as a cross join stu as border by a.id 自然连接 自然连接会以两个表中的同名列作为连接条件；如果两个表没有同名列，则自然连接与交叉连接效果完全一样——因为没有连接条件。 12345select s.*,teacher_name from student_table snatural join teacher_table t; 在连接条件中使用等于(=)运算符比较被连接列的列值，但它使用选择列表指出查询结果集合中所包括的列，并删除连接表中的重复列 使用using子句的连接 using子句可以指定一列或多列，用于显示指定两个表中的同名列作为连接条件。假设两个表中有超过一列的同名列，如果使用natural join，则会把所有的同名列当成连接条件；使用using子句，就可显示指定使用哪些同名列作为连接条件 12345select s.*,teacher_name from student_table sjoin teacher_table t using(teacher id) 使用on子句的连接 最常用的的连接方式，而且每个on子句只指定一个连接条件。这意味着：如果需要进行N表连接，则需要有N-1个join…on对 1234567select s.*, teacher_name from student_table sjoin teacher_table ton s.java_teacher = t.teacher_id; 全外连接或者左、右外链接这三种外连接分别使用left[outer]join、right[outer]join和full[outer]join，这三种外连接的连接条件一样通过on子句来指定，既可以是等值连接条件，也可以是非等值连接条件 左联接 是以左表为基准，将a.stuid = b.stuid的数据进行连接，然后将左表没有的对应项显示，右表的列为null 1234567select * from book as aleft join stu as b on a.id = b. id; 右联接和左联接一样：right join on 子查询子查询就是在查询语句中嵌套另一个查询，子查询可以支持多层嵌套。对于一个普通的查询语句而言，子查询可以出现在两个位置 form语句后当成数据表，这种用法也被称为行内视图，因为该子查询的实质就是一个临时视图 where条件后作为过滤条件的值 使用子查询时的注意点 子查询要用括号括起来 把子查询作为数据表时（出现在from后），可为其起别名，作为前缀来限定数据列时，必须给子查询起别名 把子查询作为过滤条件时，将子查询放在比较运算符的右边，可增强查询的可读性 把子查询作为过滤条件时，单行子查询使用单行运算符，多行子查询使用多行运算符 12345select * from (select * from tableName) t where t.column &gt; 1; listagg函数oracle的 listagg() WITHIN GROUP () 行转列函数的使用。 即将多行字段数据记录到一行字段中如图： 上图有2个相同的流水号，但它们的接口返回信息不同，现有业务需要将同一流水号下的接口返回信息组织在一起，那么就需要使用到此函数： 12345678 SELECT listagg(ERROR_MSG, &apos;,&apos;) within group(order by JOB_TIME) as 接口返回信息 FROM table WHERE condition 其中：listagg(&apos;需要拼接的列名&apos;,&apos;拼接符号如：，&apos;) 根据表名定位所以在库123SELECT table_schema FROM information_schema.TABLES WHERE table_name = &apos;t_sys_user&apos;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[何为rpc]]></title>
    <url>%2F2019%2F10%2F15%2Frpc%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javazhiyin.com/17912.html 。 一：什么是RPC1.什么是RPC RPC是基于JDK动态代理。 RPC是Remote Procedure Call的缩写，即远程过程调用，意思是可以在一台机器上调用远程的服务。在非分布式环境下，我们的程序调用服务都是本地调用，但是随着分布式结构的普遍，越来越多的应用需要解耦，将不同的独立功能部署发布成不同的服务供客户端调用，RPC就是为了解决这个问题的。 2.RPC原理 如何去调用一个远程的服务？ ①肯定要知道IP和端口吧（确定唯一一个进程）②肯定要知道调用什么服务吧（方法名和参数）③调用服务后可能需要结果吧。这三点又怎么实现呢？往下看： RPC的设计由Client，Client stub，Network ，Server stub，Server构成。其中Client就是用来调用服务的，Cient stub是用来把调用的方法和参数序列化的（因为要在网络中传输，必须要把对象转变成字节），Network用来传输这些信息到Server stub， Server stub用来把这些信息反序列化的，Server就是服务的提供者，最终调用的就是Server提供的方法。 RPC的结构如下图： 图中1-10序号的含义如下： Client像调用本地服务似的调用远程服务； Client stub接收到调用后，将方法、参数序列化 客户端通过sockets将消息发送到服务端 Server stub 收到消息后进行解码（将消息对象反序列化） Server stub 根据解码结果调用本地的服务 本地服务执行(对于服务端来说是本地执行)并将结果返回给Server stub Server stub将返回结果打包成消息（将结果消息对象序列化） 服务端通过sockets将消息发送到客户端 Client stub接收到结果消息，并进行解码（将结果消息发序列化） 客户端得到最终结果. 二,做个简单的RPC程序，这里使用Dubbo做demo 首先填充下节点角色： Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 这里关于消费端（客户端）和服务端的配置文件就不写，主要写一下服务的暴露，实现和调用 1.服务端接口暴露： 1234public interface xService &#123; String upPicture(String cityId) throws Exception;&#125; 2.服务端接口的实现 1234567@Slf4j@Service(interfaceClass = CloudService.class)public class xServiceImpl implements xService &#123; public String upPicture(String cityId) throws Exception &#123; log.info(&quot;xxx&quot;); &#125;&#125; 3.消费端（客户端）远程调用 1234567public class Cust &#123; @Reference private xService xservice; public void custXservice()&#123; xservice.upPicture(&quot;0755&quot;); &#125;&#125; 这样一个简单的rpc服务就完成了。 其中@Service，@Reference分别是dubbo中的接口，分别表示注解暴露服务、注解引用服务]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok基本用法]]></title>
    <url>%2F2019%2F10%2F08%2FLombok%2F</url>
    <content type="text"><![CDATA[这里主要抒写下Lombok中许多常用的方法：首先引入相关依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Lombok注解 @Data：注解在类上，将类提供的所有属性都添加get、set方法，并添加、equals、canEquals、hashCode、toString方法 @Setter：注解在类上，为所有属性添加set方法、注解在属性上为该属性提供set方法 @Getter：注解在类上，为所有的属性添加get方法、注解在属性上为该属性提供get方法 @NotNull：在参数中使用时，如果调用时传了null值，就会抛出空指针异常 @Synchronized 用于方法，可以锁定指定的对象，如果不指定，则默认创建一个对象锁定 @Log作用于类，创建一个log属性 @Builder：使用builder模式创建对象 @NoArgsConstructor：创建一个无参构造函数 @AllArgsConstructor：创建一个全参构造函数 @ToString：创建一个toString方法 @Accessors(chain = true)使用链式设置属性，set方法返回的是this对象。 @RequiredArgsConstructor：创建对象, 例: 在class上添加@RequiredArgsConstructor(staticName = “of”)会创建生成一个静态方法 @UtilityClass:工具类 @ExtensionMethod:设置父类 @FieldDefaults：设置属性的使用范围，如private、public等，也可以设置属性是否被final修饰。 @Cleanup: 关闭流、连接点。 @EqualsAndHashCode：重写equals和hashcode方法。 @toString：创建toString方法。 @Cleanup: 用于流等可以不需要关闭使用流对象. 这里主要说下@Accessors(chain = true)，使用链式创建 1.常规的实体赋值如下： 可以明显的看出实例化一个指定的类然后用类名.属性名去复制。 2.现在用注解@Accessors(chain = true)，这个注解来修饰类： 可以看出来使用了改注解后可以直接使用类名.属性名直接set数据，而不用每个数据都去使用 这样代码的整洁和布局也会有进一步的提升。 3.顺带说一下清理流对象的注解： @Cleanup 自动资源管理：没有麻烦和安全地调用您的close()方法。 您可以使用@Cleanup以确保在代码执行路径退出当前作用域之前自动清除给定资源。您可以通过使用@Cleanup注释任何局部变量声明来执行此操作： @Cleanup InputStream in = new FileInputStream(&quot;xx/file&quot;); 结果，在您作用域范围的末尾调用in.close()。保证通过try / finally构造运行此调用。 如果要清理的对象类型没有close()方法，但是有其他一些无参数方法，则可以指定此方法的名称，如下所示： @Cleanup(&quot;dispose&quot;) org.eclipse.swt.widgets.CoolBar bar = new CoolBar(parent, 0); 默认情况下，清除方法被假定为close()。@Cleanup无法调用带有1个或多个参数的清理方法。 1.使用@Cleanup注解 ： 2.常规的处理方法： 单单从代码的行数上面就可以知道已经精简了不少，同时，代码的可读性也进一步提高。从代码中我们可以容易的看出，@Cleanup的作用就是在当前变量不在有效范围内的时候，对其进行自动的资源回收。在Java中的Stream上使用Cleanup Annotation，就是对其调用close方法 Cleanup 存在一个小问题： 官网给出了提示，如果你的代码中出现了异常，那么会触发cleanup方法抛出异常，导致把原始异常吞掉，这样就导致你在上层不知道发生了什么事情,但是如果说你是在调用close方法的时候出了异常，那么Cleanup这个annotation是不会把异常吞掉的。 https://objectcomputing.com/resources/publications/sett/january-2010-reducing-boilerplate-code-with-project-lombok]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootMavenWeb，Maven中打包没有web.xml的Web项目]]></title>
    <url>%2F2019%2F09%2F29%2FSpringBootMavenWeb%2F</url>
    <content type="text"><![CDATA[关于在mavne中package,SpringBootWeb项目可能出现如下的错误提示： 那是因为在WEB-INF下找不到web.xml文件， 那么此时的解决方法有很多我这里就只提供如下一种：即在Pom.xml文件中添加相关语句： 12345678910111213141516171819&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;configuration&gt; &lt;!-- 不忽略空文件夹、空包 --&gt; &lt;includeEmptyDirectories&gt;true&lt;/includeEmptyDirectories&gt; &lt;!-- 忽略 web.xml 配置文件 --&gt; &lt;failOnMissingWebXml&gt;false&lt;/failOnMissingWebXml&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 这样就可以正常的package了]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sha1签名方法]]></title>
    <url>%2F2019%2F09%2F27%2Fsha1%2F</url>
    <content type="text"><![CDATA[这里主要简单讲一下sha1签名方法，相信接触微信端开发的人员，在调用微信公众号及小程序的部分接口时需要做sha1签名。 虽然官方中有明显的说明但是还是有部分朋友还是会问到或这感到困惑，这里就详细给出一个demo。cp修改部分参数即可用。 第一步：引入goole.guava架包 123456789&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;21.0&lt;/version&gt;&lt;/dependency&gt; 最后一步:封装相关签名方法方便调用 12345678910111213141516171819202122232425public String createOcrSign(String nonceTicket, String userId, String orderNo, String nonce, String appId, String version) &#123; //字典排序 List&lt;String&gt; signParams = new ArrayList&lt;&gt;( Arrays.asList(version, orderNo, appId, nonce, userId, nonceTicket)); //生成签名 String sign = CommonUtils.computeSign(signParams); return sign;&#125;public static String computeSign(List&lt;String&gt; values) &#123; if (values == null) &#123; throw new NullPointerException(&quot;values is null&quot;); &#125; values.removeAll(Collections.singleton(null));// remove null Collections.sort(values); StringBuilder sb = new StringBuilder(); for (String s : values) &#123; sb.append(s); &#125; return Hashing.sha1().hashString(sb, Charsets.UTF_8).toString().toUpperCase();&#125; ———到此结束]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>sha1签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootWeb项目独立在tomact中运行]]></title>
    <url>%2F2019%2F09%2F27%2FSpringBootExternalTomcat%2F</url>
    <content type="text"><![CDATA[SpringBootWeb项目独立在tomact中运行​ SpringBoot项目本身就内置了tomcat所以我们可以一键启动项目，但是在生产环境下我们还是习惯将项目放在独立的Tomact中运行，下面我来介绍如何将项目部署到第三方tomcat中运行。 1.修改Pom文件中的packaging属性将其改为：war 2.在Pom文件声明其父容器为SpringBoot 1234567891011&lt;parent&gt;​ &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;​ &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;​ &lt;version&gt;x.y.z&lt;/version&gt;&lt;/parent&gt; 3.在pom文件中声明tomcat的运行范围 1234567891011&lt;dependency&gt;​ &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;​ &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;​ &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 4.创建启动文件 123456789101112131415161718192021@SpringBootApplicationpublic class App extends SpringBootServletInitializer &#123; /** * for War Tomcat部署到外部servlet容器,需要继承SpringBootServletInitializer并重写configure() */ @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(App.class); &#125; /** * for JAR deploy */ public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; ———————–至此结束]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式id生成器 SnowFlake]]></title>
    <url>%2F2019%2F09%2F24%2FSnowFlake%2F</url>
    <content type="text"><![CDATA[分布式id生成器简单讲解下二进制 1.1 二进制概念 123二进制是计算技术中广泛采用的一种数制。二进制数据是用0和1两个数码来表示的数。 它的基数为2，进位规则是“逢二进一”，借位规则是“借一当二”，由18世纪德国数理哲学大师莱布尼兹发现。 当前的计算机系统使用的基本上是二进制系统，数据在计算机中主要是以补码的形式存储的。 计算机中的二进制则是一个非常微小的开关，用“开”来表示1，“关”来表示0。 1.2 运算法则 12345二进制的运算算术运算 二进制的加法：0+0=0，0+1=1 ，1+0=1， 1+1=10(向高位进位)；例：7=111;10=1010;3=11 二进制的减法：0-0=0，0-1=1(向高位借位) 1-0=1，1-1=0 (模二加运算或异或运算) ； 二进制的乘法：0 * 0 = 0; 0 * 1 = 0; 1 * 0 = 0; 1 * 1 = 1; 二进制的除法：0÷0 = 0，0÷1 = 0，1÷0 = 0 (无意义)，1÷1 = 1 ； 逻辑运算二进制的或运算：遇1得1 二进制的与运算：遇0得0 二进制的非运算：各位取反。 1.3 位 123数据存储的最小单位。每个二进制数字0或者1就是1个位； 1.4 字节 12345678910118个位构成一个字节；即：1 byte (字节)= 8 bit(位)； 1 KB = 1024 B(字节)； 1 MB = 1024 KB; (2^10 B) 1 GB = 1024 MB; (2^20 B) 1 TB = 1024 GB; (2^30 B) 1.5 字符 12345 一般 utf-8 编码下，一个汉字 字符 占用 3 个 字节；一般 gbk 编码下，一个汉字 字符 占用 2 个 字节； 1.6 java语音的8大基本数据类型 123456789101112| 数据类型 | 字节长度(单位：byte) | 位长度（单位：bit) || -------- | -------------------- | ------------------ || int | 4 | 32 || byte | 1 | 8 || short | 2 | 16 || long | 8 | 64 || float | 4 | 32 || double | 8 | 64 || boolean | 1 | 8 || char | 2 | 16 | 1.7 二进制原码、反码、补码 123456789101112131415 对于有符号数而言：​ (1)二进制的最高位是符号位：0表示正数，1表示负数​ (2)正数的原码、反码、补码都一样；​ (3)负数的反码 = 它的原码符号位不变，其他位取反（0 -&gt;1 ; 1-&gt;0 ）；​ (4)负数的补码 = 它的反码 +1；​ (5)0的反码、补码都是0；​ (6)在计算机运算的时候，都是以补码的方式来运算的； 1.8 二进制转十进制 1234567要从右到左用二进制的每个数去乘以2的相应次方（次方要从0开始算起）；假如：[二进制](http://baike.baidu.com/view/18536.htm)数1101转化成[十进制](http://baike.baidu.com/view/359301.htm) ，那么 1101 = 1*20+0*21+1*22+1*23 = 1+0+4+8 = 13； 注意：任何数的0[次方](http://baike.baidu.com/view/1477879.htm)都是1。 2.位运算符：左移、右移 12345678910111213141516171819\&gt;&gt; :右移 最高位是0，左边补齐0;最高为是1，左边补齐1&lt;&lt; :左移 左边最高位丢弃，右边补齐0\&gt;&gt;&gt;：无符号右移 无论最高位是0还是1，左边补齐0在数字没有溢出的前提下，对于正数和负数，左移一位都相当于乘以2的1次方，左移n位就相当于乘以2的n次方右移一位相当于除2，右移n位相当于除以2的n次方。12&gt;&gt;1 结果：6 12/2^112&gt;&gt;2 结果：3 12/2^2 12&lt;&lt;1 结果 ：24 ` 12x2^112&lt;&lt;2 结果 ：48 12x2^2 ——————————————————- 在高并发或者分表分库情况下怎么保证数据id的幂等性经常用到的解决方案有以下几种。 微软公司通用唯一识别码（UUID） Twitter公司雪花算法（SnowFlake） 基于数据库的id自增 对id进行缓存 这里主要谈snowflake算法： snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号，最后还有一个符号位，永远是0。 snowflake算法所生成的ID结构 整个结构是64位，所以我们在Java中可以使用long来进行存储。 该算法实现基本就是二进制操作,单机每秒内理论上最多可以生成1024*(2^12)，也就是409.6万个ID(1024 X 4096 = 4194304) 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 * 1位标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0 41位时间截(毫秒级)，注意，41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截) 得到的值），这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的（如下下面程序IdWorker类的startTime属性）。41位的时间截，可以使用69年，年T = (1L &lt;&lt; 41) / (1000L 60 60 24 * 365) = 69 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId 12位序列，毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号 加起来刚好64位，为一个Long型。 SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。 SnowFlake算法的优点： 1.生成ID时不依赖于DB，完全在内存生成，高性能高可用。 2.ID呈趋势递增，后续插入索引树的时候性能较好。 2.所有生成的id按时间趋势递增 整个分布式系统内不会产生重复id（因为有datacenterId和workerId来做区分） SnowFlake算法的缺点： 依赖于系统时钟的一致性。如果某台机器的系统时钟回拨，有可能造成ID冲突，或者ID乱序 算法代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public class SnowflakeIdWorker &#123; // ==============================Fields================== /** 开始时间截 (2019-08-06) */ private final long twepoch = 1565020800000L; /** 机器id所占的位数 */ private final long workerIdBits = 5L; /** 数据标识id所占的位数 */ private final long datacenterIdBits = 5L; /** 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** 支持的最大数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** 序列在id中占的位数 */ private final long sequenceBits = 12L; /** 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** 数据标识id向左移17位(12+5) */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** 时间截向左移22位(5+5+12) */ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** 工作机器ID(0~31) */ private long workerId; /** 数据中心ID(0~31) */ private long datacenterId; /** 毫秒内序列(0~4095) */ private long sequence = 0L; /** 上次生成ID的时间截 */ private long lastTimestamp = -1L; //==============================Constructors==================== /** * 构造函数 * @param workerId 工作ID (0~31) * @param datacenterId 数据中心ID (0~31) */ public SnowflakeIdWorker(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;worker Id can&apos;t be greater than %d or less than 0&quot;, maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;datacenter Id can&apos;t be greater than %d or less than 0&quot;, maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; // ==============================Methods================================= /** * 获得下一个ID (该方法是线程安全的) * @return SnowflakeId */ public synchronized long nextId() &#123; long timestamp = timeGen(); //如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException( String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;, lastTimestamp - timestamp)); &#125; //如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; //毫秒内序列溢出 if (sequence == 0) &#123; //阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; //时间戳改变，毫秒内序列重置 else &#123; sequence = 0L; &#125; //上次生成ID的时间截 lastTimestamp = timestamp; //移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // | (datacenterId &lt;&lt; datacenterIdShift) // | (workerId &lt;&lt; workerIdShift) // | sequence; &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; /** * 返回以毫秒为单位的当前时间 * @return 当前时间(毫秒) */ protected long timeGen() &#123; return System.currentTimeMillis(); &#125; //==============================Test============================================= /** 测试 */ public static void main(String[] args) &#123; SnowflakeIdWorker idWorker = new SnowflakeIdWorker(0, 0); for (int i = 0; i &lt; 1000; i++) &#123; long id = idWorker.nextId(); System.out.println(Long.toBinaryString(id)); System.out.println(id); &#125; &#125;&#125; 快速使用Snowflake算法的方式： 引入hutool依赖 1234567891011&lt;dependency&gt;​ &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-captcha&lt;/artifactId&gt;​ &lt;version&gt;$&#123;hutool.version&#125;&lt;/version&gt; &lt;/dependency&gt; ID生成器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class IdGenerator &#123; private long workerId = 0; @PostConstruct void init() &#123; try &#123; workerId = NetUtil.ipv4ToLong(NetUtil.getLocalhostStr()); log.info(&quot;当前机器 workerId: &#123;&#125;&quot;, workerId); &#125; catch (Exception e) &#123; log.warn(&quot;获取机器 ID 失败&quot;, e); workerId = NetUtil.getLocalhost().hashCode(); log.info(&quot;当前机器 workerId: &#123;&#125;&quot;, workerId); &#125; &#125; /** * 获取一个批次号，形如 2019071015301361000101237 * &lt;p&gt; * 数据库使用 char(25) 存储 * * @param tenantId 租户ID，5 位 * @param module 业务模块ID，2 位 * @return 返回批次号 */ public synchronized String batchId(int tenantId, int module) &#123; String prefix = DateTime.now().toString(DatePattern.PURE_DATETIME_MS_PATTERN); return prefix + tenantId + module + RandomUtil.randomNumbers(3); &#125; @Deprecated public synchronized String getBatchId(int tenantId, int module) &#123; return batchId(tenantId, module); &#125; /** * 生成的是不带-的字符串，类似于：b17f24ff026d40949c85a24f4f375d42 * * @return */ public String simpleUUID() &#123; return IdUtil.simpleUUID(); &#125; /** * 生成的UUID是带-的字符串，类似于：a5c8a5e8-df2b-4706-bea4-08d0939410e3 * * @return */ public String randomUUID() &#123; return IdUtil.randomUUID(); &#125; private Snowflake snowflake = IdUtil.createSnowflake(workerId, 1); public synchronized long snowflakeId() &#123; return snowflake.nextId(); &#125; public synchronized long snowflakeId(long workerId, long dataCenterId) &#123; Snowflake snowflake = IdUtil.createSnowflake(workerId, dataCenterId); return snowflake.nextId(); &#125; /** * 生成类似：5b9e306a4df4f8c54a39fb0c * &lt;p&gt; * ObjectId 是 MongoDB 数据库的一种唯一 ID 生成策略， * 是 UUID version1 的变种，详细介绍可见：服务化框架－分布式 Unique ID 的生成方法一览。 * * @return */ public String objectId() &#123; return ObjectId.next(); &#125;&#125; 测试类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class IdGeneratorTest &#123; @Autowired private IdGenerator idGenerator; @Test public void testBatchId() &#123; for (int i = 0; i &lt; 100; i++) &#123; String batchId = idGenerator.batchId(1001, 100); log.info(&quot;批次号: &#123;&#125;&quot;, batchId); &#125; &#125; @Test public void testSimpleUUID() &#123; for (int i = 0; i &lt; 100; i++) &#123; String simpleUUID = idGenerator.simpleUUID(); log.info(&quot;simpleUUID: &#123;&#125;&quot;, simpleUUID); &#125; &#125; @Test public void testRandomUUID() &#123; for (int i = 0; i &lt; 100; i++) &#123; String randomUUID = idGenerator.randomUUID(); log.info(&quot;randomUUID: &#123;&#125;&quot;, randomUUID); &#125; &#125; @Test public void testObjectID() &#123; for (int i = 0; i &lt; 100; i++) &#123; String objectId = idGenerator.objectId(); log.info(&quot;objectId: &#123;&#125;&quot;, objectId); &#125; &#125; @Test public void testSnowflakeId() &#123; ExecutorService executorService = Executors.newFixedThreadPool(20); for (int i = 0; i &lt; 20; i++) &#123; executorService.execute(() -&gt; &#123; log.info(&quot;分布式 ID: &#123;&#125;&quot;, idGenerator.snowflakeId()); &#125;); &#125; executorService.shutdown(); &#125;&#125; 本文转自：https://juejin.im/post/5d8882d8f265da03e369c063]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>snowFlake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootFilter]]></title>
    <url>%2F2019%2F09%2F24%2FSpringBootFilter%2F</url>
    <content type="text"><![CDATA[关于SpringBoot中Filter的使用，此处以添加XSS注入，SQL注入过滤为demo可能需要的lib 123456789&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt;&lt;/dependency&gt; 1.创建一个过滤类用来继承Filter如下： 123456789101112131415161718public class XssFilter implements Filter &#123; @Override public void init(FilterConfig arg0) throws ServletException &#123;&#125; @Override public void doFilter(ServletRequest arg0, ServletResponse arg1, FilterChain arg2) throws IOException, ServletException &#123; arg2.doFilter(new XssServletRequest((HttpServletRequest) arg0), arg1); &#125; @Override public void destroy() &#123;&#125;&#125; 2.创建XssServletRequest类用来处理相关的xss和sql注入过滤过处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class XssServletRequest extends HttpServletRequestWrapper &#123; XssServletRequest(HttpServletRequest request) &#123; super(request); &#125; @Override public String getParameter(String name) &#123; String value = super.getParameter(name); if (StringUtils.isNotBlank(value)) &#123; value = escape(value); &#125; return value; &#125; /** * spring的@RequestParam注解通过这个获取的参数值 */ @Override public String[] getParameterValues(String name) &#123; String[] parameters = super.getParameterValues(name); if (parameters == null || parameters.length == 0) &#123; return parameters; &#125; for (int i = 0; i &lt; parameters.length; i++) &#123; parameters[i] = escape(parameters[i]); &#125; return parameters; &#125; @Override public Map&lt;String, String[]&gt; getParameterMap() &#123; Map&lt;String, String[]&gt; map = new LinkedHashMap&lt;&gt;(); Map&lt;String, String[]&gt; parameters = super.getParameterMap(); for (Map.Entry&lt;String, String[]&gt; entry : parameters.entrySet()) &#123; String key = entry.getKey(); String[] values = entry.getValue(); for (int i = 0; i &lt; values.length; i++) &#123; values[i] = escape(values[i]); &#125; map.put(key, values); &#125; return map; &#125; @Override public String getHeader(String name) &#123; String value = super.getHeader(name); if (StringUtils.isNotBlank(value)) &#123; value = escape(value); &#125; return value; &#125; private static String escape(final String str) &#123; String result = str; // spring HtmlUtils result = HtmlUtils.htmlEscape(result); // script escape result = scriptXSS(result); return result; &#125; // eval(...) private static final Pattern P_EL = Pattern.compile(&quot;eval\\((.*?)\\)&quot;, Pattern.CASE_INSENSITIVE | Pattern.MULTILINE | Pattern.DOTALL); // expression(...) private static final Pattern P_EXP = Pattern.compile(&quot;expression\\((.*?)\\)&quot;, Pattern.CASE_INSENSITIVE | Pattern.MULTILINE | Pattern.DOTALL); // javascript:... private static final Pattern P_JS = Pattern.compile(&quot;javascript:&quot;, Pattern.CASE_INSENSITIVE); // vbscript:... private static final Pattern P_VB = Pattern.compile(&quot;vbscript:&quot;, Pattern.CASE_INSENSITIVE); // onload(...)=... private static final Pattern P_OL = Pattern.compile(&quot;onload(.*?)=&quot;, Pattern.CASE_INSENSITIVE | Pattern.MULTILINE | Pattern.DOTALL); private static String scriptXSS(final String str) &#123; String result = str; result = regexReplace(P_EL, &quot;&quot;, result); result = regexReplace(P_EXP, &quot;&quot;, result); result = regexReplace(P_JS, &quot;javascript：&quot;, result); result = regexReplace(P_VB, &quot;vbscript：&quot;, result); result = regexReplace(P_OL, &quot;&quot;, result); return result; &#125; private static String regexReplace(final Pattern pattern, final String replace, final String s) &#123; Matcher matcher = pattern.matcher(s); return matcher.replaceAll(replace); &#125; 3.注册XssFilter并配置过滤规则 123456789101112131415161718@Configurationpublic class WebMvcConfig &#123; /** * 过滤器配置文件过滤规则 */ @Bean public FilterRegistrationBean&lt;XssFilter&gt; xssFilterRegistration() &#123; FilterRegistrationBean&lt;XssFilter&gt; registration = new FilterRegistrationBean&lt;&gt;(new XssFilter()); registration.addUrlPatterns(&quot;/*&quot;); return registration; &#125;&#125; 此时配置Filter配置完成。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springbootErrorPage]]></title>
    <url>%2F2019%2F09%2F09%2FspringbootErrorPage%2F</url>
    <content type="text"><![CDATA[关于springBoot框架中全部异常页面的处理首先我们看一组官网数据截图： 由于springBoot中自带相关错误信息的处理页面，如果我们不进行显现配置则会使用springBoot自带的错误展示页面。 其实springBoot的全局处理页面的配置很简单，如上图在相关地址添加上相应的页面就行。 下面我来们看下springBoot版本2.0x以上版本的全局异常页面的配置： 首先在配置文件中配置页面文件的默认存储位置及后缀名，然后创建相关的页面地址。 这里需要注意的是：异常页面的储存需要在error的文件下以及用协议号给jsp或其他页面模板命名。 然后配置异常页面处理类： 1234567891011121314151617181920212223242526272829303132333435363738import org.springframework.boot.web.server.ConfigurableWebServerFactory;import org.springframework.boot.web.server.ErrorPage;import org.springframework.boot.web.server.WebServerFactoryCustomizer;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.http.HttpStatus;/** * * @description 全局异常处理类 */@Configurationpublic class ErrorPageConfig &#123; @Bean public WebServerFactoryCustomizer&lt;ConfigurableWebServerFactory&gt; webServerFactoryCustomizer() &#123;// //第一种：java7 常规写法// return new WebServerFactoryCustomizer&lt;ConfigurableWebServerFactory&gt;() &#123;// @Override// public void customize(ConfigurableWebServerFactory factory) &#123;// ErrorPage errorPage404 = new ErrorPage(HttpStatus.NOT_FOUND, &quot;/404.html&quot;);// factory.addErrorPages(errorPage404);// &#125;// &#125;; //第二种写法：java8 lambda写法 return (factory -&gt; &#123; ErrorPage errorPage404 = new ErrorPage(HttpStatus.NOT_FOUND, &quot;404&quot;); ErrorPage errorPage500 = new ErrorPage(HttpStatus.INTERNAL_SERVER_ERROR, &quot;500&quot;); factory.addErrorPages(errorPage404); factory.addErrorPages(errorPage500); &#125;); &#125;&#125; 这样全局异常处理页面的配置就全部完成。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[io 输入输出流学习]]></title>
    <url>%2F2019%2F08%2F26%2Fio%2F</url>
    <content type="text"><![CDATA[Java流类图结构 流的概念和作用 流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作 。 IO流的分类 1.根据处理数据类型的不同分为：字符流和字节流。 2.根据数据流向不同分为：输入流和输出流。 字符流和字节流 字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 字节流：一次读入或读出是8位二进制。 字符流：一次读入或读出是16位二进制。 设备上的数据无论是图片或者视频，文字，它们都以二进制存储的。二进制的最终都是以一个8位为数据单元进行体现，所以计算机中的最小数据单元就是字节。意味着，字节流可以处理设备上的所有数据，所以字节流一样可以处理字符数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。输入流和输出流输入流只能进行读操作，输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 输入字节流 InputStream InputStream 是所有的输入字节流的父类，它是一个抽象类。 ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。 PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。 ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 输出字节流 OutputStream OutputStream 是所有的输出字节流的父类，它是一个抽象类。 ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。 PipedOutputStream 是向与其它线程共用的管道中写入数据。 ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 总结： 输入流：InputStream或者Reader：从文件中读到程序中； 输出流：OutputStream或者Writer：从程序中输出到文件中； 节点流节点流：直接与数据源相连，读入或读出。直接使用节点流，读写不方便，为了更快的读写文件，才有了处理流。 常用的节点流 父 类 ：InputStream 、OutputStream、 Reader、 Writer 文 件 ：FileInputStream 、 FileOutputStrean 、FileReader 、FileWriter 文件进行处理的节点流 数 组 ：ByteArrayInputStream、 ByteArrayOutputStream、 CharArrayReader 、CharArrayWriter 对数组进行处理的节点流（对应的不再是文件，而是内存中的一个数组） 字符串 ：StringReader、 StringWriter 对字符串进行处理的节点流 管 道 ：PipedInputStream 、PipedOutputStream 、PipedReader 、PipedWriter 对管道进行处理的节点流 处理流处理流和节点流一块使用，在节点流的基础上，再套接一层，套接在节点流上的就是处理流。如BufferedReader.处理流的构造方法总是要带一个其他的流对象做参数。一个流对象经过其他流的多次包装，称为流的链接。 常用的处理流 缓冲流：BufferedInputStream 、BufferedOutputStream、 BufferedReader、 BufferedWriter 增加缓冲功能，避免频繁读写硬盘。 转换流：InputStreamReader 、OutputStreamReader实现字节流和字符流之间的转换。 数据流： DataInputStream 、DataOutputStream 等-提供将基础数据类型写入到文件中，或者读取出来。 转换流InputStreamReader 、OutputStreamWriter 要InputStream或OutputStream作为参数，实现从字节流到字符流的转换。 构造函数 123456InputStreamReader(InputStream); //通过构造函数初始化，使用的是本系统默认的编码表GBK。InputStreamReader(InputStream,String charSet); //通过该构造函数初始化，可以指定编码表。OutputStreamWriter(OutputStream); //通过该构造函数初始化，使用的是本系统默认的编码表GBK。OutputStreamwriter(OutputStream,String charSet); //通过该构造函数初始化，可以指定编码表。 实战演练： FileInputStream类的使用：读取文件内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;public class A1 &#123; public static void main(String[] args) &#123; A1 a1 = new A1(); //电脑d盘中的abc.txt 文档 String filePath = &quot;D:/abc.txt&quot; ; String reslut = a1.readFile( filePath ) ; System.out.println( reslut ); &#125; /** * 读取指定文件的内容 * @param filePath ： 文件的路径 * @return 返回的结果 */ public String readFile( String filePath )&#123; FileInputStream fis=null; String result = &quot;&quot; ; try &#123; // 根据path路径实例化一个输入流的对象 fis = new FileInputStream( filePath ); //2. 返回这个输入流中可以被读的剩下的bytes字节的估计值； int size = fis.available() ; //3. 根据输入流中的字节数创建byte数组； byte[] array = new byte[size]; //4.把数据读取到数组中； fis.read( array ) ; //5.根据获取到的Byte数组新建一个字符串，然后输出； result = new String(array); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if ( fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return result ; &#125;&#125;- FileOutputStream 类的使用：将内容写入文件import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;public class A2 &#123; public static void main(String[] args) &#123; A2 a2 = new A2(); //电脑d盘中的abc.txt 文档 String filePath = &quot;D:/abc.txt&quot; ; //要写入的内容 String content = &quot;今天是2017/1/9,天气很好&quot; ; a2.writeFile( filePath , content ) ; &#125; /** * 根据文件路径创建输出流 * @param filePath ： 文件的路径 * @param content : 需要写入的内容 */ public void writeFile( String filePath , String content )&#123; FileOutputStream fos = null ; try &#123; //1、根据文件路径创建输出流 fos = new FileOutputStream( filePath ); //2、把string转换为byte数组； byte[] array = content.getBytes() ; //3、把byte数组输出； fos.write( array ); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if ( fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 文件、流、byte直接的相互转换如下多以图片为例： 12345678910111213141516171819202122232425262728293031//文件转换为流：InputStream fis = new FileInputStream(&quot;xxx.jpg&quot;);//将流转换为byte[]ByteArrayOutputStream byteArray = new ByteArrayOutputStream();btye [] bytes = new byte [1024];int n;while ((n = fis.read(bytes)) != -1)&#123;​ byteArray.write(bytes, 0, n);​ &#125; fis.close();byteArray.close();byte[] imageBytes = byteArray.toByteArray();//将文件byte[] 转发为流InputStream inp = new ByteArrayInputStream(imageBytes ); 原文链接：https://blog.csdn.net/zhaoyanjun6/article/details/54292148/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[request获取项目地址]]></title>
    <url>%2F2019%2F08%2F22%2FrequestAgreement%2F</url>
    <content type="text"><![CDATA[1.关于request的使用总结： 1.2一般在项目中跳转到其他项目地址直接用： 12345response.sendRedirect(&quot;https://www.daidu.com&quot;); 1.3在本项目中跳转一般为：（存在一定的错误性） 12345678910111213141516String redirect = request.getContextPath() + &quot;目标类&quot;;response.sendRedirect(redirect);*request.getContextPath()的作用就是获取项目的跟目录：如果为空值就为“”，如果不为空值就为“xx”;**之所以这样处理是因为每个项目都有每个项目的名称，这样才能基本保证地址拼接的正确性；**如：你本地跑一个项目地址为：http://localhost:8080/index,(这样你就能访问到项目的公共)**但是正式环境可能会给项目一个指定的名称如：/pro,那么上面那个地址的就不能访问。正确地址如下：**http://xxx.com/pro/index;* 1.4由于生产环境基本都会配置nginx进行配置代理，那么外网访问地址为：https协议通过nginx代理可能会变成：http,那么此时再用上面的1.3中的方法就会导致页面打不开或者报错； 解决方法如下： 首先更改nginx配置文件配置如下属性： 作用:判断客户端是用的http请求还是https请求 然后更改tomcat中的server.xml配置文件： 1234567891011&lt;Connector port=&quot;443&quot; maxHttpHeaderSize=&quot;8192&quot; maxThreads=&quot;150&quot; enableLookups=&quot;false&quot; disableUploadTimeout=&quot;true&quot; acceptCount=&quot;100&quot; scheme=&quot;https&quot; secure=&quot;true&quot; SSLEnabled=&quot;true&quot; SSLCertificateFile=&quot;$&#123;catalina.base&#125;/conf/localhost.crt&quot; SSLCertificateKeyFile=&quot;$&#123;catalina.base&#125;/conf/localhost.key&quot; /&gt; 最后在代码处理为： 123456789101112131415161718192021//获取请求协议：https/httpString scheme = request.getHeader(&quot;X-Forwarded-Proto&quot;);if (ObjectUtils.isEmpty(scheme)) &#123;​ scheme = request.getScheme();&#125;//拼接链接//request.getServerName() --- 获取请求链接的域名 如：www.baidu.com//request.getContextPath() -- 获取项目的根路径 String redirect = scheme + &quot;://&quot; + request.getServerName() + request.getContextPath() + &quot;目标类&quot;;response.sendRedirect(redirect); –个人建议不管是生产环境还是本地环境推荐使用1.4方法。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>request</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[finderWeb日志管理平台]]></title>
    <url>%2F2019%2F08%2F08%2FfinderWeb%2F</url>
    <content type="text"><![CDATA[官方地址：http://www.finderweb.net/ 1.由于此服务部署在Tomcat中所有的确保系统有安装jdk环境，步骤如下： 自行下载Jdk文件，环境变量配置步骤如下: 1234567891011vi /etc/profileexport JAVA_HOME=/jdk/jdk1.8export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 以上路径请改为自己文件的实际路径。 然后执行刷新命令： 123source /etc/profile 2.下载文件至指定的位置：这里直接下载到Tomcat的ROOT目录下： 1234567wget -O /mnt/tomact/webapps/ROOT/finder-web-2.5.1.zip http://www.finderweb.net/download/finder-web-2.5.1.war//解压zip文件unzip -o -d /mnt/tomact/webapps/ROOT /mnt/tomact/webapps/ROOT/finder-web-2.5.1.zip 3.启动tomcat，进入bin目录执行如下命令 123456789./startup.sh//如果startup.sh不是可执行文件则需要给它赋予权限然后在执行启动命令：//为文件赋予权限chown -R 755 xx(文件名) 4.默认访问地址是8080,默认账号为admin密码为1234,也可自行更改tomcat端口首页如下： 5.添加主机检测日志 第一步：点击管理控制台–&gt;主机管理–&gt;添加主机 第二步 主机名称一般为ip如：xxx.xxx.xxx.xxx,显示名一般和主机名一致， HostURL就是自己finerd服务的地址：http://ip:port/finder，先点击测试测试成功就保存 第三步：点击一步图中出现的空间管理如下： WorkspaceName：一般为需要监控的项目名称 DisplayName:保持和WorkspaceName一致 Work:为需要监控项目日志的路径 第四步：进入系统设置–&gt;一般设置。然后新加out类型（可加可不加） 然后点击下面的同步到集群并保存 第五步：点击文件管理即可看到说要监控的文件]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>finderWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grafana]]></title>
    <url>%2F2019%2F08%2F07%2Fgrafana%2F</url>
    <content type="text"><![CDATA[什么是grafan：个人理解就是数据展示图即将数据库中的数据信息以图表的信息展示出来。官方地址：https://grafana.com/ ，详细使用方法请自行查看官方文档，本初只做简单的安装展示处理。 本处选择安装Centos版本 1.以此执行以下命令 12345wget &lt;https://dl.grafana.com/oss/release/grafana-6.3.0-1.x86_64.rpm&gt; sudo yum localinstall grafana-6.3.0-1.x86_64.rpm 2.执行成功后进入：/etc/init.d/执行grefana-server如图： 至此服务启动成功，默认端口为3000,账号密码为admin 如图登录成功的首页面 3.服务自启命令： 12345systemctl daemon-reloadsystemctl start grafana-serversystemctl status grafana-server 至此简单的服务启动已经结束，其他的仪表面板配置，数据的展示、默认人家的修改请自行查看官网]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[psAndlsof]]></title>
    <url>%2F2019%2F08%2F07%2FpsAndlsof%2F</url>
    <content type="text"><![CDATA[Linux中关于 ps、lsof、netstat的用途1.如何查看端口被那个进程占用： 1.lsof -i:端口号 2.netstat -tunlp | grep 端口号 上图可以得知PID进程61457占用了端口8080； 2.查看进程信息 ps aux | grep 进程号（PID） 这样我们就能找到相关进程的信息了]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ps，lsof</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[打包运行jar服务项目]]></title>
    <url>%2F2019%2F07%2F17%2FjarProject%2F</url>
    <content type="text"><![CDATA[第一步：确保项目中的Pom文件配置了packaging属性 123&lt;packaging&gt;jar&lt;/packaging&gt; 第二步：在pom文件中配置如下插件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;layout&gt;ZIP&lt;/layout&gt; &lt;includes&gt; &lt;include&gt; &lt;groupId&gt;nothing&lt;/groupId&gt; &lt;artifactId&gt;nothing&lt;/artifactId&gt; &lt;/include&gt; &lt;/includes&gt; &lt;attach&gt;false&lt;/attach&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;type&gt;jar&lt;/type&gt; &lt;includeTypes&gt;jar&lt;/includeTypes&gt; &lt;includeScope&gt;runtime&lt;/includeScope&gt; &lt;outputDirectory&gt; $&#123;project.build.directory&#125;/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 第三步：执行maven的clean、package命令就会看到项目下面会出现一个jar后缀的项目名称。 第四步：打开jar包查看里面的文件就会发现多出如下几个参数则表示jar打包成功 关于如何运行请自行百度，或者查看我的博客脚本运行项目]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>打包运行jar项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat常用配置参数详解]]></title>
    <url>%2F2019%2F07%2F16%2FTomcat%2F</url>
    <content type="text"><![CDATA[元素名 属性 解释 server port 指定一个端口，这个端口负责监听关闭tomcat的请求 shutdown 指定向端口发送的命令字符串 service name 指定service的名字 Connector(表示客户端和service之间的连接) port 指定服务器端要创建的端口号，并在这个断口监听来自客户端的请求 minProcessors 服务器启动时创建的处理请求的线程数 maxProcessors 最大可以创建的处理请求的线程数 enableLookups 如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址 redirectPort 指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号 acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理 connectionTimeout 指定超时的时间数(以毫秒为单位) Engine(表示指定service中的请求处理机，接收和处理来自Connector的请求) defaultHost 指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的 Context(表示一个web应用程序，通常为WAR文件，关于WAR的具体信息见servlet规范) docBase 应用程序的路径或者是WAR文件存放的路径 path 表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/**** reloadable 这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib 和/WEB-INF/classes目录的变化，自动装载新的应用程序，我们可以在不重起tomcat的情况下改变应用程序 host(表示一个虚拟主机) name 指定主机名 appBase 应用程序基本目录，即存放应用程序的目录 unpackWARs 如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序 Logger(表示日志，调试和错误信息) className 指定logger使用的类名，此类必须实现org.apache.catalina.Logger 接口 prefix 指定log文件的前缀 suffix 指定log文件的后缀 timestamp 如果为true，则log文件名中要加入时间，如下例:localhost_log.001-10-04.txt Realm(表示存放用户名，密码及role的数据库) className 指定Realm使用的类名，此类必须实现org.apache.catalina.Realm接口 Valve(功能与Logger差不多，其prefix和suffix属性解释和Logger 中的一样) className 指定Valve使用的类名，如用org.apache.catalina.valves.AccessLogValve类可以记录应用程序的访问信息 directory 指定log文件存放的位置 pattern 有两个值，common方式记录远程主机名或ip地址，用户名，日期，第一行请求的字符串，HTTP响应代码，发送的字节数。combined方式比common方式记录的值更多]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中创建web项目及jar启动脚本]]></title>
    <url>%2F2019%2F07%2F11%2FLinuxStartScript%2F</url>
    <content type="text"><![CDATA[以启动jar项目为例写一个启动脚本第一步创建一个.sh文件： 123touch xx.sh 第二步编辑xx.sh文件,然后按I输入：#!/usr/bin/env bash 12345vi xx.sh#!/usr/bin/env bash 注：#!/usr/bin/env bash #在不同的系统上提供了一些灵活性 第三步配置环境变量 同样在xx.sh文件中编写如下代码：（文件实际地址请自行根据自己的系统设置） 1234567891011121314151617181920212223export LANG=en_US.UTF-8export JAVA_HOME=/usr/java/jdk.1.8.0_144export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$CLASSPATHcd &apos;dirname $0&apos;CURRENT_DIR=&apos;pwd&apos;LOGS_DIR=&quot;$&#123;CURRENT_DIR&#125;/logs&quot;STDOUT_FILE=&quot;$&#123;CURRENT_DIR&#125;/logs/stdout.log&quot;PROJECT_HOME=&quot;$&#123;CURRENT_DIR&#125;/project&quot;JAR_FILE=&quot;$&#123;PROJECT_HOME&#125;/xx.jar&quot; 注：上面的“.”,”:”，分别表示当前目录和分隔符及多层路径，其中$path,$classpath在定义变量中没有出现，则表示引用系统环境变量，java_home有自己定义则使用上面自己定义的。 查看系统环境变量直接输入env： 123env 一个完整的jar启动脚本如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/usr/bin/env bashexport LANG=en_US.UTF-8export JAVA_HOME=/usr/java/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$CLASSPATHcd `dirname $0`CURRENT_DIR=`pwd`LOGS_DIR=&quot;$&#123;CURRENT_DIR&#125;/logs&quot;STDOUT_FILE=&quot;$&#123;CURRENT_DIR&#125;/logs/stdout.log&quot;PROJECT_HOME=&quot;$&#123;CURRENT_DIR&#125;/project&quot;JAR_FILE=&quot;$&#123;PROJECT_HOME&#125;/tisson-open-rpc-service.jar&quot;# date patternDATE_PATTERN=&quot;[$(date &apos;+%Y-%m-%d %H:%M:%S&apos;)] $&#123;JAR_FILE&#125;&quot;# pidPIDS=`ps -ef | grep &quot;$&#123;JAR_FILE&#125;&quot; | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`# -- ps -ef | grep xx 查文本的信息的数据 grep -v grep 查询不含有grep数据的值 # awk &apos;&#123;pring $2&#125;&apos; $2:表示第二个字段，print$2 : 打印第二个字段 # JVM optionsJAVA_OPT=&quot;-server -Xms1024m -Xmx1024m -Xss256k&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Dloader.path=$&#123;PROJECT_HOME&#125;/lib/ -Djava.io.tmpdir=$&#123;LOGS_DIR&#125;&quot;#-XX:+DisableExplicitGC 启用禁用处理对System.gc（）的调用的选项。 默认情况下禁用此选项，这意味着将处理对System.gc（）的调用。 如果禁用对System.gc（）的调用处理，则JVM在必要时仍执行GC。#UseConcMarkSweepGC 允许为旧一代使用CMS垃圾收集器。 Oracle建议您在吞吐量（-XX：+ UseParallelGC）垃圾收集器无法满足应用程序延迟要求时使用CMS垃圾收集器--#。 G1垃圾收集器（-XX：+ UseG1GC）是另一种选择。--#默认情况下，禁用此选项，并根据计算机的配置和JVM的类型自动选择收集器。 启用此选项后，将自动设置-XX：+ UseParNewGC选项，--#您不应禁用它，因为JDK 8中已弃用以下选项组合：-XX：+ UseConcMarkSweepGC -XX：-UseParNewGC。#CMSParallelRemarkEnabled #LargePageSizeInBytes 在Solaris上，设置用于Java堆的大页面的最大大小（以字节为单位）。 size参数必须是2的幂（2,4,8,16，...）。# 附加字母k或K表示千字节，m或M表示兆字节，g或G表示千兆字节。 默认情况下，大小设置为0，这意味着JVM会自动选择大页面的大小。#UseFastAccessorMethods 原始类型的快速优化 #XX:+UseCMSInitiatingOccupancyOnly 使用手动定义初始化定义开始CMS收集#XX:CMSInitiatingOccupancyFraction=70 使用cms作为垃圾回收使用70％后开始CMS收集JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;LOGS_DIR&#125;/$&#123;PIDS&#125;.hprof&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Xloggc:$&#123;LOGS_DIR&#125;/gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Ddubbo.protocol.port=20882&quot;#启动Java应用时，通过-D传入系统参数#-Dspring.profiles.active=devJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -Dspring.profiles.active=dev&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -jar $&#123;JAR_FILE&#125;&quot;function operate()&#123; if [[ &quot;$1&quot; = &quot;kill&quot; ]]; then if [ -n &quot;$&#123;PIDS&#125;&quot; ]; then echo &quot;$&#123;DATE_PATTERN&#125; kill $&#123;PIDS&#125; begining&quot; | tee -a $&#123;STDOUT_FILE&#125; kill -9 $&#123;PIDS&#125; echo &quot;$&#123;DATE_PATTERN&#125; kill $&#123;PIDS&#125; success&quot; | tee -a $&#123;STDOUT_FILE&#125; else echo &quot;$&#123;DATE_PATTERN&#125; is not alive&quot; | tee -a $&#123;STDOUT_FILE&#125; fi elif [[ &quot;$1&quot; = &quot;start&quot; ]] ; then cd $PROJECT_HOME # starting nohup $JAVA_HOME/bin/java $&#123;JAVA_OPT&#125; &gt;&gt; $&#123;STDOUT_FILE&#125; 2&gt;&amp;1 &amp; PIDS=`ps -ef | grep &quot;$JAR_FILE&quot; | grep -v grep | awk &apos;&#123;print $2&#125;&apos;` echo &quot;$&#123;DATE_PATTERN&#125; The $JAR_FILE started OK! pid: $&#123;PIDS&#125;&quot; | tee -a $&#123;STDOUT_FILE&#125; else echo &quot;$&#123;DATE_PATTERN&#125; is not support $1&quot; | tee -a $&#123;STDOUT_FILE&#125; fi&#125;#tee -a file#输出到标准输出的同时，追加到文件file中。如果文件不存在，#则创建；如果已经存在，就在末尾追加内容，而不是覆盖。if [[ &quot;$1&quot; = &quot;start&quot; || &quot;$1&quot; = &quot;check&quot; ]]; then if [ -n &quot;$&#123;PIDS&#125;&quot; ]; then echo &quot;$&#123;DATE_PATTERN&#125; already started! pid: $&#123;PIDS&#125;&quot; &gt;&gt; $&#123;STDOUT_FILE&#125; exit 1 fi operate startelif [[ &quot;$1&quot; = &quot;&quot; || &quot;$1&quot; = &quot;restart&quot; ]]; then operate kill echo &quot;$&#123;DATE_PATTERN&#125; starting&quot; | tee -a $&#123;STDOUT_FILE&#125; operate startelif [[ &quot;$1&quot; = &quot;kill&quot; ]]; then operate killelse echo &quot;$&#123;DATE_PATTERN&#125; is not support $1&quot; | tee -a $&#123;STDOUT_FILE&#125;fi 配置一个web项目启动脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#!/usr/bin/env bashcd `dirname $0`HOME_DIR=`pwd`# Tomcat服务器路基TS_HOME=&quot;$HOME_DIR/tomcat&quot;# crontab检测日记文件TS_LOG_OPERATE=&quot;$TS_HOME/logs/console.out&quot;# 检测Tomcat是否启动（进程ID）TS_PID=`ps -ef | grep $TS_HOME/bin | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`# 执行检测的用户IP地址TS_IP=`who am i | awk &apos;&#123;print $5&#125;&apos; | sed &apos;s/(//g&apos; | sed &apos;s/)//g&apos;`# 检测输出的日记格式TS_PATTERN=&quot;[$(date &apos;+%Y-%m-%d %H:%M:%S&apos;)]-[$1]-[$TS_IP] $TS_HOME&quot;#===========================================================================================# JVM Configuration#===========================================================================================export LANG=en_US.UTF-8export JAVA_HOME=/usr/java/jdk1.8.0_144export JAVA_OPTS=&quot;$JAVA_OPTS -server -Xms1g -Xmx1g -Xss256k&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -XX:+DisableExplicitGC -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+ParallelRefProcEnabled&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Xloggc:$TS_HOME/logs/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$TS_HOME/logs/dump-`date +&quot;%Y-%m-%d&quot;`.hprof&quot;export JAVA_OPTS=&quot;$JAVA_OPTS -Dspring.profiles.active=dev&quot;#===========================================================================================# log file delete#===========================================================================================function cleanLog()&#123; ls -d -t $TS_HOME/logs/catalina.*.log | tail -n +7 | xargs rm -rf ls -d -t $TS_HOME/logs/host-manager.*.log | tail -n +7 | xargs rm -rf ls -d -t $TS_HOME/logs/localhost.*.log | tail -n +7 | xargs rm -rf ls -d -t $TS_HOME/logs/manager.*.log | tail -n +7 | xargs rm -rf ls -d -t $TS_HOME/logs/dump.*.hprof | tail -n +3 | xargs rm -rf&#125;cleanLogfunction operate()&#123; if [[ &quot;$1&quot; = &quot;kill&quot; ]] ; then if [[ &quot;$TS_PID&quot; = &quot;&quot; ]] ; then echo &quot;$TS_PATTERN is not alive&quot; | tee -a $TS_LOG_OPERATE else echo &quot;$TS_PATTERN kill $TS_PID begining&quot; | tee -a $TS_LOG_OPERATE kill -9 $TS_PID echo &quot;$TS_PATTERN kill $TS_PID success&quot; | tee -a $TS_LOG_OPERATE fi elif [[ &quot;$1&quot; = &quot;start&quot; ]]; then cd $TS_HOME/bin sh startup.sh else echo &quot;$TS_PATTERN is not support $1&quot; | tee -a $TS_LOG_OPERATE fi&#125;if [[ &quot;$1&quot; = &quot;&quot; || &quot;$1&quot; = &quot;restart&quot; || &quot;$1&quot; = &quot;start&quot; ]] ; then operate kill echo &quot;$TS_PATTERN starting&quot; | tee -a $TS_LOG_OPERATE operate startelif [[ &quot;$1&quot; = &quot;kill&quot; ]] ; then operate killelif [[ &quot;$1&quot; = &quot;check&quot; ]] ; then if [[ &quot;$TS_PID&quot; = &quot;&quot; ]] ; then echo &quot;$TS_PATTERN starting&quot; &gt;&gt; $TS_LOG_OPERATE operate start else echo &quot;$TS_PATTERN pid $TS_PID&quot; &gt;&gt; $TS_LOG_OPERATE fielse echo &quot;$TS_PATTERN is not support $1&quot; | tee -a $TS_LOG_OPERATEfi]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tomcat启动脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何上传jar到maven私服]]></title>
    <url>%2F2019%2F07%2F09%2Fmaven%2F</url>
    <content type="text"><![CDATA[第一步在需要打包的pom文件中，添加如下参数： 12345678910111213141516171819202122232425&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;xx&lt;/id&gt; &lt;url&gt;http://localhost:8081/repository/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;xxx&lt;/id&gt; &lt;url&gt;http://localhost:8081/repository/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;skip&gt;false&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 注：其中的repository下的id和url要和maven中的setting.xml中的配置一致。如下画红线部分： 然后再执行maven中的deploy 这个时候Jar已经上传到了上图的rul中地址中。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xml报文解析基本方法]]></title>
    <url>%2F2019%2F07%2F02%2Fxml%2F</url>
    <content type="text"><![CDATA[使用dom4j处理xml报文 需要引入的jar 123456789&lt;dependency&gt; &lt;groupId&gt;org.dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 我们以如下例子来分别讲解常用的xml解析方法： 如上图这是一个返回的报文：是以String类型接收的，此时我们需要获取里面的值。 第一步：将String类型解析为Document. 12345Document document = DocumentHelper.parseText(&quot;xml&quot;);//xml为上图的报文 第二步：拿取报文中的节点值： 1.拿取 的值： 12345Node rtNode = document.selectSingleNode(&quot;//SendData/resultcode&quot;);String rtResult = rtNode.getStringValue(); //resultcode的值 2.拿取 的值： 12345Node rsNode = document.selectSingleNode(&quot;//SendData/reason&quot;);String rsResult = rsNode.getStringValue();//reason的值 3.拿取的值，此时由于col存在多个，所以不能再用selectSingleNode； 123456789List&lt;Node&gt; list = document.selectNodes(&quot;//SendData/result/row/col&quot;);String col_1 = list.get(0).getText();String col_2 = list.get(1).getText();String col_3 = list.get(2).getText(); 4.拿取 中的属性值：如param_name: 1234567List&lt;Node&gt; list = document.selectNodes(&quot;//SendData/result/row/col&quot;);Element element = (Element) list.get(2);String paramNameStr = element.attributeValue(&quot;param_name&quot;); 至此一个简单常见的xml解析至此完成了。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle表分区处理及全局索引和本地索引]]></title>
    <url>%2F2019%2F06%2F27%2Fpartition%2F</url>
    <content type="text"><![CDATA[创建分区表1.按年创建分区： *注：粗体部分可以自行定义(即用双星号标志的地方) 123456789101112131415161718192021222324252627create table **tableName**(​ **ID NUMBER(20) NOT NULL,**​ **REMARK VARCHAR(100),**​ **CREATE_TIME DATE**)PARTITION BY RANGE (CREATE_TIME) INTERVAL (numtoyminterval(1, &apos;year&apos;))//按月创建分区表//PARTITION BY RANGE (CREATE_TIME) INTERVAL (numtoyminterval(1, &apos;month&apos;))//按天创建分区表//PARTITION BY RANGE (CREATE_TIME) INTERVAL (numtodsinterval(1, &apos;day&apos;))//按周创建分区表//PARTITION BY RANGE (CREATE_TIME) INTERVAL （numtodsinterval(7, &apos;day&apos;)）(partition **part_name** values less than (**to_date(&apos;2019-06-27&apos;, &apos;yyyy-mm-dd&apos;)**)); 已存在的表创建分区1.给已存在的表进行重命名 123alter table **tableName** rename to **newTableName** 2.创建新表和以前要做分区表的名称一致 123456789101112131415create table **tableName**(​ **ID NUMBER(20) NOT NULL,**​ **REMARK VARCHAR(100),**​ **CREATE_TIME DATE**)PARTITION BY RANGE (CREATE_TIME) INTERVAL (numtoyminterval(1, &apos;month&apos;))(PARTITION **PART_NAME** values less than(to_date(&apos;2019-06-27&apos;,&apos;yyyy-mm-dd&apos;))); 3.将重命名表的数据导入到新分区表中 123insert into **tableName** select * from **tableName** 4.删除重命名的表 123drop table **tableName** 分区表创建全局索引123456789101112131415161718192021222324252627282930313233343536373839//第一步创建一个分区表create table **tableName**(​ **ID NUMBER(20) NOT NULL,**​ **REMARK VARCHAR(100),**​ **CREATE_TIME DATE**)PARTITION BY RANGE (CREATE_TIME) INTERVAL (numtoyminterval(1, &apos;month&apos;))(PARTITION **PART_NAME** values less than(to_date(&apos;2019-06-27&apos;,&apos;yyyy-mm-dd&apos;)));//创建全局索引create index **indexName** on **tableName(column)**global partition by range (**column**)(partition **parName1** values less than (**column**) ,partition **parName2** values less than (maxvalue) );//全局索引和表没有直接的关联，必须显式的指定maxvalue值。假如表中新加了分区，不会在全局索引中自动增加新的分区，必须手工添加相应的分区。 //手动添加分区alter talbe **tableName** add partition **parName3** values less than(**column**); 创建本地索引使用本地索引，不需要指定分区范围因为索引对于表而言是本地的，当本地索引创建时，Oracle会自动为表中的每个分区创建独立的索引分区。 123456789101112131415create index **indexName** on **tableName(column)**local (partition **parName1**,partition **parName2** );//手动添加分区alter talbe **tableName** add partition **parName3** values less than(**column**); – 查询当前表有多少分区 123****select table_name,partition_name from user_tab_partitions where table_name = &apos;T_TEST_PART&apos; –查询这个表的某个分区表里的数据 123select * from T_TEST_PART partition(分区表名); 本地索引和全局索引有一个显著的差别，本地索引可以创建成本地非前缀型，而全局索引只能是前缀型。 清空区分表数据、删除分区表1234567891011121314151617//用来删除分区，元数据和数据将一并删除alter table drop partition;//全部删除alter table tableName drop partition partitionName;//清数据alter table tableName truncate partition partitionName;]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>index(索引分区)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle数据库索引的创建和查询]]></title>
    <url>%2F2019%2F06%2F20%2ForacleTaboeIndex%2F</url>
    <content type="text"><![CDATA[索引命名规范：普通索引最好i_开头，唯一索引最好u_开头，primary key索引最好pk_开头 1.创建索引： create index 索引名 on 表名(字段名); 2.查询索引 oracle中表的索引信息存在 user_indexes 和 user_ind_columns 两张表里面， 其中： user_indexes 系统视图存放的是索引的名称以及该索引是否是唯一索引等信息， user_ind_columns 系统视图存放的是索引名称，对应的表和列等。 select * from user_indexes where table_name = upper(‘表名’); select * from user_ind_columns t; 3.判断是否需要重建索引以及如何重建索引如果索引因为某些原因无效或者因为很长时间没有维护而产生过多的索引碎片（Index Fragment），需要通过重建索引来消除索引碎片。何时需要重建索引，可以利用下面的过程进行判断。 查询数据库中有哪些索引。 123SELECT TABLE_OWNER,INDEX_NAME FROM user_indexes; // 对索引EM_T_LOG_IM_LINKAGE_OPER_PK进行分析。 123ANALYZE index EM_T_LOG_IM_LINKAGE_OPER_PK VALIDATE STRUCTURE; 从视图INDEX_STATS中获得索引EM_T_LOG_IM_LINKAGE_OPER_PK的统计信息 123SELECT HEIGHT,(DEL_LF_ROWS_LEN/LF_ROWS_LEN)*100 FROM INDEX_STATS WHERE NAME=&apos;EM_T_LOG_IM_LINKAGE_OPER_PK&apos;； //其中，字典INDEX_STATS表示存放索引的统计信息；列DEL_LF_ROWS_LEN表示索引删除行数；列LF_ROWS_LEN表示索引总行数；列HEIGHT表示二叉树 中从根块到叶块的层次（深度）。 如果满足下面其中一个条件，则要考虑重建索引： (DEL_LF_ROWS_LEN/LF_ROWS_LEN)*100的值大于20 HEIGHT的值大于4（说明二叉树的层次太多） 使用ALTER INDEX … REBUILD命令重建索引EM_T_LOG_IM_LINKAGE_OPER_PK。 123 ALTER INDEX EM_T_LOG_IM_LINKAGE_OPER_PK REBUILD;]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>index(索引)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[commons-io]]></title>
    <url>%2F2019%2F06%2F18%2Fcommons-io%2F</url>
    <content type="text"><![CDATA[commons-io架包的妙用1.引入架包： 123456789&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt;&lt;/dependency&gt; IOUtils.toByteArray(inputStream) //将InputStream转为byte; InputStream inputStream = new ByteArrayInputStream(byte [] b); //将byte转为InputStream FileUtils.copyInputStreamToFile(inputStream, imagePahth); //将流转化为对应的文件//imagePath(文件名称路径) 如：d:/xx/xx.mp4 注：服务直接的调用数据请求返回数据类型为：byte时 接收端可能需要做 base64转义处理： IOUtils.decodeBase64((String) value)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js中身份证的准确检验]]></title>
    <url>%2F2019%2F06%2F14%2FjsIdcCheck%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143**直接粘贴复制即可：**var vcity = &#123; 11: &quot;北京&quot;, 12: &quot;天津&quot;, 13: &quot;河北&quot;, 14: &quot;山西&quot;, 15: &quot;内蒙古&quot;, 21: &quot;辽宁&quot;, 22: &quot;吉林&quot;, 23: &quot;黑龙江&quot;, 31: &quot;上海&quot;, 32: &quot;江苏&quot;, 33: &quot;浙江&quot;, 34: &quot;安徽&quot;, 35: &quot;福建&quot;, 36: &quot;江西&quot;, 37: &quot;山东&quot;, 41: &quot;河南&quot;, 42: &quot;湖北&quot;, 43: &quot;湖南&quot;, 44: &quot;广东&quot;, 45: &quot;广西&quot;, 46: &quot;海南&quot;, 50: &quot;重庆&quot;, 51: &quot;四川&quot;, 52: &quot;贵州&quot;, 53: &quot;云南&quot;, 54: &quot;西藏&quot;, 61: &quot;陕西&quot;, 62: &quot;甘肃&quot;, 63: &quot;青海&quot;, 64: &quot;宁夏&quot;, 65: &quot;新疆&quot;, 71: &quot;台湾&quot;, 81: &quot;香港&quot;, 82: &quot;澳门&quot;, 91: &quot;国外&quot;&#125;;checkCard = function (cardNo) &#123; //是否为空 if (cardNo === &apos;&apos;) &#123; layer.msgCustom(&apos;请输入身份证号，身份证号不能为空&apos;) return false; &#125; //校验长度，类型 if (isCardNo(cardNo) === false) &#123; layer.msgCustom(&apos;您输入的身份证号码不正确，请重新输入&apos;); return false; &#125; //检查省份 if (checkProvince(cardNo) === false) &#123; layer.msgCustom(&apos;您输入的身份证号码不正确,请重新输入&apos;); return false; &#125; //校验生日 if (checkBirthday(cardNo) === false) &#123; layer.msgCustom(&apos;您输入的身份证号码生日不正确,请重新输入&apos;); return false; &#125; //检验位的检测 if (checkParity(cardNo) === false) &#123; layer.msgCustom(&apos;您的身份证校验位不正确,请重新输入&apos;); return false; &#125; return true;&#125;;//检查号码是否符合规范，包括长度，类型isCardNo = function (card) &#123; //身份证号码为15位或者18位，15位时全为数字，18位前17位为数字，最后一位是校验位，可能为数字或字符X var reg = /(^\d&#123;15&#125;$)|(^\d&#123;17&#125;(\d|X)$)/; if (reg.test(card) === false) &#123; return false; &#125; return true;&#125;;//取身份证前两位,校验省份checkProvince = function (card) &#123; var province = card.substr(0, 2); if (vcity[province] == undefined) &#123; return false; &#125; return true;&#125;;//检查生日是否正确checkBirthday = function (card) &#123; var len = card.length; //身份证15位时，次序为省（3位）市（3位）年（2位）月（2位）日（2位）校验位（3位），皆为数字 if (len == &apos;15&apos;) &#123; var re_fifteen = /^(\d&#123;6&#125;)(\d&#123;2&#125;)(\d&#123;2&#125;)(\d&#123;2&#125;)(\d&#123;3&#125;)$/; var arr_data = card.match(re_fifteen); var year = arr_data[2]; var month = arr_data[3]; var day = arr_data[4]; var birthday = new Date(&apos;19&apos; + year + &apos;/&apos; + month + &apos;/&apos; + day); return verifyBirthday(&apos;19&apos; + year, month, day, birthday); &#125; //身份证18位时，次序为省（3位）市（3位）年（4位）月（2位）日（2位）校验位（4位），校验位末尾可能为X if (len == &apos;18&apos;) &#123; var re_eighteen = /^(\d&#123;6&#125;)(\d&#123;4&#125;)(\d&#123;2&#125;)(\d&#123;2&#125;)(\d&#123;3&#125;)([0-9]|X)$/; var arr_data = card.match(re_eighteen); var year = arr_data[2]; var month = arr_data[3]; var day = arr_data[4]; var birthday = new Date(year + &apos;/&apos; + month + &apos;/&apos; + day); return verifyBirthday(year, month, day, birthday); &#125; return false;&#125;;//校验日期verifyBirthday = function (year, month, day, birthday) &#123; var now = new Date(); var now_year = now.getFullYear(); //年月日是否合理 if (birthday.getFullYear() == year &amp;&amp; (birthday.getMonth() + 1) == month &amp;&amp; birthday.getDate() == day) &#123; //判断年份的范围（3岁到100岁之间) var time = now_year - year; if (time &gt;= 3 &amp;&amp; time &lt;= 100) &#123; return true; &#125; return false; &#125; return false;&#125;;//校验位的检测checkParity = function (card) &#123; //15位转18位 card = changeFivteenToEighteen(card); var len = card.length; if (len == &apos;18&apos;) &#123; var arrInt = new Array(7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2); var arrCh = new Array(&apos;1&apos;, &apos;0&apos;, &apos;X&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;); var cardTemp = 0, i, valnum; for (i = 0; i &lt; 17; i++) &#123; cardTemp += card.substr(i, 1) * arrInt[i]; &#125; valnum = arrCh[cardTemp % 11]; if (valnum == card.substr(17, 1)) &#123; return true; &#125; return false; &#125; return false;&#125;;//15位转18位身份证号changeFivteenToEighteen = function (card) &#123; if (card.length == &apos;15&apos;) &#123; var arrInt = new Array(7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2); var arrCh = new Array(&apos;1&apos;, &apos;0&apos;, &apos;X&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;); var cardTemp = 0, i; card = card.substr(0, 6) + &apos;19&apos; + card.substr(6, card.length - 6); for (i = 0; i &lt; 17; i++) &#123; cardTemp += card.substr(i, 1) * arrInt[i]; &#125; card += arrCh[cardTemp % 11]; return card; &#125; return card;&#125;;]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>身份证校验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[okhttp基本服务请求用法]]></title>
    <url>%2F2019%2F06%2F13%2Fokhttp%2F</url>
    <content type="text"><![CDATA[此次主要介绍用okhttp进行服务器的基本访问方法如：post、get的基本请求或者post模拟表单上传文件。 第一步引入需要的jar： 123456789&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.10.0&lt;/version&gt;&lt;/dependency&gt; 普通的数据请求POST方式提交String 1234567891011String requestInfo = &quot;Nihao&quot;;Request request = new Request.Builder() .url(&quot;&quot;) .post(RequestBody.create(MediaType.parse(&quot;text/x-markdown;charset=utf-8&quot;), requestInfo)) .build();OkHttpClient okHttpClient = new OkHttpClient();Response response = okHttpClient.newCall(request).execute(); POST方式提交单个文件 123456789101112131415161718OkHttpClient client = new OkHttpClient();String filePath = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + &quot;1.mp4&quot;;RequestBody requestBody = RequestBody.create(MediaType.parse(&quot;mp4&quot;), filePath);RequestBody requestBody1 = new MultipartBody.Builder() .setType(MultipartBody.FORM) .addFormDataPart(&quot;请求携带的参数&quot;,&quot;123&quot;) //请求的名称,文件的名称,//创建RequestBody，把上传的文件放入 .addFormDataPart(&quot;file&quot;, &quot;xx.mp4&quot;,requestBody) .build();Request request = new Request.Builder() .url(&quot;&quot;) .post(requestBody1) .build();Response response = client.newCall(request).execute(); POST方式提交多个文件 12345678910111213141516171819202122232425262728293031List&lt;String&gt; strList = new ArrayList&lt;&gt;();String filePath = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + &quot;1.mp4&quot;;String filePath2 = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + &quot;timg.jpg&quot;;String filePath3 = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + &quot;timg2.jpg&quot;;strList.add(filePath);strList.add(filePath2);strList.add(filePath3);MultipartBody.Builder builder = new MultipartBody.Builder();for (int i = 0; i &lt; strList.size(); i++) &#123; File file = new File(strList.get(i)); //获得文件名称 String fileName = file.getName(); //获得文件后缀名 String suffix = fileName.substring(fileName.lastIndexOf(&quot;.&quot;) + 1); builder.addFormDataPart(&quot;file&quot; + i, file.getName(), RequestBody.create(MediaType.parse(suffix), file));&#125;builder.addFormDataPart(&quot;cityId&quot;, &quot;cityId&quot;);RequestBody requestBody = builder.build();Request.Builder builderReq = new Request.Builder();//&quot;http:localhost:8081/api/wx/cloud&quot;//http:localhost:8082/legaPeople.do?action=imageSRequest request = builderReq.url(&quot;http:localhost:8081/api/wx/cloud&quot;) .post(requestBody) .build();OkHttpClient okHttpClient = new OkHttpClient();okhttp3.Response response = okHttpClient.newCall(request).execute();log.info(&quot;response:&quot; + response); 服务端接收处理文件 1234567891011121314151617181920212223242526List&lt;MultipartFile&gt; multipartFile = m.getFiles(&quot;file&quot;);String originalFilename = &quot;&quot;;if (!multipartFile.isEmpty()) &#123; for (int i = 0; i &lt; multipartFile.size(); i++) &#123; //获取原始文件名称 originalFilename = multipartFile.get(i).getOriginalFilename(); //将原始文件名前缀和后缀分开放入数组 String[] fileNames = originalFilename.split(&quot;\\.&quot;); //文件名后缀 String suffixName = fileNames[fileNames.length - 1]; //保存文件名 String saveName = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;) + &quot;.&quot; + suffixName; //保存文件路径 String filePathName = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + &quot;timg&quot; + File.separator; File file = new File(filePathName); File file1 = new File(filePathName + saveName); if (!file.exists()) &#123; file.mkdirs(); &#125; FileUtils.copyInputStreamToFile(multipartFile.get(i).getInputStream(), file1); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>okhttp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot关于Tomcat的配置]]></title>
    <url>%2F2019%2F06%2F06%2FspringTomcatConfig%2F</url>
    <content type="text"><![CDATA[一：Springboot中关于http请求头过大的处理： 注：在相应的配置文件中配置如下参数：此处以xxx.yml文件为例： 123456789 spring:​ server:​ tomcat:​ max-http-header-size : 999999 注：上面的99999为http请求头的容量； 二：Springboot中关于文件上传最大容量处理： 123456789 spring:​ servlet:​ multipart:​ max-request-size: 100MB 其他详情用法其自行查阅资料]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aop的基本用法]]></title>
    <url>%2F2019%2F05%2F29%2FaopMethod%2F</url>
    <content type="text"><![CDATA[AOP环绕切面的基本用法需要引入的jar： 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;version&gt;2.0.8.RELEASE&lt;/version&gt;&lt;/dependency&gt; @Around环绕标签注解用于注解在方法类上 12345@Around(&quot;execution(* com.demo.wx.controller.LegaPeopleController.*(..))&quot;) execution：为执行命令 第一个星表示 方法返回类型，‘’表示所有类型； com.x.x:表示所要拦截的类路径; 第二个星表示：类下面的所有方法； （..）表示方法中的所有参数； 重点在用其ProceedingJoinPoint接口的方法使用1234567891011121314public xxx(ProceedingJoinPoint pjp) throws Exception &#123; //getSignature返回连接点处的签名，MethodSignature包含了 类的返回类型和类的方法名称 MethodSignature signature = (MethodSignature) pjp.getSignature(); //获取类的方法数据 Method method = signature.getMethod(); //获取类的返回类型 returnType = method.getReturnType(); //返回此连接点处的参数。也就是方法中的参数类型如String,int.. Object[] args = pjp.getArgs();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片处理（水印，Base64转图片）]]></title>
    <url>%2F2019%2F05%2F29%2FimageUtils%2F</url>
    <content type="text"><![CDATA[Base64格式转图片123456789101112131415161718192021222324252627282930313233343536373839404142 public static boolean Base64ToImage(String imgStr) throws WxException &#123; Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String string = sdf.format(date); //图片存放路径 String filePath = &quot;D:&quot; + File.separator + string; File file = new File(filePath); //判断文件是否存在 if (!file.exists()) &#123; file.mkdir(); &#125; //图片名称 Long lg = System.currentTimeMillis(); String imageName = lg + &quot;.jpg&quot;; //图片存放路径 String imagePath = filePath + File.separator + imageName; if (imgStr == null) &#123; throw new WxException(&quot;图片base64编码数据为空&quot;); &#125; BASE64Decoder decoder = new BASE64Decoder(); try &#123;// 解密 byte[] b = decoder.decodeBuffer(imgStr);// 处理数据 for (int i = 0; i &lt; b.length; ++i) &#123; if (b[i] &lt; 0) &#123; b[i] += 256; &#125; &#125; OutputStream out = new FileOutputStream(imagePath); out.write(b); out.flush(); out.close(); return true; &#125; catch (Exception e) &#123; throw new WxException(&quot;图片base64格式转换图片失败。&quot;); &#125; &#125; 图片增加图片水印处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 public static String watermark(File image) throws Exception &#123;// String logoFileName = &quot;logo_&quot;+image.getOriginalFilename(); OutputStream os = null; try &#123; Image image2 = ImageIO.read(image); int width = image2.getWidth(null); int height = image2.getHeight(null);// 1、创建缓存图片对象 BufferedImage bufferedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);// 2、 创建JAVA绘图工具对象 Graphics2D graphics2D = bufferedImage.createGraphics();// 3、使用绘图工具对象，将原图绘制到缓存图片对象 graphics2D.drawImage(image2, 0, 0, width, height, null);// 4、使用绘图工具，将水印（文字/图片）绘制到缓存图片对象Logo为水印图片 File logo = ResourceUtils.getFile(ResourceUtils.CLASSPATH_URL_PREFIX + &quot;static/shuiyin.png&quot;);// File logo = new File(logoPath); Image imageLogo = ImageIO.read(logo); int width1 = imageLogo.getWidth(null); int height1 = imageLogo.getHeight(null); ////设置水印 ALPHA大小 graphics2D.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_ATOP, 0.3F)); graphics2D.rotate(Math.toRadians(30), bufferedImage.getHeight() / 2, bufferedImage.getHeight() / 2);//图层以中心为基点，转30度的弧度。 int x = -width / 2; int y = -height / 2; while (x &lt; width * 1.5) &#123; y = -height / 2; while (y &lt; height * 1.5) &#123; graphics2D.drawImage(imageLogo, x, y, null); //MARGIN_ALL 边缘参数 y += height1 + 50; &#125; x += width1 + 50; &#125; graphics2D.dispose();// 5、 创建图像编码工具类 指定地址 Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String string = sdf.format(date); // realUploadPath + File.separator + image.getName(); String des = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + string + File.separator; File file = new File(des); if (!file.exists()) &#123; file.mkdirs(); &#125; String watermarkPaht = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + string + File.separator + image.getName(); //C:\Users\Tanghaiwen\AppData\Local\Temp\tomcat-docbase.4840919478909796944.8083\images\logo_1.jpg os = new FileOutputStream(watermarkPaht); JPEGImageEncoder jpegImageEncoder = JPEGCodec.createJPEGEncoder(os);// 6、使用图片编码工具类，输出缓存图像到模板图片文件 jpegImageEncoder.encode(bufferedImage); return des; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new WxException(&quot;图片水印处理失败&quot;); &#125; finally &#123; if (os != null) &#123; try &#123; os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new WxException(&quot;图片水印处理失败&quot;); &#125; &#125; &#125; &#125; 图片增加文字水印处理注：和图片文印一样,文字水印未经测试(若有问题就用上面的走一遍即可) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 public String watermark(File image) &#123; OutputStream os = null; try &#123; Image image2 = ImageIO.read(image); int width = image2.getWidth(null); int height = image2.getHeight(null);// 1、创建缓存图片对象 BufferedImage bufferedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);// 2、 创建JAVA绘图工具对象 Graphics2D graphics2D = bufferedImage.createGraphics();// 3、使用绘图工具对象，将原图绘制到缓存图片对象 graphics2D.drawImage(image2, 0, 0, width, height, null);// 4、使用绘图工具，将水印（文字/图片）绘制到缓存图片对象 graphics2D.setFont(new Font(&quot;微软雅黑&quot;, Font.BOLD, 30)); graphics2D.setColor(Color.RED); //S1-获取文字水印的宽、高 int width1 = 30 * getTextLength(&quot;水印文字&quot;); int height1 = 30; int widthDiff = width - width1; int heightDiff = height - height1; int x = 10; int y = 10; if (x &gt; widthDiff) &#123; x = widthDiff; &#125; if (y &gt; heightDiff) &#123; y = heightDiff; &#125; graphics2D.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_ATOP, 0.3F));//设置水印 graphics2D.drawString(&quot;水印文字&quot;, x, y + 30);//x:横坐标，y:纵坐标 graphics2D.dispose();// 5、 创建图像编码工具类 Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String string = sdf.format(date); // realUploadPath + File.separator + image.getName(); String des = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + string + File.separator; File file = new File(des); if (!file.exists()) &#123; file.mkdirs(); &#125; String watermarkPaht = &quot;D:&quot; + File.separator + &quot;workCatalog&quot; + File.separator + string + File.separator + image.getName(); os = new FileOutputStream(des); JPEGImageEncoder jpegImageEncoder = JPEGCodec.createJPEGEncoder(os);// 6、使用图片编码工具类，输出缓存图像到模板图片文件 jpegImageEncoder.encode(bufferedImage); return des; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new WxException(&quot;图片水印处理失败&quot;); &#125; finally &#123; if (os != null) &#123; try &#123; os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new WxException(&quot;图片水印处理失败&quot;); &#125; &#125; &#125; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>image水印处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java统一数据返回集]]></title>
    <url>%2F2019%2F05%2F24%2FresultClass%2F</url>
    <content type="text"><![CDATA[在项目的开发中，不管是前后数据的交互，还是远程接口的调用，我们都需要统一数据的返回格式。 这样我们的代码和开发效率都会规范提升起来。 1.用到的知识点： 需要引入jar: 123456789&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.0&lt;/version&gt;&lt;/dependency&gt; 2.创建返回实体类 12345678910111213141516171819202122232425262728293031@Builder@Getterpublic class Result&lt;T&gt; implements Serializable &#123; private static final long serialVersionUID = 1L; @Builder.Default private String code = &quot;0&quot;; @Builder.Default private String msg = &quot;success&quot;; @Builder.Default private boolean success = true; @Builder.Default private T data; public static &lt;T&gt;Result&lt;T&gt; success(T data)&#123; return Result.&lt;T&gt;builder().data(data).build(); &#125; public static &lt;T&gt;Result&lt;T&gt; success(String msg,T data)&#123; return Result.&lt;T&gt;builder().data(data).msg(msg).build(); &#125; public static &lt;T&gt;Result&lt;T&gt; fail(String msg,String code)&#123; return Result.&lt;T&gt;builder().code(code).msg(msg).build(); &#125;&#125; @Builder :用于修饰类表示可以进行Builder方式进行初始化； @Getter：表示只提供属性的get方法； @Default: 表示赋予字段默认的初始化值； 至此统一格式的返回类已经创建完毕 只需在实际的数据返回中调用即可。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>resultUtil</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webservice服务搭建与调用]]></title>
    <url>%2F2019%2F05%2F23%2Fwebservices%2F</url>
    <content type="text"><![CDATA[什么是Web Services: Web Services 是应用程序组件 Web Services 使用开放协议进行通信 Web Services 是独立的（self-contained）并可自我描述 Web Services 可通过使用UDDI来发现 Web Services 可被其他应用程序使用 XML 是 Web Services 的基础 它如何工作？基础的 Web Services 平台是 XML + HTTP。 HTTP 协议是最常用的因特网协议。 XML 提供了一种可用于不同的平台和编程语言之间的语言。 Web services 平台的元素（三要素）： SOAP (简易对象访问协议) UDDI (通用描述、发现及整合) WSDL (Web services 描述语言) 一：搭建Web Service服务使用的技术：Springboot1.5.17.RELEASE版本,无需创建Web项目简单的java项目就行。 在确保springboot项目正常运行的情况下需要额外导入如下jar： 123456789&lt;dependency&gt; &lt;groupId&gt;org.apache.cxf&lt;/groupId&gt; &lt;artifactId&gt;cxf-spring-boot-starter-jaxws&lt;/artifactId&gt; &lt;version&gt;3.1.12&lt;/version&gt;&lt;/dependency&gt; 下面正式开始WebService的流程搭建：1.创建一个接口类： 12345678910111213141516171819package com.demo.freemarker.service;import javax.jws.WebMethod;import javax.jws.WebService;//实现 Web Service 的 Java 类必须指定 @WebService 或 @WebServiceProvider 注释。不能同时提供这两种注//释@WebServicepublic interface HelloWorld &#123;//@WebMethod 注释表示作为一项 Web Service 操作的方法。 仅支持在使用 @WebService 注释来注释的类上使用 //@WebMethod 注释。 @WebMethod String sayHi(String text, String text1);&#125; 2.创建接口类的实现类： 12345678910111213141516package com.demo.freemarker.service;import javax.jws.WebService;@WebService(targetNamespace = &quot;http://service.freemarker.demo.com/&quot;, endpointInterface = &quot;com.demo.freemarker.service.HelloWorld&quot;)public class HelloWorldImpl implements HelloWorld &#123; @Override public String sayHi(String text, String text1) &#123; System.out.println(&quot;shyHi called text:&quot; + text + &quot; text1: &quot; + text1); return &quot;Hello&quot; + text; &#125;&#125; 注：endpointInterface为接口类，targetNamespace和wsdl文档中的targetNamespace一致（否则在接口调用的时候可能会报错,至于这个地址怎么写在启动服务后可以查看是否一致不一致则修改查看的方法后面会提到）。 3.创建webService实体配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.demo.freemarker.config;import com.demo.freemarker.service.HelloWorld;import com.demo.freemarker.service.HelloWorldImpl;import lombok.extern.slf4j.Slf4j;import org.apache.cxf.Bus;import org.apache.cxf.bus.spring.SpringBus;import org.apache.cxf.jaxws.EndpointImpl;import org.apache.cxf.transport.servlet.CXFServlet;import org.springframework.boot.web.servlet.ServletRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import javax.xml.ws.Endpoint;@Configuration@Slf4jpublic class wsdlConfig &#123; @Bean public ServletRegistrationBean dispatcherServlet() &#123; //发布服务名称 return new ServletRegistrationBean(new CXFServlet(), &quot;/service/*&quot;); &#125; @Bean public HelloWorld helloWorld() &#123; return new HelloWorldImpl(); &#125; @Bean(name = Bus.DEFAULT_BUS_ID) public SpringBus springBus() &#123; return new SpringBus(); &#125; @Bean public Endpoint endpoint() &#123; EndpointImpl endpoint = new EndpointImpl(springBus(), helloWorld()); //wsdl文件名称 endpoint.publish(&quot;/user&quot;); log.info(&quot;________success&quot;); return endpoint; &#125;&#125; 5.启动App程序 1234567891011121314151617package com.demo.freemarker;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class FreemarkerApp &#123; public static void main(String[] args) &#123; SpringApplication.run(FreemarkerApp.class, args); &#125;&#125; 6.启动成功后的步骤 1.在页面打开：http://localhost:8080/service/user?wsdl 会出现如视图： 注：其中targetNamespace的值应和上面实现类的targetNamespace值一致 至此简单WebService服务到此创建完成。 二：调用WebService服务上的接口调用的方法有很多种这里说的是org.apache.axis.client中的Call方法 1.引入jar包： 123456789&lt;dependency&gt; &lt;groupId&gt;org.apache.axis&lt;/groupId&gt; &lt;artifactId&gt;axis&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 2.创建一个调用类 123456789101112131415161718192021222324public static void main(String[] args) &#123; Service SERVICE = new Service() //对应WebService服务地址 String url = &quot;http://localhost:8080/service/user?wsdl&quot;; Call call = (Call) SERVICE.createCall(); //超时时间 call.setTimeout(8000); //当调用提示SOAPAcation不一致时需要加上如下代码：即 wsdl地址+接口方法名 //call.setSOAPActionURI(&quot;http://localhost:8080/service/user/sayHi&quot;); call.setTargetEndpointAddress(new URL(url)); //QNmae即上面的targetNamespace值,sayHi即表示portType中operation的值 call.setOperationName(new QName(&quot;http://service.freemarker.demo.com/&quot;, &quot;sayHi&quot;)); //参数arg0即表示message Name属性下对应的elemen，elemen对应types下面的sayHi,sayHi对应arg0 //可自行更具上图wsdl理解，当wsdl中没有types这个属性时 arg0可直接改为 name值即sayHi call.addParameter(&quot;arg0&quot;, org.apache.axis.encoding.XMLType.XSD_STRING, ParameterMode.IN); call.addParameter(&quot;arg1&quot;, org.apache.axis.encoding.XMLType.XSD_STRING, ParameterMode.IN); call.setReturnType(org.apache.axis.encoding.XMLType.SOAP_STRING); String str = (String) call.invoke(new String[]&#123;&quot;1&quot;,&quot;2&quot;&#125;); System.out.println(str);&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>webService</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js闭包]]></title>
    <url>%2F2019%2F05%2F22%2Fjsclosure%2F</url>
    <content type="text"><![CDATA[闭包是函数和声明该函数的词法环境的组合。 词法作用域考虑如下情况： 123456789101112function init() &#123; var name = &quot;Mozilla&quot;; // name 是一个被 init 创建的局部变量 function displayName() &#123; // displayName() 是内部函数,一个闭包 alert(name); // 使用了父函数中声明的变量 &#125; displayName();&#125;init(); init() 创建了一个局部变量 name 和一个名为 displayName() 的函数。displayName() 是定义在 init() 里的内部函数，仅在该函数体内可用。displayName() 内没有自己的局部变量，然而它可以访问到外部函数的变量，所以 displayName() 可以使用父函数 init() 中声明的变量 name 。但是，如果有同名变量 name 在 displayName() 中被定义，则会使用 displayName() 中定义的 name 。 运行代码可以发现 displayName() 内的 alert() 语句成功的显示了在其父函数中声明的 name 变量的值。这个词法作用域的例子介绍了引擎是如何解析函数嵌套中的变量的。词法作用域中使用的域，是变量在代码中声明的位置所决定的。嵌套的函数可以访问在其外部声明的变量。 闭包例子如下： 12345678910111213function makeFunc() &#123; var name = &quot;Mozilla&quot;; return function () &#123; alert(name); &#125;;&#125;var myFunc = makeFunc();myFunc(); 运行这段代码和之前的 init() 示例的效果完全一样。其中的不同 — 也是有意思的地方 — 在于内部函数，外部函数返回。 第一眼看上去，也许不能直观的看出这段代码能够正常运行。在一些编程语言中，函数中的局部变量仅在函数的执行期间可用。一旦 makeFunc() 执行完毕，我们会认为 name 变量将不能被访问。然而，因为代码运行得没问题，所以很显然在 JavaScript 中并不是这样的。 这个谜题的答案是，JavaScript中的函数会形成闭包。 闭包是由函数以及创建该函数的词法环境组合而成。这个环境包含了这个闭包创建时所能访问的所有局部变量。在我们的例子中，myFunc 是执行 makeFunc 时创建的 返回函数实例的引用，而返回实例仍可访问其词法作用域中的变量，即可以访问到 name 。由此，当 myFunc 被调用时，name 仍可被访问，其值 Mozilla 就被传递到alert中。 再看下面一组实例： 1234567891011121314function makeAdder(x) &#123; return function(y) &#123; return x + y; &#125;;&#125;var add5 = makeAdder(5);var add10 = makeAdder(10);console.log(add5(2)); // 7console.log(add10(2)); // 12 在这个示例中，我们定义了 makeAdder(x) 函数，它接受一个参数 x ，并返回一个新的函数。返回的函数接受一个参数 y，并返回x+y的值。 从本质上讲，makeAdder 是一个函数工厂 — 他创建了将指定的值和它的参数相加求和的函数。在上面的示例中，我们使用函数工厂创建了两个新函数 — 一个将其参数和 5 求和，另一个和 10 求和。 add5 和 add10 都是闭包。它们共享相同的函数定义，但是保存了不同的词法环境。在 add5的环境中，x 为 5。而在 add10 中，x 则为 10。 注：本文转来自：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Closures 更多详解请移步链接。]]></content>
      <categories>
        <category>js</category>
      </categories>
      <tags>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客面板动画效果]]></title>
    <url>%2F2019%2F05%2F21%2Fanimation%2F</url>
    <content type="text"><![CDATA[官网地址配置：https://www.npmjs.com/package/hexo-helper-live2d 执行步骤如下： 1.首先检查博客主目录下面的 package.json里面是否有 “hexo-helper-live2d:” “3.1.3”依赖,有的话可以先卸载 卸载命令： npm uninstall hexo-helper-live2d 然后再安装 npm install –save hexo-helper-live2d 注：命令都是在博客的主目录执行 安装完成之后在 package.json 会看到安装的model依赖如下： 2.然后在 node_moduels 目录下会看到一下文件，这些都是动画主配置： 然后下载我们自己需要的动画model： 地址：https://github.com/xiazeyu/live2d-widget-models.git 下载后将上图的文件全部复制到node_moduels文件夹中； 3.配置站点配置文件，_config.yml 1234567891011121314live2d: enable: true pluginModelPath: assets/ model: use: live2d-widget-model-epsilon2_1 #模板目录，在node_modules里 display: position: right width: 150 height: 300 mobile: show: false #是否在手机进行显示 4.开始部署 使用命令 hexo g 重新构造文件 hexo s 启动服务 就能看到效果了—前提是上面两句命令成功执行。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>博客知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客页面爱心点击效果]]></title>
    <url>%2F2019%2F05%2F21%2Fclick%2F</url>
    <content type="text"><![CDATA[创建js文件注：此处以next主题模板为例 在/themes/next/source/js/src下新建一个js如：clicklove.js，然后把以下内容复制到js中 123!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 修改_layout.swing在/themes/next/layout/_layout.swig文件末尾添加： 123456&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>博客知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[freemarker模板]]></title>
    <url>%2F2019%2F05%2F21%2Ffreemarker%2F</url>
    <content type="text"><![CDATA[一.什么是 Apache FreeMarker。 1.Apache FreeMarker™是一个模板引擎：一个Java库，用于根据模板和更改数据生成文本输出（HTML网页，电子邮件，配置文件，源代码等）。模板是用FreeMarker模板语言（FTL）编写的，这是一种简单的专用语言（不像PHP这样的完整编程语言） 特征FreeMarker的一些亮点： 强大的模板语言：条件块，迭代，赋值，字符串和算术运算和格式，宏和函数，包括其他模板，默认情况下转义（可选）等等 多用途和轻量级：零依赖性，任何输出格式，可以从任何地方（可插入）加载模板，许多配置选项 国际化/本地化感知：区域设置敏感数字和日期/时间格式，本地化模板变体。 XML处理功能：将XML DOM-s放入数据模型并遍历它们，甚至以声明方式处理它们 多功能数据模型：Java对象通过可插拔适配器作为变量树暴露给模板，该适配器决定模板如何看待它们。 二.springboot中快速使用freemarker模板。 1.在Pom文件中引入相应的jar； 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt;&lt;/dependency&gt; 2.在项目的的resources文件下创建freemarker文件夹用于存放freemarker模板文件 文件后缀名为：.ftl 3.在配置文件application.yml中配置freemarker的属性 123456789101112spring: freemarker: suffix: .ftl template-loader-path: file:D:/project-demo-template/project-demo-freemarker/target/classes/freemarker/ mvc: view: suffix: .ftl prefix: /templates/ template-loader-path为模板文件存放的地址 注:在使用freemarker模板时请注意关闭模板缓存： spring.freemarker.cache=false 到此整个流程已经完成。 4.当freemarker模板不用于页面展示时如：用于xml格式模板用于写xml数据 则需要配置其模板初始化 一般在Service层使用 12345678910111213141516171819202122232425262728@Servicepublic class FreemarkerService &#123; private final Configuration configuration; //init @Autowired public FreemarkerService(Configuration configuration) &#123; this.configuration = configuration; &#125; /** *name:为模板名称如：test.ftl,map表参数 *configuration.getTemplate(name) 获取模板 * FreeMarkerTemplateUtils.processTemplateIntoString(template, map) 给模板赋值并转换为String; * 相当于model.addAttribute(&quot;&quot;,&quot;&quot;); reutrn &quot;xx.jsp&quot; 一样； * 只不过前者是附后拿到相应的类型数据 后者是在页面展示 */ public String processTemplateIntoString(String name, Map&lt;String, Object&gt; map) throws IOException, TemplateException &#123; Template template = configuration.getTemplate(name); map.put(&quot;name&quot;, &quot;这是一个名称&quot;); String str = FreeMarkerTemplateUtils.processTemplateIntoString(template, map); return str; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot异常处理的几种方法]]></title>
    <url>%2F2019%2F05%2F20%2FexceptionPage%2F</url>
    <content type="text"><![CDATA[springboot中异常处理的几种方式： 1).使用默认springBoot中默认的异常处理：BasicErrorController.class 2).自定义异常处理类 3).使用@ExceptionHandler注解做全局异常处理 1.自带的异常处理分为2类一种是返回页面，一种是返回json格式如下： 我们可以使用自定义的异常处理类和错误配置页面来覆盖的默认的错误提示页面如下： 1234567891011121314151617181920212223public class WxException extends Exception &#123; private static final long serialVersionUID = 1L; private String message; private String errCode; public WxException(String message) &#123; super(message); &#125; public WxException(String errCode, String message) &#123; super(message); this.errCode = errCode; this.message = message; &#125;&#125;配置页面则在相应的配置在资源地址下如：webapp/WEB-INF/error/xxx.jsp (注xxx应该为明确的状态协议码)这样就完成了对默认错误页面的覆盖 2.使用@ExceptionHandler注解做全局异常处理 1234567891011121314151617首先自定义一个异常处理类：WxException.class；然后定义一个全局异常类如下：public class BaseController &#123; @ExceptionHandler(WxException.class) @ResponseBody public Rs handleValidationException(WxException e) &#123; return Rs.ofFail(&quot;-10000&quot;, e.getMessage()); &#125;&#125;其中@ExceptionHandler中的value值可以自行定义可以是自定义的异常类也可以是针对单独某一个类。然后返回自己想要的格式就行。 3.若只返回json数据我们也可以继承BasicErrorContorller类，重写error方法编写自己想要的数据如下： 123456789101112131415161718192021222324252627@Controllerpublic class GlobalErrorController extends BasicErrorController &#123; public GlobalErrorController() &#123; super(new DefaultErrorAttributes(), new ErrorProperties()); &#125; @Override @RequestMapping @ResponseBody public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = this.getErrorAttributes(request, this.isIncludeStackTrace(request, MediaType.ALL)); // HttpStatus status = this.getStatus(request); String errcode = String.valueOf(body.get(&quot;status&quot;) != null ? body.get(&quot;status&quot;) : &quot;500&quot;); // String errmsg = (String) body.get(&quot;error&quot;); String path = (String) body.get(&quot;path&quot;); String message = (String) body.get(&quot;message&quot;); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(8); map.put(&quot;errCode&quot;, errcode); map.put(&quot;path&quot;, path); map.put(&quot;message&quot;, message); map.put(&quot;success&quot;, &quot;false&quot;); return new ResponseEntity&lt;&gt;(map, HttpStatus.OK); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[输入框自动切换光标demo]]></title>
    <url>%2F2019%2F05%2F20%2FinputFocus%2F</url>
    <content type="text"><![CDATA[关于多输入框自动切换光标及粘贴功能(代码仅供参考可以直接用) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//js代码:如下功能实现了输入框的粘贴切换光标问题//按多长间隔截取字符串，n为长度，返回按长度分割成的字符串数组；String.prototype.StrCut2Arr = function (n) &#123; var str = this; var arr = []; var len = Math.ceil(str.length / n); for (var i = 0; i &lt; len; i++) &#123; if (str.length &gt;= n) &#123; var strCut = str.substring(0, n); arr.push(strCut); str = str.substring(n); &#125; else &#123; str = str; arr.push(str); &#125; &#125; return arr;&#125;$(&apos;.slinput&apos;).bind(&apos;input propertychange&apos;, function () &#123; // 删除往前 添加往后 $(this).index(); if (&quot;slinput&quot; == this.className) &#123; var thisName = this.name; if ($(&quot;#&quot; + thisName).val().trim().length == 4) &#123; $(this).next(&apos;input&apos;).focus(); &#125; else if ($(&quot;#&quot; + thisName).val().trim().length &gt; 4) &#123; var strContent = $(&quot;#&quot; + thisName).val().trim().replace(/-/g, &quot;&quot;); $(&apos;.slinput&apos;).val(&quot;&quot;); const foo = strContent.StrCut2Arr(4); if (foo.length &gt;= 4) &#123; for (let i = 0; i &lt; 4; i++) &#123; let inputIndex = $(&quot;.slinput&quot;).get(i); let inputName = inputIndex.name; $(&quot;#&quot; + inputName).val(foo[i]); &#125; &#125; else &#123; for (let i = 0; i &lt; foo.length; i++) &#123; let inputIndex = $(&quot;.slinput&quot;).get(i); let inputName = inputIndex.name; $(&quot;#&quot; + inputName).val(foo[i]); &#125; &#125; &#125; if ($(&quot;#&quot; + thisName).val().trim().length &lt; 1) &#123; $(this).prev(&apos;input&apos;).focus(); &#125; &#125;&#125;) //简单的输入框切换光标代码如下：123456789101112131415$(&apos;#num_input input&apos;).keyup(function (event) &#123; // 删除往前 添加往后 var this_name = this.name; // .trim() if ($(&quot;#&quot; + this_name).val().trim().length &gt;= 4) &#123; $(this).next(&apos;input&apos;).focus(); &#125; if ($(&quot;#&quot; + this_name).val().trim().length &lt; 1) &#123; if (event.keyCode == 46 || event.keyCode == 8) &#123; $(this).prev(&apos;input&apos;).focus(); &#125; &#125;&#125;) 注：根据自身业务的需求上面两组代码任选一种即可。 123456789101112131415//页面代码如下：&lt;div class=&quot;slbh&quot; id=&quot;num_input&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;xxxx&quot; class=&quot;slinput&quot; style=&quot;margin-left: 0;&quot; name=&quot;sic_a&quot; id=&quot;sic_a&quot; onfocus=&quot;this.placeholder=&apos;&apos;&quot; onblur=&quot;this.placeholder=&apos;xxxx&apos;&quot;/&gt; &lt;input type=&quot;text&quot; placeholder=&quot;xxxx&quot; class=&quot;slinput&quot; name=&quot;sic_b&quot; id=&quot;sic_b&quot; onfocus=&quot;this.placeholder=&apos;&apos;&quot; onblur=&quot;this.placeholder=&apos;xxxx&apos;&quot;/&gt; &lt;input type=&quot;text&quot; placeholder=&quot;xxxx&quot; class=&quot;slinput&quot; name=&quot;sic_c&quot; id=&quot;sic_c&quot; onfocus=&quot;this.placeholder=&apos;&apos;&quot; onblur=&quot;this.placeholder=&apos;xxxx&apos;&quot;/&gt; &lt;input type=&quot;text&quot; placeholder=&quot;xx&quot; maxlength=&quot;2&quot; class=&quot;slinput&quot; name=&quot;sic_d&quot; id=&quot;sic_d&quot; onfocus=&quot;this.placeholder=&apos;&apos;&quot; onblur=&quot;this.placeholder=&apos;xx&apos;&quot;/&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>jsp</category>
      </categories>
      <tags>
        <tag>input自动切换光标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[@NotBlank踩坑日常]]></title>
    <url>%2F2019%2F05%2F15%2FNotBlank%2F</url>
    <content type="text"><![CDATA[@NotNill: 不能为null,但可以为empty @NotEmpty: 不能为null，而且长度必须大于0 @NotBlank:只能用在String类型上,不能为null，而且调用trim()后长度必须大于0， 注：NotBlank注解必须配合@Validated使用否则无效]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scheduled定时器的用法]]></title>
    <url>%2F2019%2F05%2F07%2FScheduled%E5%AE%9A%E6%97%B6%E5%99%A8%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[三种常用的Scheduled方法的表达式：cron,fixedDelay,fixedRate 注：使用此方法的类需要配置@Configuration，@EnableScheduling注解当然也可以开启其他注解， 使用@Scheduled标志的类其返回类型为void； 定时器执行任务1.cron //每隔20分钟执行一次此方法 @Scheduled(cron = “0 /20 ?”) public void cornMethod(){ //log.info(“—-“) } 2.fixedDelay //每隔20分钟执行一次此方法,且在项目启动的时候会立马执行一次 @Scheduled(fixedDelay = 20 60 1000) public void fixedDelayMethod(){ //log.info(“—-“) } 3.fixedRate //每隔20分钟执行一次此方法,且在项目启动的时候会立马执行一次,在每次调用的连续开始时间之间测量 public void fixedRateMethod(){ //log.info(“—-“) } 异步执行任务为什么要使用异步执行任务：当处理一个业务时业务符合某些条件时需要执行其他的步骤，但是这个步骤并不影响业务正常流程，这个时候就可以使用异步。 使用异步任务执行和使用上面的同步是同一个道理，同样需要配置相关的注解。 我们可以先配置一个异步线程池如下： 12345678910111213141516171819202122232425262728293031@Configuration//开启异步任务执行注解@EnableAsync//开启同步任务执行注解（定时器）@EnableSchedulingpublic class IdapAsyncConfigurer implements AsyncConfigurer &#123; //@使用@Asunc(&quot;eventThreadPoolTaskExecutor&quot;)标注类时使用下面的配置数据 @Bean(&quot;eventThreadPoolTaskExecutor&quot;) public Executor eventThreadPoolTaskExecutor() &#123; return executor(5, 10, 20, 30); &#125; //使用默认@Async标注类时使用下面的配置数据 @Override public Executor getAsyncExecutor() &#123; return executor(10, 128, 256, 60); &#125; private Executor executor(int coreSize, int maxSize, int queueSize, int keepAlive) &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(coreSize); executor.setMaxPoolSize(maxSize); executor.setQueueCapacity(queueSize); executor.setKeepAliveSeconds(keepAlive); executor.setThreadNamePrefix(&quot;async-executor-&quot;); executor.initialize(); return executor; &#125;&#125; coreSize:核心线程大小 maxSize:最大线程数 queueSize: 队列大小（容量） keepAlive:线程存活时间 queue-capacity还提供了一个值。还应根据执行者的队列容量来考虑线程池的配置。有关池大小和队列容量之间关系的完整说明，请参阅文档 ThreadPoolExecutor。主要思想是，在提交任务时，如果活动线程数当前小于核心大小，则执行程序首先尝试使用空闲线程。如果已达到核心大小，则只要尚未达到其容量，就将任务添加到队列中。只有这样，如果达到队列的容量，执行程序才创建超出核心大小的新线程。如果还达到了最大大小，那么执行者将拒绝该任务。 keep-alive设置确定线程在终止之前可以保持空闲状态的时间限制（以秒为单位）。如果当前池中的线程数超过核心数，则在等待此时间而不处理任务后，多余的线程将被终止。时间值为零会导致多余的线程在执行任务后立即终止，而不会在任务队列中保留后续工作 一个的异步代码： 您可以@Async在方法上提供注释，以使该方法的调用异步发生。换句话说，调用方在调用后立即返回，而方法的实际执行发生在已提交给Spring的任务中TaskExecutor。在最简单的情况下 123456789@Async(&quot;eventThreadPoolTaskExecutor&quot;)public void SyncUpdateWxUserInfo(String param) &#123; log.info(&quot;在这个异步方法里面处理需要被处理的业务&quot;);&#125; 注：详情用法请参考官网：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/integration.html#scheduling-annotation-support-scheduled]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>springBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令]]></title>
    <url>%2F2019%2F05%2F05%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1.docker常用命令 docker search mysql 这条命令表示查询mysql的所有镜像信息 docker pull mysql 表示从官方下载默认版本的mysql，latest docker pull mysql:5.6 表示下载mysql版本5.6的 版本 docker images 查询当前本地的所有镜像 docker rmi image-id 删除指定镜像，image-id是每个镜像独有的id docker rum …. 根据镜像启动容器 docker ps 查询运行中的容器 docker ps -a 查询所有容器 docker start 容器id 启动容器 docker stop 容器id 停止容器 docker rm 容器id 删除容器]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>dockerBaseCommand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker创建mysql]]></title>
    <url>%2F2019%2F05%2F05%2Fdocker%E5%88%9B%E5%BB%BAmysql%2F</url>
    <content type="text"><![CDATA[使用docker创建mysql数据库： 1.查找Docker Hub上的mysql镜像： docker search mysql 这里我们拉取官方的镜像，标签为5.6 docker pull mysql:5.6 然后查看下本地的所有镜像： docker images 2.创建并启动一个Mysql容器： 123456789101112131415输入一下命令:docker run --name xx -e MYSQL_ROOT_PASSWORD=xx -p 3306:3306 -d mysql–name：给新创建的容器命名，此处命名为xx-e：配置信息，此处配置`mysql`的`root用户`的登陆密码-p：端口映射，此处映射`主机3306端口`到`容器name的3306端口`-d：成功启动容器后输出容器的完整ID最后一个`mysql`指的是`mysql镜像名字` 然后查看容器的运行状态： docker ps 此时mysql的创建已经完成；然后就是打开mysql 3306的端口 这个请自行百度或者查看我的 hexoBaseOperation博客； 3.使用Navicat连接数据库 连接时可能会提示如下的错误： 2059 -Authentication plugin ‘caching_sha2_password’ cannot be loaded; 这个错误出现的原因是在mysql8之前的版本中加密规则为mysql_native_password，而在mysql8以后的加密规则为caching_sha2_password。 此时需要进入mysql中修改加密规则： 进入mysql mysql -uxx -pxx (u表示用户名 p表示数据库密码) 修改加密规则及密码，然后刷新即可 ALTER USER ‘root‘@’localhost’ IDENTIFIED BY ‘你有的mysql密码’ PASSWORD EXPIRE NEVER; 修改加密规则 ALTER USER ‘root‘@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘你的mysql密码’; #修改密码FLUSH PRIVILEGES; #刷新数据 注：’localhost’可以用‘%’代替表示所有 然后重新使用Navicat链接既可。 4.Linux中如何进入docker镜像Mysql数据库 docker ps —查询所有容器 然后启动容器的id docker start 003b99142b74d (这个看上图的CONTAINER ID) 然后执行 docker exec -it pwc-mysql bash;(其中pwc-mysql 为上图的NAMES对应的值 我这里是pwc-mysql) 退出命令： exit; 新：docker pull mysql:xx(版本号，不加默认拉取最新版本) //启动命名 docker run -p 3306:3306 –name mysql -v /mydata/mysql/log:/var/log/mysql -v /mydata/mysql/data:/var/lib/mysql -v /mydata/mysql/conf:/etc/mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7 命令详解： -p 3306:3306 将容器的3306端口映射到主机的3306端口 -v /mydata/mysql/log:/var/log/mysql 将日志文件夹挂在到主机 -v /mydata/mysql/data:/var/lib/mysql\ 将配置文件加挂在到主机 -v /mydata/mysql/conf:/etc;mysql\ 将数据文件夹挂在到主机 -e MYSQL_ROOT_PASSWORD=root 初始化root用户的名称 -d 以后台方式运行 sudo docker update mysql –restart=always //虚拟机启动，mysql跟随重启 (mysql是自己给容器起的名字，也可以容器id) exit //退出容器docker restart container_id/names 进入mysql容器：docker exec -it container_id/names]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinuxCentos7网管配置]]></title>
    <url>%2F2019%2F04%2F30%2FLinuxCentos7%E7%BD%91%E7%AE%A1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Centos7对外开放开端口1.查看对外开放的端口状态 查询已开放的端口 : netstat -anp 查看指定端口是否已开： 123firewall-cmd --query-port=xx/tcp 注：上面firewall执行后提示 yes表示开启，no表示未开启,- -之间没有空格请在使用的时候自行去掉 2.查看防火墙状态 查看防火墙状态：systemctl status firewalld 开启防火墙： systemctl start firewalld 关闭防火墙：systemmctl stop firewalld 若无法开启请先用：systemctl unmask firewalld.service 然后执行： systemctl start firewalld.service 3.添加指定需要开发的端口 123456789firewall-cmd --add-port=xxx/tcp --permanent重新载入添加的端口firewall-cmd --reload然后执行上面的查询端口开放命令 4.移除指定端口 123firewall-cmd --permanent --remove-port=xxx/tcp]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Centos7网络配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令]]></title>
    <url>%2F2019%2F04%2F30%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hexo基本命令$ hexo generate (hexo g) 生成静态文件 $ hexo server (hexo s) 启动本地服务 $ hexo deploy (hexo d) 提交到远程仓库 $ hexo new page “xx” (hexo n p ) 创建页面 $ hexo d -g 生成静态文件并提交到远程参库 $ hexo s -g 生成静态文件并启动本地预览 $ hexo clean 清除本地 public 文件]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>博客知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github + hexo搭建博客]]></title>
    <url>%2F2019%2F04%2F30%2Fdemo%2F</url>
    <content type="text"><![CDATA[什么是 Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 什么是 GitHub ？GitHub 是通过 Git 进行版本控制的软件源代码托管服务，由 GitHub 公司（曾称 Logical Awesome）的开发者 Chris Wanstrath、PJ Hyett 和 Tom Preston-Werner 使用 Ruby on Rails 编写而成。（维基百科） 下面开始准备步骤1.首先检查自己电脑上是否安装了 Git，检查方式：Windos键+R,在弹出框里输入cmd 然后在Doc窗口输入： git version 检查电脑是否安装了Node.js 如上在Doc窗口输入：node -v 安装Git 下载Git 安装NodeJs 下载NodeJs Git安装成功后，点击鼠标右键会发现多出如下两先选： 安装Hexo鼠标点击Git Base Here，一次执行如下代码 12345$ npm install hexo-cli -g $ hexo init blog$ cd blog$ npm install$ hexo server 第一步是安装hexo插件 第二步是创建一个为blog的文件夹然后把hexo的代码下载到改该文件中，结果如下： 12345678node_modules npm 文件缓存目录scaffolds 文夹件下存放的是文章、页面模版scource 文夹件下存放的是我们的资源文件themes 文件下存放的是我们的主题文件.gitignore git 忽略文件，设置提交文件时，哪些文件不提交_config.yml 站点配置文件package.json 站点版本，站点依赖文件yarn.lock yarn.lock 文件由 Yarn 自动创建，并且完全通过 Yarn 进行操作。 第三步是进入 blog 文件夹，第四步是安装 hexo 相关的代码。 第五步是启动本地服务，启动完成后，如图 ： 在浏览器输入 http://localhost:4000/ 就可以访问刚刚创建的博客了。如下图 : 本文转自 https://www.jianshu.com/p/eded1dd2d794/详情的安装搭建请自行跳转 注 ： 当我们需要关闭本地服务的时候，可以在通过在 Git Bash 窗口输入 Ctrl + C 关闭服务。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>博客知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
